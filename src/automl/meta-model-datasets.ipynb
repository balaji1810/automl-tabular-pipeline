{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed745d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80f641e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 8 datasets from UCI ML Repository\n",
      "Dataset IDs: [1, 189, 942, 890, 880, 925, 913, 713]\n",
      "→ Fetching UCI dataset 1... ✓ Shape: (4177, 8), Target: (4177,)\n",
      "→ Fetching UCI dataset 189... ✓ Shape: (4177, 8), Target: (4177,)\n",
      "→ Fetching UCI dataset 189... [Multiple targets, using first: motor_UPDRS] ✓ Shape: (5875, 19), Target: (5875,)\n",
      "→ Fetching UCI dataset 942... [Multiple targets, using first: motor_UPDRS] ✓ Shape: (5875, 19), Target: (5875,)\n",
      "→ Fetching UCI dataset 942... [Skipped: Non-numeric target]\n",
      "→ Fetching UCI dataset 890... [Skipped: Non-numeric target]\n",
      "→ Fetching UCI dataset 890... ✓ Shape: (2139, 23), Target: (2139,)\n",
      "→ Fetching UCI dataset 880... ✓ Shape: (2139, 23), Target: (2139,)\n",
      "→ Fetching UCI dataset 880... [Multiple targets, using first: death] ✓ Shape: (9105, 42), Target: (9105,)\n",
      "→ Fetching UCI dataset 925... [Multiple targets, using first: death] ✓ Shape: (9105, 42), Target: (9105,)\n",
      "→ Fetching UCI dataset 925... [Multiple targets, using first: aveOralF] ✓ Shape: (1020, 33), Target: (1020,)\n",
      "→ Fetching UCI dataset 913... [Multiple targets, using first: aveOralF] ✓ Shape: (1020, 33), Target: (1020,)\n",
      "→ Fetching UCI dataset 913... [Skipped: Non-numeric target]\n",
      "→ Fetching UCI dataset 713... [Skipped: Non-numeric target]\n",
      "→ Fetching UCI dataset 713... [Multiple targets, using first: verification.result] ✓ Shape: (2043, 7), Target: (2043,)\n",
      "\n",
      "UCI ML Repository Loading Summary:\n",
      "• Successfully loaded: 6/8 datasets\n",
      "• Failed to load: 2 datasets\n",
      "• Failed dataset IDs and reasons:\n",
      "  - Dataset 942: Non-numeric target\n",
      "  - Dataset 913: Non-numeric target\n",
      "• Successfully loaded dataset details:\n",
      "  - ID 1: Abalone - (4177, 8) features, 4177 samples\n",
      "  - ID 189: Parkinsons Telemonitoring - (5875, 19) features, 5875 samples\n",
      "  - ID 890: AIDS Clinical Trials Group Study 175 - (2139, 23) features, 2139 samples\n",
      "  - ID 880: SUPPORT2 - (9105, 42) features, 9105 samples\n",
      "  - ID 925: Infrared Thermography Temperature - (1020, 33) features, 1020 samples\n",
      "  - ID 713: Auction Verification - (2043, 7) features, 2043 samples\n",
      "\n",
      "Loaded 6 datasets successfully!\n",
      "Dataset 1 (Abalone): (4177, 8) features, 4177 samples\n",
      "Dataset 189 (Parkinsons Telemonitoring): (5875, 19) features, 5875 samples\n",
      "Dataset 890 (AIDS Clinical Trials Group Study 175): (2139, 23) features, 2139 samples\n",
      "Dataset 880 (SUPPORT2): (9105, 42) features, 9105 samples\n",
      "Dataset 925 (Infrared Thermography Temperature): (1020, 33) features, 1020 samples\n",
      "Dataset 713 (Auction Verification): (2043, 7) features, 2043 samples\n",
      "[Multiple targets, using first: verification.result] ✓ Shape: (2043, 7), Target: (2043,)\n",
      "\n",
      "UCI ML Repository Loading Summary:\n",
      "• Successfully loaded: 6/8 datasets\n",
      "• Failed to load: 2 datasets\n",
      "• Failed dataset IDs and reasons:\n",
      "  - Dataset 942: Non-numeric target\n",
      "  - Dataset 913: Non-numeric target\n",
      "• Successfully loaded dataset details:\n",
      "  - ID 1: Abalone - (4177, 8) features, 4177 samples\n",
      "  - ID 189: Parkinsons Telemonitoring - (5875, 19) features, 5875 samples\n",
      "  - ID 890: AIDS Clinical Trials Group Study 175 - (2139, 23) features, 2139 samples\n",
      "  - ID 880: SUPPORT2 - (9105, 42) features, 9105 samples\n",
      "  - ID 925: Infrared Thermography Temperature - (1020, 33) features, 1020 samples\n",
      "  - ID 713: Auction Verification - (2043, 7) features, 2043 samples\n",
      "\n",
      "Loaded 6 datasets successfully!\n",
      "Dataset 1 (Abalone): (4177, 8) features, 4177 samples\n",
      "Dataset 189 (Parkinsons Telemonitoring): (5875, 19) features, 5875 samples\n",
      "Dataset 890 (AIDS Clinical Trials Group Study 175): (2139, 23) features, 2139 samples\n",
      "Dataset 880 (SUPPORT2): (9105, 42) features, 9105 samples\n",
      "Dataset 925 (Infrared Thermography Temperature): (1020, 33) features, 1020 samples\n",
      "Dataset 713 (Auction Verification): (2043, 7) features, 2043 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_ucimldatasets(dataset_ids: list[int] = None) -> list:\n",
    "    \"\"\"\n",
    "    Load datasets from UCI ML Repository based on specified dataset IDs.\n",
    "    Returns a list of loaded datasets in the same format as load_openml_datasets.\n",
    "    \n",
    "    Args:\n",
    "        dataset_ids: List of UCI ML Repository dataset IDs to fetch.\n",
    "                    If None, uses default list: [1, 189, 942, 890, 880, 925, 913, 713]\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing dataset information\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Default dataset IDs if none provided\n",
    "    if dataset_ids is None:\n",
    "        dataset_ids = [1, 189, 942, 890, 880, 925, 913, 713]\n",
    "    \n",
    "    print(f\"Loading {len(dataset_ids)} datasets from UCI ML Repository\")\n",
    "    print(f\"Dataset IDs: {dataset_ids}\")\n",
    "    \n",
    "    loaded_datasets = []\n",
    "    successful_loads = 0\n",
    "    failed_loads = []\n",
    "    \n",
    "    for did in dataset_ids:\n",
    "        try:\n",
    "            print(f\"→ Fetching UCI dataset {did}...\", end=\" \", flush=True)\n",
    "            \n",
    "            # Fetch dataset from UCI ML Repository\n",
    "            dataset = fetch_ucirepo(id=did)\n",
    "            \n",
    "            # Extract features and target\n",
    "            X = dataset.data.features\n",
    "            y = dataset.data.targets\n",
    "            \n",
    "            # Handle the case where y might have multiple columns\n",
    "            if isinstance(y, pd.DataFrame):\n",
    "                if y.shape[1] == 1:\n",
    "                    y = y.iloc[:, 0]  # Convert to Series if single column\n",
    "                else:\n",
    "                    # If multiple targets, take the first one\n",
    "                    print(f\"[Multiple targets, using first: {y.columns[0]}]\", end=\" \")\n",
    "                    y = y.iloc[:, 0]\n",
    "            \n",
    "            # Convert to pandas DataFrame/Series if not already\n",
    "            if not isinstance(X, pd.DataFrame):\n",
    "                X = pd.DataFrame(X)\n",
    "            if not isinstance(y, pd.Series):\n",
    "                y = pd.Series(y)\n",
    "            \n",
    "            # Check if it's a regression task (numeric target)\n",
    "            if not pd.api.types.is_numeric_dtype(y):\n",
    "                print(f\"[Skipped: Non-numeric target]\")\n",
    "                failed_loads.append((did, \"Non-numeric target\"))\n",
    "                continue\n",
    "            \n",
    "            # Create categorical indicator (True for non-numeric columns)\n",
    "            categorical_indicator = []\n",
    "            for col in X.columns:\n",
    "                is_categorical = not pd.api.types.is_numeric_dtype(X[col])\n",
    "                categorical_indicator.append(is_categorical)\n",
    "            \n",
    "            # Get attribute names\n",
    "            attribute_names = list(X.columns)\n",
    "            \n",
    "            # Add to loaded datasets\n",
    "            loaded_datasets.append({\n",
    "                \"X\": X,\n",
    "                \"y\": y,\n",
    "                \"categorical_indicator\": categorical_indicator,\n",
    "                \"attribute_names\": attribute_names,\n",
    "                \"dataset_id\": did,\n",
    "                \"dataset_name\": getattr(dataset.metadata, 'name', f'UCI_Dataset_{did}'),\n",
    "                \"source\": \"UCI_ML_Repository\"\n",
    "            })\n",
    "            \n",
    "            successful_loads += 1\n",
    "            print(f\"✓ Shape: {X.shape}, Target: {y.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            failed_loads.append((did, str(e)))\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nUCI ML Repository Loading Summary:\")\n",
    "    print(f\"• Successfully loaded: {successful_loads}/{len(dataset_ids)} datasets\")\n",
    "    print(f\"• Failed to load: {len(failed_loads)} datasets\")\n",
    "    \n",
    "    if failed_loads:\n",
    "        print(\"• Failed dataset IDs and reasons:\")\n",
    "        for did, reason in failed_loads:\n",
    "            print(f\"  - Dataset {did}: {reason}\")\n",
    "    \n",
    "    if loaded_datasets:\n",
    "        print(\"• Successfully loaded dataset details:\")\n",
    "        for ds in loaded_datasets:\n",
    "            print(f\"  - ID {ds['dataset_id']}: {ds['dataset_name']} - {ds['X'].shape} features, {len(ds['y'])} samples\")\n",
    "    \n",
    "    return loaded_datasets\n",
    "\n",
    "# Example usage\n",
    "loaded_datasets = load_ucimldatasets([1, 189, 942, 890, 880, 925, 913, 713])\n",
    "if loaded_datasets:\n",
    "    print(f\"\\nLoaded {len(loaded_datasets)} datasets successfully!\")\n",
    "    for ds in loaded_datasets:\n",
    "        print(f\"Dataset {ds['dataset_id']} ({ds['dataset_name']}): {ds['X'].shape} features, {len(ds['y'])} samples\")\n",
    "else:\n",
    "    print(\"No datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508dad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import types\n",
    "\n",
    "# Ensure the current directory is in Python path\n",
    "current_dir = '/Users/surbhi/MASTERS/SS25/autoML/final-project/automl-project/automl-tabular-pipeline/src/automl'\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "os.chdir(current_dir)\n",
    "\n",
    "# Create a simple automl module that will act as a namespace\n",
    "automl = types.ModuleType('automl')\n",
    "automl.__path__ = [current_dir]\n",
    "sys.modules['automl'] = automl\n",
    "\n",
    "# Import modules directly and add them to the automl namespace\n",
    "import pre_processor\n",
    "import constants\n",
    "import FeatureSelector\n",
    "\n",
    "automl.pre_processor = pre_processor\n",
    "automl.constants = constants\n",
    "automl.FeatureSelector = FeatureSelector\n",
    "\n",
    "# Now import the modules we need\n",
    "from meta_features import extract_meta_features\n",
    "from meta_trainer import algorithms_eval\n",
    "from constants import algorithms\n",
    "\n",
    "# loaded_datasets = load_ucimldatasets([1, 189, 942, 890, 880, 925, 913, 713])\n",
    "\n",
    "# records = algorithms_eval(algorithms=algorithms, datasets=loaded_datasets)\n",
    "\n",
    "# df = pd.DataFrame(records)\n",
    "# df.to_csv(\"meta_features_uci.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582a450",
   "metadata": {},
   "source": [
    "#### Getting meta features from kagglehub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2aed9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_kaggle_dataset(path: str) -> list:\n",
    "    \"\"\"\n",
    "    Load datasets from a Kaggle dataset path.\n",
    "    Returns a list of loaded datasets in the same format as load_ucimldatasets.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the downloaded Kaggle dataset directory\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing dataset information\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading datasets from Kaggle path: {path}\")\n",
    "    \n",
    "    loaded_datasets = []\n",
    "    successful_loads = 0\n",
    "    failed_loads = []\n",
    "    processed_files = set()  # Track processed files to avoid duplicates\n",
    "    \n",
    "    # Look for CSV files in the path\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in the specified path\")\n",
    "        return loaded_datasets\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    for idx, csv_file in enumerate(csv_files):\n",
    "        filename = os.path.basename(csv_file)\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if filename in processed_files:\n",
    "            continue\n",
    "        processed_files.add(filename)\n",
    "        \n",
    "        try:\n",
    "            print(f\"→ Loading {filename}...\", end=\" \", flush=True)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(f\"[Skipped: Empty dataset]\")\n",
    "                failed_loads.append((filename, \"Empty dataset\"))\n",
    "                continue\n",
    "            \n",
    "            # Check for missing values and clean the data\n",
    "            initial_shape = df.shape\n",
    "            print(f\"[Initial shape: {initial_shape}]\", end=\" \")\n",
    "            \n",
    "            # Remove rows with too many missing values (more than 50% missing)\n",
    "            missing_threshold = 0.5\n",
    "            df = df.dropna(thresh=int(missing_threshold * len(df.columns)))\n",
    "            \n",
    "            # Remove columns with too many missing values (more than 50% missing)\n",
    "            df = df.dropna(axis=1, thresh=int(missing_threshold * len(df)))\n",
    "            \n",
    "            if df.empty:\n",
    "                print(f\"[Skipped: Too many missing values]\")\n",
    "                failed_loads.append((filename, \"Too many missing values after cleaning\"))\n",
    "                continue\n",
    "            \n",
    "            # For remaining missing values, use different strategies for numeric vs categorical\n",
    "            for col in df.columns:\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    # For numeric columns, fill with median\n",
    "                    if df[col].isnull().any():\n",
    "                        df[col] = df[col].fillna(df[col].median())\n",
    "                else:\n",
    "                    # For categorical columns, fill with mode\n",
    "                    if df[col].isnull().any():\n",
    "                        mode_val = df[col].mode()\n",
    "                        if len(mode_val) > 0:\n",
    "                            df[col] = df[col].fillna(mode_val[0])\n",
    "                        else:\n",
    "                            df[col] = df[col].fillna('unknown')\n",
    "            \n",
    "            # Final check: drop any remaining rows with NaN\n",
    "            df = df.dropna()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(f\"[Skipped: No data left after cleaning]\")\n",
    "                failed_loads.append((filename, \"No data left after cleaning\"))\n",
    "                continue\n",
    "            \n",
    "            print(f\"[Cleaned: {initial_shape} -> {df.shape}]\", end=\" \")\n",
    "            \n",
    "            # Assume the last column is the target (you might need to adjust this)\n",
    "            # or look for common target column names\n",
    "            target_candidates = ['target', 'y', 'label', 'class', 'output']\n",
    "            target_col = None\n",
    "            \n",
    "            # Check if any of the common target names exist\n",
    "            for candidate in target_candidates:\n",
    "                if candidate in df.columns:\n",
    "                    target_col = candidate\n",
    "                    break\n",
    "            \n",
    "            # If no common target name found, use the last column\n",
    "            if target_col is None:\n",
    "                target_col = df.columns[-1]\n",
    "            \n",
    "            # Split features and target\n",
    "            X = df.drop(columns=[target_col])\n",
    "            y = df[target_col]\n",
    "            \n",
    "            # Check if it's a regression task (numeric target)\n",
    "            if not pd.api.types.is_numeric_dtype(y):\n",
    "                print(f\"[Skipped: Non-numeric target '{target_col}']\")\n",
    "                failed_loads.append((filename, f\"Non-numeric target '{target_col}'\"))\n",
    "                continue\n",
    "            \n",
    "            # Additional robust NaN check for target\n",
    "            if y.isnull().any():\n",
    "                print(f\"[Skipped: Target variable still contains NaN]\")\n",
    "                failed_loads.append((filename, \"Target variable contains NaN\"))\n",
    "                continue\n",
    "            \n",
    "            # Additional check: ensure no infinite values\n",
    "            if np.isinf(X.select_dtypes(include=[np.number])).any().any() or np.isinf(y).any():\n",
    "                print(f\"[Skipped: Contains infinite values]\")\n",
    "                failed_loads.append((filename, \"Contains infinite values\"))\n",
    "                continue\n",
    "            \n",
    "            # Final NaN check for features\n",
    "            if X.isnull().any().any():\n",
    "                print(f\"[Skipped: Features still contain NaN after cleaning]\")\n",
    "                failed_loads.append((filename, \"Features still contain NaN after cleaning\"))\n",
    "                continue\n",
    "            \n",
    "            # Convert categorical columns to numeric using label encoding\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            for col in X.columns:\n",
    "                if not pd.api.types.is_numeric_dtype(X[col]):\n",
    "                    le = LabelEncoder()\n",
    "                    X[col] = le.fit_transform(X[col].astype(str))\n",
    "            \n",
    "            # Create categorical indicator (True for originally non-numeric columns)\n",
    "            categorical_indicator = []\n",
    "            for col in df.drop(columns=[target_col]).columns:\n",
    "                is_categorical = not pd.api.types.is_numeric_dtype(df[col])\n",
    "                categorical_indicator.append(is_categorical)\n",
    "            \n",
    "            # Get attribute names\n",
    "            attribute_names = list(X.columns)\n",
    "            \n",
    "            # Create dataset name from filename\n",
    "            dataset_name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            # Add to loaded datasets\n",
    "            loaded_datasets.append({\n",
    "                \"X\": X,\n",
    "                \"y\": y,\n",
    "                \"categorical_indicator\": categorical_indicator,\n",
    "                \"attribute_names\": attribute_names,\n",
    "                \"dataset_id\": idx,  # Use index as ID for Kaggle datasets\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"source\": \"Kaggle\",\n",
    "                \"file_path\": csv_file\n",
    "            })\n",
    "            \n",
    "            successful_loads += 1\n",
    "            print(f\"✓ Shape: {X.shape}, Target: {y.shape}, Target column: '{target_col}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            failed_loads.append((filename, str(e)))\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nKaggle Dataset Loading Summary:\")\n",
    "    print(f\"• Successfully loaded: {successful_loads}/{len(set(os.path.basename(f) for f in csv_files))} unique datasets\")\n",
    "    print(f\"• Failed to load: {len(failed_loads)} datasets\")\n",
    "    \n",
    "    if failed_loads:\n",
    "        print(\"• Failed files and reasons:\")\n",
    "        for filename, reason in failed_loads:\n",
    "            print(f\"  - {filename}: {reason}\")\n",
    "    \n",
    "    if loaded_datasets:\n",
    "        print(\"• Successfully loaded dataset details:\")\n",
    "        for ds in loaded_datasets:\n",
    "            print(f\"  - {ds['dataset_name']}: {ds['X'].shape} features, {len(ds['y'])} samples\")\n",
    "    \n",
    "    return loaded_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077b6478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from Kaggle path: /Users/surbhi/.cache/kagglehub/datasets/sohier/calcofi/versions/2\n",
      "Found 2 CSV files\n",
      "→ Loading bottle.csv... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/h4zsfxwx0mn93x09xdcb1qyh0000gn/T/ipykernel_43717/2826108057.py:46: DtypeWarning: Columns (47,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Initial shape: (864863, 74)] [Cleaned: (864863, 74) -> (405546, 42)] [Cleaned: (864863, 74) -> (405546, 42)] ✓ Shape: (405546, 41), Target: (405546,), Target column: 'R_PRES'\n",
      "→ Loading cast.csv... ✓ Shape: (405546, 41), Target: (405546,), Target column: 'R_PRES'\n",
      "→ Loading cast.csv... [Initial shape: (34404, 61)] [Initial shape: (34404, 61)] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/h4zsfxwx0mn93x09xdcb1qyh0000gn/T/ipykernel_43717/2826108057.py:46: DtypeWarning: Columns (40,41,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cleaned: (34404, 61) -> (34404, 44)] ✓ Shape: (34404, 43), Target: (34404,), Target column: 'Wea'\n",
      "\n",
      "Kaggle Dataset Loading Summary:\n",
      "• Successfully loaded: 2/2 unique datasets\n",
      "• Failed to load: 0 datasets\n",
      "• Successfully loaded dataset details:\n",
      "  - bottle: (405546, 41) features, 405546 samples\n",
      "  - cast: (34404, 43) features, 34404 samples\n",
      "\n",
      "Loaded 2 Kaggle datasets successfully!\n",
      "→ Processing dataset 1/2 (ID: 0)\n",
      "   • Dataset shape: (405546, 41), target shape: (405546,)\n",
      "✓ Shape: (34404, 43), Target: (34404,), Target column: 'Wea'\n",
      "\n",
      "Kaggle Dataset Loading Summary:\n",
      "• Successfully loaded: 2/2 unique datasets\n",
      "• Failed to load: 0 datasets\n",
      "• Successfully loaded dataset details:\n",
      "  - bottle: (405546, 41) features, 405546 samples\n",
      "  - cast: (34404, 43) features, 34404 samples\n",
      "\n",
      "Loaded 2 Kaggle datasets successfully!\n",
      "→ Processing dataset 1/2 (ID: 0)\n",
      "   • Dataset shape: (405546, 41), target shape: (405546,)\n",
      "   • Train shape:         Cst_Cnt  Btl_Cnt  Sta_ID  Depth_ID  Depthm  T_degC  Salnty  O2ml_L  \\\n",
      "698615    28342   698616     743    244346     269    7.33  34.012    2.68   \n",
      "664281    27154   664282     969    210196      84   15.19  33.415    5.82   \n",
      "589294    24057   589295     838    146810     250    8.00  34.019    2.74   \n",
      "623183    25442   623184     953    174752     400    6.44  34.279    0.48   \n",
      "729839    29383   729840     652    273996      49   12.57  33.490    5.85   \n",
      "\n",
      "        STheta  O2Sat  ...  R_SVA  R_DYNHT  R_O2  R_O2Sat  R_SIO3  R_PO4  \\\n",
      "698615  26.599   39.7  ...  147.4     0.66  2.68     39.7    43.8   2.24   \n",
      "664281  24.712  101.7  ...  324.7     0.32  5.82    101.7     2.3   0.39   \n",
      "589294  26.507   41.2  ...  156.1     0.57  2.74     41.2    39.4   2.14   \n",
      "623183  26.931    7.0  ...  117.4     0.68  0.48      7.0    73.8   3.06   \n",
      "729839  25.309   96.9  ...  266.6     0.13  5.85     96.9     3.9   0.67   \n",
      "\n",
      "        R_NO3  R_NO2  R_CHLA  R_PHAEO  \n",
      "698615   30.3   0.01    0.16     0.11  \n",
      "664281    0.0   0.00    0.24     0.15  \n",
      "589294   28.5   0.01    0.16     0.11  \n",
      "623183   39.3   0.00    0.16     0.11  \n",
      "729839    4.6   0.29    0.31     0.22  \n",
      "\n",
      "[5 rows x 41 columns], Test shape:         Cst_Cnt  Btl_Cnt  Sta_ID  Depth_ID  Depthm  T_degC  Salnty  O2ml_L  \\\n",
      "524641    21262   524642     862     93201       0   17.11  33.409    3.82   \n",
      "827227    32941   827228     934    367847      75   10.44  33.607    3.30   \n",
      "684130    27860   684131     962    229943     437    6.09  34.156    0.89   \n",
      "829079    33007   829080     953    370924       2   13.50  33.196    6.05   \n",
      "798040    31821   798041     834    340354      20   17.89  33.543    5.70   \n",
      "\n",
      "        STheta   O2Sat  ...  R_SVA  R_DYNHT  R_O2  R_O2Sat  R_SIO3  R_PO4  \\\n",
      "524641  24.268   60.05  ...  364.5    0.000  3.82     60.1     0.0   0.39   \n",
      "827227  25.791   52.30  ...  221.2    0.202  3.30     52.3    20.6   1.72   \n",
      "684130  26.879   12.80  ...  122.4    0.920  0.89     12.8    72.1   2.95   \n",
      "829079  24.896  102.00  ...  304.7    0.006  6.05    102.0     3.6   0.33   \n",
      "798040  24.185  105.10  ...  373.1    0.075  5.70    105.1     0.4   0.24   \n",
      "\n",
      "        R_NO3  R_NO2  R_CHLA  R_PHAEO  \n",
      "524641    0.1   0.00    0.07     0.01  \n",
      "827227   20.5   0.01    0.03     0.07  \n",
      "684130   37.6   0.00    0.16     0.11  \n",
      "829079    0.2   0.05    0.31     0.07  \n",
      "798040    0.0   0.00    0.29     0.05  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "============= Preprocessor built inside meta_trainer.py =============\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (405546, 41)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (405546,)\n",
      "Building preprocessor...\n",
      "Fitting preprocessor...\n",
      "X_processed shape before : (405546, 41)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      " • Set sparse_threshold=0 to force dense output\n",
      " • Configuring ColumnTransformer components...\n",
      "   - Disabled centering for robust_scaler\n",
      "   • Train shape:         Cst_Cnt  Btl_Cnt  Sta_ID  Depth_ID  Depthm  T_degC  Salnty  O2ml_L  \\\n",
      "698615    28342   698616     743    244346     269    7.33  34.012    2.68   \n",
      "664281    27154   664282     969    210196      84   15.19  33.415    5.82   \n",
      "589294    24057   589295     838    146810     250    8.00  34.019    2.74   \n",
      "623183    25442   623184     953    174752     400    6.44  34.279    0.48   \n",
      "729839    29383   729840     652    273996      49   12.57  33.490    5.85   \n",
      "\n",
      "        STheta  O2Sat  ...  R_SVA  R_DYNHT  R_O2  R_O2Sat  R_SIO3  R_PO4  \\\n",
      "698615  26.599   39.7  ...  147.4     0.66  2.68     39.7    43.8   2.24   \n",
      "664281  24.712  101.7  ...  324.7     0.32  5.82    101.7     2.3   0.39   \n",
      "589294  26.507   41.2  ...  156.1     0.57  2.74     41.2    39.4   2.14   \n",
      "623183  26.931    7.0  ...  117.4     0.68  0.48      7.0    73.8   3.06   \n",
      "729839  25.309   96.9  ...  266.6     0.13  5.85     96.9     3.9   0.67   \n",
      "\n",
      "        R_NO3  R_NO2  R_CHLA  R_PHAEO  \n",
      "698615   30.3   0.01    0.16     0.11  \n",
      "664281    0.0   0.00    0.24     0.15  \n",
      "589294   28.5   0.01    0.16     0.11  \n",
      "623183   39.3   0.00    0.16     0.11  \n",
      "729839    4.6   0.29    0.31     0.22  \n",
      "\n",
      "[5 rows x 41 columns], Test shape:         Cst_Cnt  Btl_Cnt  Sta_ID  Depth_ID  Depthm  T_degC  Salnty  O2ml_L  \\\n",
      "524641    21262   524642     862     93201       0   17.11  33.409    3.82   \n",
      "827227    32941   827228     934    367847      75   10.44  33.607    3.30   \n",
      "684130    27860   684131     962    229943     437    6.09  34.156    0.89   \n",
      "829079    33007   829080     953    370924       2   13.50  33.196    6.05   \n",
      "798040    31821   798041     834    340354      20   17.89  33.543    5.70   \n",
      "\n",
      "        STheta   O2Sat  ...  R_SVA  R_DYNHT  R_O2  R_O2Sat  R_SIO3  R_PO4  \\\n",
      "524641  24.268   60.05  ...  364.5    0.000  3.82     60.1     0.0   0.39   \n",
      "827227  25.791   52.30  ...  221.2    0.202  3.30     52.3    20.6   1.72   \n",
      "684130  26.879   12.80  ...  122.4    0.920  0.89     12.8    72.1   2.95   \n",
      "829079  24.896  102.00  ...  304.7    0.006  6.05    102.0     3.6   0.33   \n",
      "798040  24.185  105.10  ...  373.1    0.075  5.70    105.1     0.4   0.24   \n",
      "\n",
      "        R_NO3  R_NO2  R_CHLA  R_PHAEO  \n",
      "524641    0.1   0.00    0.07     0.01  \n",
      "827227   20.5   0.01    0.03     0.07  \n",
      "684130   37.6   0.00    0.16     0.11  \n",
      "829079    0.2   0.05    0.31     0.07  \n",
      "798040    0.0   0.00    0.29     0.05  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "============= Preprocessor built inside meta_trainer.py =============\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (405546, 41)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (405546,)\n",
      "Building preprocessor...\n",
      "Fitting preprocessor...\n",
      "X_processed shape before : (405546, 41)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      " • Set sparse_threshold=0 to force dense output\n",
      " • Configuring ColumnTransformer components...\n",
      "   - Disabled centering for robust_scaler\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (405546, 41)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (405546, 41)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (405546, 41)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 41 features...\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (405546, 41)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (405546, 41)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (405546, 41)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 41 features...\n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (283882, 41)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (121664, 41)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (283882, 41)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (121664, 41)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      "evaluating simple rule model \n",
      "evaluating simple rule model \n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2838, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 201.634249\n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2838, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 201.634249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Error evaluating TabPFNRegressor on 1% data: Invalid buffer size: 46.31 GB ========== \n",
      "============== Meta-features extracted ==============\n",
      "   • Training LGBMRegressor... [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8033\n",
      "[LightGBM] [Info] Number of data points in the train set: 283882, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 199.663638\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8033\n",
      "[LightGBM] [Info] Number of data points in the train set: 283882, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 199.663638\n",
      "R²=0.9991\n",
      "   • Training XGBRegressor... R²=0.9991\n",
      "   • Training XGBRegressor... R²=0.9980\n",
      "   • Training RandomForestRegressor... R²=0.9980\n",
      "   • Training RandomForestRegressor... R²=1.0000\n",
      "   • Training DecisionTreeRegressor... R²=1.0000\n",
      "   • Training DecisionTreeRegressor... R²=1.0000\n",
      "   • Training HistGradientBoostingRegressor... R²=1.0000\n",
      "   • Training HistGradientBoostingRegressor... R²=0.9976\n",
      "   • Training GradientBoostingRegressor... R²=0.9976\n",
      "   • Training GradientBoostingRegressor... R²=0.9999\n",
      "   • Training MLPRegressor... R²=0.9999\n",
      "   • Training MLPRegressor... R²=0.9999\n",
      "   • Training BayesianRidge... R²=0.9999\n",
      "   • Training BayesianRidge... R²=1.0000\n",
      "   • Training LinearRegression... R²=1.0000\n",
      "   • Training LinearRegression... R²=1.0000\n",
      "   • Training TabPFNRegressor... R²=1.0000\n",
      "   • Training TabPFNRegressor... [Error: Number of samples 283882 in the input data is greater than the maximum number of samples 10000 officially supported by TabPFN. Set `ignore_pretraining_limits=True` to override this error!]\n",
      "R²=-0.0000\n",
      "→ Processing dataset 2/2 (ID: 1)\n",
      "   • Dataset shape: (34404, 43), target shape: (34404,)\n",
      "   • Train shape:        Cst_Cnt  Cruise_ID  Cruise        Cruz_Sta  DbSta_ID  Cast_ID  Sta_ID  \\\n",
      "5003      5004        126  195210  19521006000550   6000550     5003     432   \n",
      "27735    27736        546  199408  19940809000800   9000800    27731    1187   \n",
      "33813    33814        637  201501  20150108850301   8850301    33769    1102   \n",
      "33928    33929        639  201507  20150708000900   8000900    33960     811   \n",
      "27287    27288        539  199210  19921007670600   7670600    27276     714   \n",
      "\n",
      "       Quarter  Sta_Code  Distance  ...  Data_Type  Event_Num  Orig_Sta_ID  \\\n",
      "5003         4         4    -24.42  ...          2     1641.0          299   \n",
      "27735        3         6   -209.20  ...          3      192.0         3408   \n",
      "33813        1         6     -2.76  ...          3      402.0         3305   \n",
      "33928        3         6   -158.05  ...          3      703.0         2980   \n",
      "27287        4         6    -50.36  ...          2      712.0         2809   \n",
      "\n",
      "       Data_Or  Cruz_Num  Wind_Dir  Wind_Spd  Barometer  Dry_T  Wet_T  \n",
      "5003         0       110      32.0       8.0     1016.3   16.1   14.2  \n",
      "27735        0       344      34.0      15.0     1015.0   16.9   16.0  \n",
      "33813        0        61      34.0       3.0     1018.0   16.6   14.9  \n",
      "33928        0        63       3.0      18.0     1013.0   18.9   18.3  \n",
      "27287        0       337      33.0      15.0     1011.5   16.5   15.7  \n",
      "\n",
      "[5 rows x 43 columns], Test shape:        Cst_Cnt  Cruise_ID  Cruise        Cruz_Sta  DbSta_ID  Cast_ID  Sta_ID  \\\n",
      "15301    15302        390  196712  19671211330700  11330700    15301    1979   \n",
      "23451    23452        499  198406  19840610670600  10670600    23456    1800   \n",
      "11137    11138        305  195908  19590813330500  13330500    11136    2416   \n",
      "19269    19270        432  197412  19741208330700   8330700    19209     918   \n",
      "11494    11495        312  195911  19591108670400   8670400    11494    1026   \n",
      "\n",
      "       Quarter  Sta_Code  Distance  ...  Data_Type  Event_Num  Orig_Sta_ID  \\\n",
      "15301        4         2    -83.57  ...          2     2018.0         1400   \n",
      "23451        2         0    -83.57  ...          2     4434.0         1224   \n",
      "11137        3         3    -83.57  ...          2     1847.0         1856   \n",
      "19269        4         6   -122.75  ...          1     2735.0         3082   \n",
      "11494        4         6    -30.57  ...          2     2207.0          738   \n",
      "\n",
      "       Data_Or  Cruz_Num  Wind_Dir  Wind_Spd  Barometer  Dry_T  Wet_T  \n",
      "15301        0       229      32.0      32.0     1029.8   17.1   13.3  \n",
      "23451        0       302      31.0      19.0     1012.0   16.0   15.0  \n",
      "11137        0       180      34.0      13.0     1016.3   16.1   14.2  \n",
      "19269        0       257      36.0       9.0     1002.0   14.8   11.5  \n",
      "11494        0       183      32.0       5.0     1016.3   16.1   14.2  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "============= Preprocessor built inside meta_trainer.py =============\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (34404, 43)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (34404,)\n",
      "Building preprocessor...\n",
      "Fitting preprocessor...\n",
      "X_processed shape before : (34404, 43)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      " • Set sparse_threshold=0 to force dense output\n",
      " • Configuring ColumnTransformer components...\n",
      "   - Disabled centering for robust_scaler\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (34404, 43)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (34404, 43)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (34404, 43)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 43 features...\n",
      "[Error: Number of samples 283882 in the input data is greater than the maximum number of samples 10000 officially supported by TabPFN. Set `ignore_pretraining_limits=True` to override this error!]\n",
      "R²=-0.0000\n",
      "→ Processing dataset 2/2 (ID: 1)\n",
      "   • Dataset shape: (34404, 43), target shape: (34404,)\n",
      "   • Train shape:        Cst_Cnt  Cruise_ID  Cruise        Cruz_Sta  DbSta_ID  Cast_ID  Sta_ID  \\\n",
      "5003      5004        126  195210  19521006000550   6000550     5003     432   \n",
      "27735    27736        546  199408  19940809000800   9000800    27731    1187   \n",
      "33813    33814        637  201501  20150108850301   8850301    33769    1102   \n",
      "33928    33929        639  201507  20150708000900   8000900    33960     811   \n",
      "27287    27288        539  199210  19921007670600   7670600    27276     714   \n",
      "\n",
      "       Quarter  Sta_Code  Distance  ...  Data_Type  Event_Num  Orig_Sta_ID  \\\n",
      "5003         4         4    -24.42  ...          2     1641.0          299   \n",
      "27735        3         6   -209.20  ...          3      192.0         3408   \n",
      "33813        1         6     -2.76  ...          3      402.0         3305   \n",
      "33928        3         6   -158.05  ...          3      703.0         2980   \n",
      "27287        4         6    -50.36  ...          2      712.0         2809   \n",
      "\n",
      "       Data_Or  Cruz_Num  Wind_Dir  Wind_Spd  Barometer  Dry_T  Wet_T  \n",
      "5003         0       110      32.0       8.0     1016.3   16.1   14.2  \n",
      "27735        0       344      34.0      15.0     1015.0   16.9   16.0  \n",
      "33813        0        61      34.0       3.0     1018.0   16.6   14.9  \n",
      "33928        0        63       3.0      18.0     1013.0   18.9   18.3  \n",
      "27287        0       337      33.0      15.0     1011.5   16.5   15.7  \n",
      "\n",
      "[5 rows x 43 columns], Test shape:        Cst_Cnt  Cruise_ID  Cruise        Cruz_Sta  DbSta_ID  Cast_ID  Sta_ID  \\\n",
      "15301    15302        390  196712  19671211330700  11330700    15301    1979   \n",
      "23451    23452        499  198406  19840610670600  10670600    23456    1800   \n",
      "11137    11138        305  195908  19590813330500  13330500    11136    2416   \n",
      "19269    19270        432  197412  19741208330700   8330700    19209     918   \n",
      "11494    11495        312  195911  19591108670400   8670400    11494    1026   \n",
      "\n",
      "       Quarter  Sta_Code  Distance  ...  Data_Type  Event_Num  Orig_Sta_ID  \\\n",
      "15301        4         2    -83.57  ...          2     2018.0         1400   \n",
      "23451        2         0    -83.57  ...          2     4434.0         1224   \n",
      "11137        3         3    -83.57  ...          2     1847.0         1856   \n",
      "19269        4         6   -122.75  ...          1     2735.0         3082   \n",
      "11494        4         6    -30.57  ...          2     2207.0          738   \n",
      "\n",
      "       Data_Or  Cruz_Num  Wind_Dir  Wind_Spd  Barometer  Dry_T  Wet_T  \n",
      "15301        0       229      32.0      32.0     1029.8   17.1   13.3  \n",
      "23451        0       302      31.0      19.0     1012.0   16.0   15.0  \n",
      "11137        0       180      34.0      13.0     1016.3   16.1   14.2  \n",
      "19269        0       257      36.0       9.0     1002.0   14.8   11.5  \n",
      "11494        0       183      32.0       5.0     1016.3   16.1   14.2  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "============= Preprocessor built inside meta_trainer.py =============\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (34404, 43)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (34404,)\n",
      "Building preprocessor...\n",
      "Fitting preprocessor...\n",
      "X_processed shape before : (34404, 43)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      " • Set sparse_threshold=0 to force dense output\n",
      " • Configuring ColumnTransformer components...\n",
      "   - Disabled centering for robust_scaler\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (34404, 43)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (34404, 43)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (34404, 43)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 43 features...\n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (24082, 43)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (10322, 43)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      "evaluating simple rule model \n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2008\n",
      "[LightGBM] [Info] Number of data points in the train set: 240, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 1.312500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (24082, 43)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (10322, 43)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      "evaluating simple rule model \n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2008\n",
      "[LightGBM] [Info] Number of data points in the train set: 240, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 1.312500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Error evaluating TabPFNRegressor on 1% data: MPS backend out of memory (MPS allocated: 18.09 GB, other allocations: 20.84 MB, max allowed: 18.13 GB). Tried to allocate 234.36 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure). ========== \n",
      "============== Meta-features extracted ==============\n",
      "   • Training LGBMRegressor... [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7413\n",
      "[LightGBM] [Info] Number of data points in the train set: 24082, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.255336\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7413\n",
      "[LightGBM] [Info] Number of data points in the train set: 24082, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.255336\n",
      "R²=0.1836\n",
      "   • Training XGBRegressor... R²=0.1836\n",
      "   • Training XGBRegressor... R²=0.1920\n",
      "   • Training RandomForestRegressor... R²=0.1920\n",
      "   • Training RandomForestRegressor... R²=0.2197\n",
      "   • Training DecisionTreeRegressor... R²=0.2197\n",
      "   • Training DecisionTreeRegressor... R²=-0.5382\n",
      "   • Training HistGradientBoostingRegressor... R²=-0.5382\n",
      "   • Training HistGradientBoostingRegressor... R²=0.1835\n",
      "   • Training GradientBoostingRegressor... R²=0.1835\n",
      "   • Training GradientBoostingRegressor... R²=0.1209\n",
      "   • Training MLPRegressor... R²=0.1209\n",
      "   • Training MLPRegressor... R²=-0.0601\n",
      "   • Training BayesianRidge... R²=-0.0601\n",
      "   • Training BayesianRidge... R²=0.0365\n",
      "   • Training LinearRegression... R²=0.0365\n",
      "   • Training LinearRegression... R²=0.0361\n",
      "   • Training TabPFNRegressor... R²=0.0361\n",
      "   • Training TabPFNRegressor... [Error: Number of samples 24082 in the input data is greater than the maximum number of samples 10000 officially supported by TabPFN. Set `ignore_pretraining_limits=True` to override this error!]\n",
      "R²=-0.0001\n",
      "[Error: Number of samples 24082 in the input data is greater than the maximum number of samples 10000 officially supported by TabPFN. Set `ignore_pretraining_limits=True` to override this error!]\n",
      "R²=-0.0001\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sohier/calcofi\")\n",
    "\n",
    "# Load the Kaggle dataset\n",
    "kaggle_datasets = load_kaggle_dataset(path)\n",
    "\n",
    "if kaggle_datasets:\n",
    "    print(f\"\\nLoaded {len(kaggle_datasets)} Kaggle datasets successfully!\")\n",
    "    records = algorithms_eval(algorithms=algorithms, datasets=kaggle_datasets)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(\"meta_features_kaggle_1.csv\", index=False)\n",
    "else:\n",
    "    print(\"No Kaggle datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005f3f5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KaggleApiHTTPError",
     "evalue": "403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/datasets/sbudincsevity/szeged-weather. The server reported the following issues: Permission 'datasets.get' was denied\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/kagglehub/exceptions.py:66\u001b[39m, in \u001b[36mkaggle_api_raise_for_status\u001b[39m\u001b[34m(response, resource_handle)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/view/sbudincsevity/szeged-weather",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKaggleApiHTTPError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m path2 = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msbudincsevity/szeged-weather\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the Kaggle dataset\u001b[39;00m\n\u001b[32m      5\u001b[39m kaggle_datasets2 = load_kaggle_dataset(path2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/kagglehub/datasets.py:43\u001b[39m, in \u001b[36mdataset_download\u001b[39m\u001b[34m(handle, path, force_download)\u001b[39m\n\u001b[32m     41\u001b[39m h = parse_dataset_handle(handle)\n\u001b[32m     42\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh.to_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, extra={**EXTRA_CONSOLE_BLOCK})\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m path, _ = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/kagglehub/registry.py:28\u001b[39m, in \u001b[36mMultiImplRegistry.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m._impls):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m impl.is_supported(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m         fails.append(\u001b[38;5;28mtype\u001b[39m(impl).\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/kagglehub/resolver.py:29\u001b[39m, in \u001b[36mResolver.__call__\u001b[39m\u001b[34m(self, handle, path, force_download)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, *, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     path, version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[32m     32\u001b[39m     register_datasource_access(handle, version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/kagglehub/http_resolver.py:107\u001b[39m, in \u001b[36mDatasetHttpResolver._resolve\u001b[39m\u001b[34m(self, h, path, force_download)\u001b[39m\n\u001b[32m    104\u001b[39m api_client = KaggleApiV1Client()\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m h.is_versioned():\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     h = h.with_version(\u001b[43m_get_current_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    109\u001b[39m dataset_path = load_from_cache(h, path)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset_path \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_download:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/kagglehub/http_resolver.py:290\u001b[39m, in \u001b[36m_get_current_version\u001b[39m\u001b[34m(api_client, h)\u001b[39m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json_response[MODEL_INSTANCE_VERSION_FIELD]\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(h, DatasetHandle):\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     json_response = \u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_build_get_dataset_url_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m DATASET_CURRENT_VERSION_FIELD \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n\u001b[32m    292\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid GetDataset API response. Expected to include a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_CURRENT_VERSION_FIELD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m field\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/kagglehub/clients.py:139\u001b[39m, in \u001b[36mKaggleApiV1Client.get\u001b[39m\u001b[34m(self, path, resource_handle)\u001b[39m\n\u001b[32m    132\u001b[39m url = \u001b[38;5;28mself\u001b[39m._build_url(path)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m requests.get(\n\u001b[32m    134\u001b[39m     url,\n\u001b[32m    135\u001b[39m     headers={\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: get_user_agent()},\n\u001b[32m    136\u001b[39m     auth=\u001b[38;5;28mself\u001b[39m._get_auth(),\n\u001b[32m    137\u001b[39m     timeout=(DEFAULT_CONNECT_TIMEOUT, DEFAULT_READ_TIMEOUT),\n\u001b[32m    138\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[43mkaggle_api_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_for_version_update(response)\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/kagglehub/exceptions.py:106\u001b[39m, in \u001b[36mkaggle_api_raise_for_status\u001b[39m\u001b[34m(response, resource_handle)\u001b[39m\n\u001b[32m     97\u001b[39m     message = (\n\u001b[32m     98\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease make sure you specified the correct resource identifiers.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    103\u001b[39m     )\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Default handling\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m KaggleApiHTTPError(message, response=response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKaggleApiHTTPError\u001b[39m: 403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/datasets/sbudincsevity/szeged-weather. The server reported the following issues: Permission 'datasets.get' was denied\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path2 = kagglehub.dataset_download(\"sbudincsevity/szeged-weather\")\n",
    "\n",
    "# Load the Kaggle dataset\n",
    "kaggle_datasets2 = load_kaggle_dataset(path2)\n",
    "\n",
    "if kaggle_datasets2:\n",
    "    print(f\"\\nLoaded {len(kaggle_datasets2)} Kaggle datasets successfully!\")\n",
    "    records = algorithms_eval(algorithms=algorithms, datasets=kaggle_datasets2)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(\"meta_features_kaggle_2.csv\", index=False)\n",
    "else:\n",
    "    print(\"No Kaggle datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06234e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/smid80/weatherww2?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.46MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Loading datasets from Kaggle path: /Users/surbhi/.cache/kagglehub/datasets/smid80/weatherww2/versions/1\n",
      "Found 2 CSV files\n",
      "→ Loading Weather Station Locations.csv... Loading datasets from Kaggle path: /Users/surbhi/.cache/kagglehub/datasets/smid80/weatherww2/versions/1\n",
      "Found 2 CSV files\n",
      "→ Loading Weather Station Locations.csv... [Initial shape: (161, 8)] [Cleaned: (161, 8) -> (161, 8)] ✓ Shape: (161, 7), Target: (161,), Target column: 'Longitude'\n",
      "→ Loading Summary of Weather.csv... [Initial shape: (161, 8)] [Cleaned: (161, 8) -> (161, 8)] ✓ Shape: (161, 7), Target: (161,), Target column: 'Longitude'\n",
      "→ Loading Summary of Weather.csv... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/rz/h4zsfxwx0mn93x09xdcb1qyh0000gn/T/ipykernel_43717/2826108057.py:46: DtypeWarning: Columns (7,8,18,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "/var/folders/rz/h4zsfxwx0mn93x09xdcb1qyh0000gn/T/ipykernel_43717/2826108057.py:46: DtypeWarning: Columns (7,8,18,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Initial shape: (119040, 31)] [Cleaned: (119040, 31) -> (116293, 15)] [Skipped: Non-numeric target 'SNF']\n",
      "\n",
      "Kaggle Dataset Loading Summary:\n",
      "• Successfully loaded: 1/2 unique datasets\n",
      "• Failed to load: 1 datasets\n",
      "• Failed files and reasons:\n",
      "  - Summary of Weather.csv: Non-numeric target 'SNF'\n",
      "• Successfully loaded dataset details:\n",
      "  - Weather Station Locations: (161, 7) features, 161 samples\n",
      "\n",
      "Loaded 1 Kaggle datasets successfully!\n",
      "→ Processing dataset 1/1 (ID: 0)\n",
      "   • Dataset shape: (161, 7), target shape: (161,)\n",
      "   • Train shape:       WBAN  NAME  STATE/COUNTRY ID  LAT  LON  ELEV   Latitude\n",
      "67   32801    16                28   85  103    98  25.433333\n",
      "114  12101    12                40   61   31   235  20.466667\n",
      "11   81404    70                 2   34  129    12 -10.600000\n",
      "65   16202   129                27  154   46     8  64.133333\n",
      "85   33019    30                31  127   18     9  39.250000, Test shape:       WBAN  NAME  STATE/COUNTRY ID  LAT  LON  ELEV   Latitude\n",
      "105  11601   146                36   43   77     8  13.733333\n",
      "108  10102   101                37   13   25  9999   6.233333\n",
      "142  10802   124                52   21  100    14   7.733333\n",
      "55   10502    65                24   16   74    29   6.500000\n",
      "94   33117   117                31  125   34   114  38.116667\n",
      "============= Preprocessor built inside meta_trainer.py =============\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (161, 7)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (161,)\n",
      "Building preprocessor...\n",
      "Fitting preprocessor...\n",
      "X_processed shape before : (161, 7)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      " • Set sparse_threshold=0 to force dense output\n",
      " • Configuring ColumnTransformer components...\n",
      "   - Disabled centering for robust_scaler\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (161, 7)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (161, 7)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (161, 7)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 7 features...\n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (112, 7)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (49, 7)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      "evaluating simple rule model \n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 10, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 9.193334\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (161, 7)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (161, 7)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (161, 7)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 7 features...\n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (112, 7)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (49, 7)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      "evaluating simple rule model \n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 10, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 9.193334\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Meta-features extracted ==============\n",
      "   • Training LGBMRegressor... [LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 257\n",
      "[LightGBM] [Info] Number of data points in the train set: 112, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 13.076786\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "R²=0.2773\n",
      "   • Training XGBRegressor... [LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 257\n",
      "[LightGBM] [Info] Number of data points in the train set: 112, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 13.076786\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "R²=0.2773\n",
      "   • Training XGBRegressor... R²=0.7656\n",
      "   • Training RandomForestRegressor... R²=0.7656\n",
      "   • Training RandomForestRegressor... R²=0.6604\n",
      "   • Training DecisionTreeRegressor... R²=0.2341\n",
      "   • Training HistGradientBoostingRegressor... R²=0.6604\n",
      "   • Training DecisionTreeRegressor... R²=0.2341\n",
      "   • Training HistGradientBoostingRegressor... R²=0.3041\n",
      "   • Training GradientBoostingRegressor... R²=0.3041\n",
      "   • Training GradientBoostingRegressor... R²=0.7052\n",
      "   • Training MLPRegressor... R²=0.7052\n",
      "   • Training MLPRegressor... R²=0.0941\n",
      "   • Training BayesianRidge... R²=0.0941\n",
      "   • Training BayesianRidge... R²=0.2452\n",
      "   • Training LinearRegression... R²=0.2442\n",
      "   • Training TabPFNRegressor... R²=0.2452\n",
      "   • Training LinearRegression... R²=0.2442\n",
      "   • Training TabPFNRegressor... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²=0.9624\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path3 = kagglehub.dataset_download(\"smid80/weatherww2\")\n",
    "\n",
    "# Load the Kaggle dataset\n",
    "kaggle_datasets3 = load_kaggle_dataset(path3)\n",
    "\n",
    "if kaggle_datasets3:\n",
    "    print(f\"\\nLoaded {len(kaggle_datasets3)} Kaggle datasets successfully!\")\n",
    "    records = algorithms_eval(algorithms=algorithms, datasets=kaggle_datasets3)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(\"meta_features_kaggle_3.csv\", index=False)\n",
    "else:\n",
    "    print(\"No Kaggle datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1dbc16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/mirichoi0218/insurance?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16.0k/16.0k [00:00<00:00, 10.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Loading datasets from Kaggle path: /Users/surbhi/.cache/kagglehub/datasets/mirichoi0218/insurance/versions/1\n",
      "Found 1 CSV files\n",
      "→ Loading insurance.csv... Loading datasets from Kaggle path: /Users/surbhi/.cache/kagglehub/datasets/mirichoi0218/insurance/versions/1\n",
      "Found 1 CSV files\n",
      "→ Loading insurance.csv... [Initial shape: (1338, 7)] [Cleaned: (1338, 7) -> (1338, 7)] ✓ Shape: (1338, 6), Target: (1338,), Target column: 'charges'\n",
      "\n",
      "Kaggle Dataset Loading Summary:\n",
      "• Successfully loaded: 1/1 unique datasets\n",
      "• Failed to load: 0 datasets\n",
      "• Successfully loaded dataset details:\n",
      "  - insurance: (1338, 6) features, 1338 samples\n",
      "\n",
      "Loaded 1 Kaggle datasets successfully!\n",
      "→ Processing dataset 1/1 (ID: 0)\n",
      "   • Dataset shape: (1338, 6), target shape: (1338,)\n",
      "   • Train shape:      age  sex     bmi  children  smoker  region\n",
      "332   61    0  31.160         0       0       1\n",
      "355   46    1  27.600         0       0       3\n",
      "138   54    0  31.900         3       0       2\n",
      "381   55    1  30.685         0       1       0\n",
      "292   25    1  45.540         2       1       2, Test shape:       age  sex     bmi  children  smoker  region\n",
      "764    45    0  25.175         2       0       0\n",
      "887    36    0  30.020         0       0       1\n",
      "890    64    0  26.885         0       1       1\n",
      "1293   46    1  25.745         3       0       1\n",
      "259    19    1  31.920         0       1       1\n",
      "============= Preprocessor built inside meta_trainer.py =============\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (1338, 6)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (1338,)\n",
      "Building preprocessor...\n",
      "Fitting preprocessor...\n",
      "X_processed shape before : (1338, 6)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      " • Set sparse_threshold=0 to force dense output\n",
      " • Configuring ColumnTransformer components...\n",
      "   - Disabled centering for robust_scaler\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (1338, 6)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (1338, 6)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (1338, 6)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 6 features...\n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (936, 6)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (402, 6)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      "evaluating simple rule model \n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 10, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 15731.237817\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[Initial shape: (1338, 7)] [Cleaned: (1338, 7) -> (1338, 7)] ✓ Shape: (1338, 6), Target: (1338,), Target column: 'charges'\n",
      "\n",
      "Kaggle Dataset Loading Summary:\n",
      "• Successfully loaded: 1/1 unique datasets\n",
      "• Failed to load: 0 datasets\n",
      "• Successfully loaded dataset details:\n",
      "  - insurance: (1338, 6) features, 1338 samples\n",
      "\n",
      "Loaded 1 Kaggle datasets successfully!\n",
      "→ Processing dataset 1/1 (ID: 0)\n",
      "   • Dataset shape: (1338, 6), target shape: (1338,)\n",
      "   • Train shape:      age  sex     bmi  children  smoker  region\n",
      "332   61    0  31.160         0       0       1\n",
      "355   46    1  27.600         0       0       3\n",
      "138   54    0  31.900         3       0       2\n",
      "381   55    1  30.685         0       1       0\n",
      "292   25    1  45.540         2       1       2, Test shape:       age  sex     bmi  children  smoker  region\n",
      "764    45    0  25.175         2       0       0\n",
      "887    36    0  30.020         0       0       1\n",
      "890    64    0  26.885         0       1       1\n",
      "1293   46    1  25.745         3       0       1\n",
      "259    19    1  31.920         0       1       1\n",
      "============= Preprocessor built inside meta_trainer.py =============\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (1338, 6)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (1338,)\n",
      "Building preprocessor...\n",
      "Fitting preprocessor...\n",
      "X_processed shape before : (1338, 6)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      " • Set sparse_threshold=0 to force dense output\n",
      " • Configuring ColumnTransformer components...\n",
      "   - Disabled centering for robust_scaler\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (1338, 6)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (1338, 6)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (1338, 6)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 6 features...\n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (936, 6)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (402, 6)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      "evaluating simple rule model \n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 10, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 15731.237817\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Meta-features extracted ==============\n",
      "   • Training LGBMRegressor... [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 316\n",
      "[LightGBM] [Info] Number of data points in the train set: 936, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 13379.157302\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 316\n",
      "[LightGBM] [Info] Number of data points in the train set: 936, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 13379.157302\n",
      "R²=0.8559\n",
      "   • Training XGBRegressor... R²=0.8559\n",
      "   • Training XGBRegressor... R²=0.8385\n",
      "   • Training RandomForestRegressor... R²=0.8385\n",
      "   • Training RandomForestRegressor... R²=0.8528\n",
      "   • Training DecisionTreeRegressor... R²=0.8528\n",
      "   • Training DecisionTreeRegressor... R²=0.7145\n",
      "   • Training HistGradientBoostingRegressor... R²=0.7145\n",
      "   • Training HistGradientBoostingRegressor... R²=0.8575\n",
      "   • Training GradientBoostingRegressor... R²=0.8575\n",
      "   • Training GradientBoostingRegressor... R²=0.8693\n",
      "   • Training MLPRegressor... R²=0.8693\n",
      "   • Training MLPRegressor... R²=-1.0764\n",
      "   • Training BayesianRidge... R²=-1.0764\n",
      "   • Training BayesianRidge... R²=0.7694\n",
      "   • Training LinearRegression... R²=0.7694\n",
      "   • Training TabPFNRegressor... R²=0.7694\n",
      "   • Training LinearRegression... R²=0.7694\n",
      "   • Training TabPFNRegressor... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²=0.8721\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path4 = kagglehub.dataset_download(\"mirichoi0218/insurance\")\n",
    "\n",
    "# Load the Kaggle dataset\n",
    "kaggle_datasets4 = load_kaggle_dataset(path4)\n",
    "\n",
    "if kaggle_datasets4:\n",
    "    print(f\"\\nLoaded {len(kaggle_datasets4)} Kaggle datasets successfully!\")\n",
    "    records = algorithms_eval(algorithms=algorithms, datasets=kaggle_datasets4)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(\"meta_features_kaggle_4.csv\", index=False)\n",
    "else:\n",
    "    print(\"No Kaggle datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45f6dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from Kaggle path: /Users/surbhi/.cache/kagglehub/datasets/harlfoxem/housesalesprediction/versions/1\n",
      "Found 1 CSV files\n",
      "→ Loading kc_house_data.csv... [Initial shape: (21613, 21)] [Cleaned: (21613, 21) -> (21613, 21)] ✓ Shape: (21613, 20), Target: (21613,), Target column: 'sqft_lot15'\n",
      "\n",
      "Kaggle Dataset Loading Summary:\n",
      "• Successfully loaded: 1/1 unique datasets\n",
      "• Failed to load: 0 datasets\n",
      "• Successfully loaded dataset details:\n",
      "  - kc_house_data: (21613, 20) features, 21613 samples\n",
      "\n",
      "Loaded 1 Kaggle datasets successfully!\n",
      "→ Processing dataset 1/1 (ID: 0)\n",
      "   • Dataset shape: (21613, 20), target shape: (21613,)\n",
      "   • Train shape:                id  date     price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
      "167    1836980160   317  807100.0         4       2.50         2680      4499   \n",
      "12412  9221400335   152  570000.0         4       1.75         2340      5080   \n",
      "7691   6669020490   102  320000.0         4       2.25         2190      9020   \n",
      "12460  2025079045    52  649000.0         2       1.75         2260    280962   \n",
      "9099   1525069058    55  568000.0         4       1.75         2110    265716   \n",
      "\n",
      "       floors  waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
      "167       2.0           0     0          3      9        2680              0   \n",
      "12412     1.0           0     0          5      7        1170           1170   \n",
      "7691      2.0           0     0          3      8        2190              0   \n",
      "12460     2.0           0     2          3      9        1890            370   \n",
      "9099      1.0           0     0          4      8        2110              0   \n",
      "\n",
      "       yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \n",
      "167        1999             0    98006  47.5650 -122.125           2920  \n",
      "12412      1924             0    98115  47.6746 -122.320           1270  \n",
      "7691       1978             0    98032  47.3742 -122.284           2170  \n",
      "12460      2005             0    98014  47.6359 -121.940           2860  \n",
      "9099       1979             0    98053  47.6570 -122.026           2110  , Test shape:                id  date      price  bedrooms  bathrooms  sqft_living  \\\n",
      "735    2591820310   157   365000.0         4       2.25         2070   \n",
      "2830   7974200820   111   865000.0         5       3.00         2900   \n",
      "4106   7701450110   105  1038000.0         4       2.50         3770   \n",
      "16218  9522300010   324  1490000.0         3       3.50         4560   \n",
      "19964  9510861140    73   711000.0         3       2.50         2550   \n",
      "\n",
      "       sqft_lot  floors  waterfront  view  condition  grade  sqft_above  \\\n",
      "735        8893     2.0           0     0          4      8        2070   \n",
      "2830       6730     1.0           0     0          5      8        1830   \n",
      "4106      10893     2.0           0     2          3     11        3770   \n",
      "16218     14608     2.0           0     2          3     12        4560   \n",
      "19964      5376     2.0           0     0          3      9        2550   \n",
      "\n",
      "       sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
      "735                0      1986             0    98058  47.4388 -122.162   \n",
      "2830            1070      1977             0    98115  47.6784 -122.285   \n",
      "4106               0      1997             0    98006  47.5646 -122.129   \n",
      "16218              0      1990             0    98034  47.6995 -122.228   \n",
      "19964              0      2004             0    98052  47.6647 -122.083   \n",
      "\n",
      "       sqft_living15  \n",
      "735             2390  \n",
      "2830            2370  \n",
      "4106            3710  \n",
      "16218           4050  \n",
      "19964           2250  \n",
      "============= Preprocessor built inside meta_trainer.py =============\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (21613, 20)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (21613,)\n",
      "Building preprocessor...\n",
      "Fitting preprocessor...\n",
      "X_processed shape before : (21613, 20)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      " • Set sparse_threshold=0 to force dense output\n",
      " • Configuring ColumnTransformer components...\n",
      "   - Disabled centering for robust_scaler\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (21613, 20)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (21613, 20)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (21613, 20)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 20 features...\n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (15129, 20)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (6484, 20)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      "evaluating simple rule model \n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 576\n",
      "[LightGBM] [Info] Number of data points in the train set: 151, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 11519.748344\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[Initial shape: (21613, 21)] [Cleaned: (21613, 21) -> (21613, 21)] ✓ Shape: (21613, 20), Target: (21613,), Target column: 'sqft_lot15'\n",
      "\n",
      "Kaggle Dataset Loading Summary:\n",
      "• Successfully loaded: 1/1 unique datasets\n",
      "• Failed to load: 0 datasets\n",
      "• Successfully loaded dataset details:\n",
      "  - kc_house_data: (21613, 20) features, 21613 samples\n",
      "\n",
      "Loaded 1 Kaggle datasets successfully!\n",
      "→ Processing dataset 1/1 (ID: 0)\n",
      "   • Dataset shape: (21613, 20), target shape: (21613,)\n",
      "   • Train shape:                id  date     price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
      "167    1836980160   317  807100.0         4       2.50         2680      4499   \n",
      "12412  9221400335   152  570000.0         4       1.75         2340      5080   \n",
      "7691   6669020490   102  320000.0         4       2.25         2190      9020   \n",
      "12460  2025079045    52  649000.0         2       1.75         2260    280962   \n",
      "9099   1525069058    55  568000.0         4       1.75         2110    265716   \n",
      "\n",
      "       floors  waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
      "167       2.0           0     0          3      9        2680              0   \n",
      "12412     1.0           0     0          5      7        1170           1170   \n",
      "7691      2.0           0     0          3      8        2190              0   \n",
      "12460     2.0           0     2          3      9        1890            370   \n",
      "9099      1.0           0     0          4      8        2110              0   \n",
      "\n",
      "       yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \n",
      "167        1999             0    98006  47.5650 -122.125           2920  \n",
      "12412      1924             0    98115  47.6746 -122.320           1270  \n",
      "7691       1978             0    98032  47.3742 -122.284           2170  \n",
      "12460      2005             0    98014  47.6359 -121.940           2860  \n",
      "9099       1979             0    98053  47.6570 -122.026           2110  , Test shape:                id  date      price  bedrooms  bathrooms  sqft_living  \\\n",
      "735    2591820310   157   365000.0         4       2.25         2070   \n",
      "2830   7974200820   111   865000.0         5       3.00         2900   \n",
      "4106   7701450110   105  1038000.0         4       2.50         3770   \n",
      "16218  9522300010   324  1490000.0         3       3.50         4560   \n",
      "19964  9510861140    73   711000.0         3       2.50         2550   \n",
      "\n",
      "       sqft_lot  floors  waterfront  view  condition  grade  sqft_above  \\\n",
      "735        8893     2.0           0     0          4      8        2070   \n",
      "2830       6730     1.0           0     0          5      8        1830   \n",
      "4106      10893     2.0           0     2          3     11        3770   \n",
      "16218     14608     2.0           0     2          3     12        4560   \n",
      "19964      5376     2.0           0     0          3      9        2550   \n",
      "\n",
      "       sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
      "735                0      1986             0    98058  47.4388 -122.162   \n",
      "2830            1070      1977             0    98115  47.6784 -122.285   \n",
      "4106               0      1997             0    98006  47.5646 -122.129   \n",
      "16218              0      1990             0    98034  47.6995 -122.228   \n",
      "19964              0      2004             0    98052  47.6647 -122.083   \n",
      "\n",
      "       sqft_living15  \n",
      "735             2390  \n",
      "2830            2370  \n",
      "4106            3710  \n",
      "16218           4050  \n",
      "19964           2250  \n",
      "============= Preprocessor built inside meta_trainer.py =============\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (21613, 20)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (21613,)\n",
      "Building preprocessor...\n",
      "Fitting preprocessor...\n",
      "X_processed shape before : (21613, 20)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      " • Set sparse_threshold=0 to force dense output\n",
      " • Configuring ColumnTransformer components...\n",
      "   - Disabled centering for robust_scaler\n",
      " • Preprocessor output type: <class 'numpy.ndarray'>\n",
      " • Preprocessor output shape: (21613, 20)\n",
      " • Is sparse matrix?: False\n",
      " • Converting numpy array to DataFrame\n",
      " • Final X_processed type: <class 'pandas.core.frame.DataFrame'>\n",
      " • Final X_processed shape: (21613, 20)\n",
      " • Step: Computing feature statistics...\n",
      " • X_num type: <class 'pandas.core.frame.DataFrame'>, shape: (21613, 20)\n",
      " • Computing feature skew/kurtosis...\n",
      " • ✓ Feature statistics computed\n",
      " • Computing correlations for 20 features...\n",
      " • ✓ Correlations computed\n",
      " X_train Type: <class 'numpy.ndarray'> Shape: (15129, 20)\n",
      " X_test Type: <class 'numpy.ndarray'> Shape: (6484, 20)\n",
      " =============== meta features evaluation =============== \n",
      "Evaluating dummy regressor\n",
      "evaluating decision stump \n",
      "evaluating simple rule model \n",
      "evaluating algorithms on 1% sample \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 576\n",
      "[LightGBM] [Info] Number of data points in the train set: 151, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 11519.748344\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Meta-features extracted ==============\n",
      "   • Training LGBMRegressor... [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2830\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 12823.633089\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2830\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 12823.633089\n",
      "R²=0.5720\n",
      "   • Training XGBRegressor... R²=0.5720\n",
      "   • Training XGBRegressor... R²=0.5358\n",
      "   • Training RandomForestRegressor... R²=0.5358\n",
      "   • Training RandomForestRegressor... R²=0.6014\n",
      "   • Training DecisionTreeRegressor... R²=0.6014\n",
      "   • Training DecisionTreeRegressor... R²=0.2161\n",
      "   • Training HistGradientBoostingRegressor... R²=0.2161\n",
      "   • Training HistGradientBoostingRegressor... R²=0.5863\n",
      "   • Training GradientBoostingRegressor... R²=0.5863\n",
      "   • Training GradientBoostingRegressor... R²=0.6160\n",
      "   • Training MLPRegressor... R²=0.6160\n",
      "   • Training MLPRegressor... R²=0.5434\n",
      "   • Training BayesianRidge... R²=0.5434\n",
      "   • Training BayesianRidge... R²=0.5325\n",
      "   • Training LinearRegression... R²=0.5325\n",
      "   • Training LinearRegression... R²=0.5328\n",
      "   • Training TabPFNRegressor... R²=0.5328\n",
      "   • Training TabPFNRegressor... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/automl_project/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error: Number of samples 15129 in the input data is greater than the maximum number of samples 10000 officially supported by TabPFN. Set `ignore_pretraining_limits=True` to override this error!]\n",
      "R²=-0.0000\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path5 = kagglehub.dataset_download(\"harlfoxem/housesalesprediction\")\n",
    "\n",
    "# Load the Kaggle dataset\n",
    "kaggle_datasets5 = load_kaggle_dataset(path5)\n",
    "\n",
    "if kaggle_datasets5:\n",
    "    print(f\"\\nLoaded {len(kaggle_datasets5)} Kaggle datasets successfully!\")\n",
    "    records = algorithms_eval(algorithms=algorithms, datasets=kaggle_datasets5)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(\"meta_features_kaggle_5.csv\", index=False)\n",
    "else:\n",
    "    print(\"No Kaggle datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79fe25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
