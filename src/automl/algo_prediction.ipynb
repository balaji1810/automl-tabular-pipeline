{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9496690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25a2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = Path(\"../../\").absolute().resolve()\n",
    "DATADIR = FILE / \"data\"\n",
    "path = DATADIR / 'brazilian_houses' / str(1)\n",
    "\n",
    "X_train_path = path / \"X_train.parquet\"\n",
    "y_train_path = path / \"y_train.parquet\"\n",
    "# X_test_path = path / \"X_test.parquet\"\n",
    "# y_test_path = path / \"y_test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499c6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"bike_share_demand\"\n",
    "fold = str(1)\n",
    "\n",
    "X = pd.read_parquet(X_train_path)\n",
    "y = pd.read_parquet(y_train_path).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15833498",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"meta_model.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c41a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_state_dict': OrderedDict([('encoder.0.weight', tensor([[-4.1670e-02,  7.1547e-03, -1.3277e-01,  ...,  2.4904e-07,\n",
      "         -8.5422e-05, -3.0629e-02],\n",
      "        [ 1.1206e-01, -6.3058e-02, -1.4844e-01,  ...,  6.8688e-08,\n",
      "          1.2046e-01,  1.2475e-01],\n",
      "        [ 1.7652e-01, -4.7932e-02, -1.0690e-01,  ..., -2.5593e-07,\n",
      "          4.1545e-03,  2.7546e-02],\n",
      "        ...,\n",
      "        [ 5.9922e-02,  8.3964e-02,  5.3604e-02,  ...,  2.6614e-07,\n",
      "         -1.2531e-04,  2.7919e-03],\n",
      "        [-7.3739e-02, -1.6441e-01, -1.5593e-02,  ...,  3.6142e-08,\n",
      "         -2.3487e-04, -1.8271e-02],\n",
      "        [ 4.1473e-02,  1.7159e-01, -6.0685e-02,  ..., -2.7386e-07,\n",
      "          2.0635e-02,  1.0353e-01]])), ('encoder.0.bias', tensor([-8.1845e-07,  1.9188e-07, -8.0561e-07, -5.3351e-07,  2.3363e-07,\n",
      "         1.9830e-07,  1.5638e-07, -1.4242e-07,  4.7628e-07,  1.6814e-07,\n",
      "        -1.0068e-07,  5.2144e-08,  2.3770e-07, -4.1597e-08, -2.3000e-07,\n",
      "        -1.3255e-07,  2.7373e-08, -4.8816e-08,  7.0127e-08, -3.2467e-07,\n",
      "        -3.5138e-07, -2.6697e-08,  7.1488e-07,  2.0294e-07,  2.5069e-07,\n",
      "        -4.6688e-07,  4.3602e-08,  1.3755e-06,  1.9255e-07,  1.6989e-07,\n",
      "        -1.5407e-07, -2.2293e-07,  1.7981e-07,  2.1993e-08, -1.1754e-07,\n",
      "         6.2209e-08,  5.3188e-07, -6.1537e-07,  2.2226e-07,  2.1920e-07,\n",
      "         6.0564e-07, -5.7159e-07,  7.9931e-07, -2.5103e-07,  2.5200e-07,\n",
      "         1.0259e-07, -5.8380e-07,  2.5172e-07, -7.2982e-07,  1.0865e-07,\n",
      "         4.4856e-07,  2.8377e-08,  8.8261e-08,  8.6340e-07,  4.5942e-08,\n",
      "        -2.8484e-07, -9.8644e-07,  8.3060e-08, -7.8993e-07, -8.9610e-08,\n",
      "        -2.6092e-07,  8.1845e-07,  1.0437e-07,  9.3908e-08, -9.4592e-09,\n",
      "        -1.2096e-07, -3.6835e-08, -2.3767e-07, -1.2501e-07, -1.1505e-07,\n",
      "         2.7767e-07,  7.1600e-07, -9.8390e-07,  2.0976e-07, -3.3154e-07,\n",
      "        -1.9076e-07, -2.4250e-07,  2.7767e-07, -1.8516e-07,  7.1703e-07,\n",
      "         4.6280e-07,  5.2354e-07,  1.1040e-06, -2.7519e-07,  5.7619e-07,\n",
      "         2.3759e-07, -3.0546e-07,  2.5456e-07])), ('encoder.1.weight', tensor([0.8618, 0.8437, 0.9262, 0.8779, 0.8879, 0.8455, 0.8690, 0.8997, 0.8916,\n",
      "        0.8639, 0.8368, 0.8896, 0.9205, 0.9200, 0.9444, 0.8852, 0.9296, 0.8710,\n",
      "        0.8956, 0.8598, 0.8942, 0.8681, 0.9071, 0.8166, 0.8352, 0.9399, 0.9000,\n",
      "        0.8223, 0.8942, 0.8018, 0.9281, 0.8860, 0.8526, 0.9353, 0.8922, 0.8122,\n",
      "        0.8483, 0.8245, 0.8499, 0.9018, 0.8638, 0.8598, 0.9028, 0.9124, 0.8622,\n",
      "        0.8830, 0.9938, 0.8513, 0.8917, 0.8668, 0.8737, 0.8399, 0.8630, 0.8896,\n",
      "        0.8747, 0.9134, 0.8282, 0.8971, 0.8900, 0.8732, 0.8378, 0.8175, 0.8819,\n",
      "        0.8850, 0.8816, 0.8756, 0.8318, 0.9527, 0.9626, 0.9093, 0.8847, 0.8743,\n",
      "        0.8046, 0.8290, 0.9820, 0.8341, 0.8732, 0.9476, 0.8329, 0.8386, 0.9004,\n",
      "        0.8111, 0.8761, 0.8360, 0.8583, 0.8400, 0.9023, 0.9528])), ('encoder.1.bias', tensor([-4.2811e-06, -1.0304e-06, -5.9079e-07, -1.5491e-06, -7.1107e-05,\n",
      "        -4.4076e-06, -2.0703e-06,  2.0175e-06, -1.2640e-06, -3.7592e-06,\n",
      "         1.0286e-06, -2.2192e-06, -6.3952e-03, -7.8463e-04, -2.4639e-05,\n",
      "        -5.4734e-06, -4.0798e-03, -2.3159e-06, -5.0912e-06, -1.2372e-02,\n",
      "         1.2347e-06, -3.0319e-06, -7.7297e-06, -2.2558e-06, -7.4045e-07,\n",
      "        -2.5380e-05,  2.4246e-04,  2.3859e-06, -9.2881e-04, -6.3399e-06,\n",
      "        -3.2148e-03, -4.9473e-07,  4.6103e-07, -2.9114e-03, -2.0792e-07,\n",
      "         2.2326e-06,  2.2240e-06, -8.5272e-07, -2.9891e-06, -5.3328e-03,\n",
      "        -1.5728e-05,  1.1511e-06,  5.9096e-07, -5.0158e-07,  1.9418e-06,\n",
      "        -6.6642e-06, -1.6351e-03,  2.9803e-06, -3.4277e-04, -1.1392e-02,\n",
      "         5.7702e-06, -3.0446e-07, -1.6000e-03, -1.7813e-05, -6.6647e-06,\n",
      "         3.0536e-06,  5.9970e-07,  1.1259e-06,  2.6278e-06,  6.4308e-08,\n",
      "        -4.2079e-07, -1.0007e-07, -2.0813e-06,  1.5918e-07,  2.4486e-06,\n",
      "        -3.4184e-07,  8.4335e-07, -2.0743e-02, -1.4391e-02,  6.7603e-07,\n",
      "        -5.9739e-06, -2.6783e-03, -1.7584e-06, -9.8844e-06, -2.3359e-07,\n",
      "        -1.6528e-06, -7.7050e-04, -3.0886e-05,  2.4745e-07, -2.8655e-05,\n",
      "        -1.4100e-04,  5.3032e-08, -8.5271e-06, -3.8102e-07, -4.4266e-08,\n",
      "         1.2096e-04, -1.8457e-06, -2.8065e-05])), ('encoder.1.running_mean', tensor([ 4.7788e-04,  8.6130e-04,  1.4183e-03, -6.4315e-04,  3.4551e-03,\n",
      "        -1.3629e-03,  3.0918e-03,  1.0919e-03,  1.0411e-03, -7.9748e-04,\n",
      "        -1.0174e-03, -3.1425e-03,  1.5924e-03,  9.9738e-04,  2.2186e-03,\n",
      "         8.4577e-05,  2.8586e-03, -2.5307e-03, -2.0675e-03, -2.5053e-03,\n",
      "        -2.1371e-03, -1.4619e-03,  3.5042e-03,  4.3183e-04, -1.9737e-03,\n",
      "        -1.6363e-03, -3.4717e-04, -5.1651e-04, -2.2888e-03, -1.6894e-03,\n",
      "         2.0773e-03,  1.4922e-03, -3.9368e-04, -2.1414e-04,  1.3518e-04,\n",
      "         7.8947e-04, -6.8322e-04, -1.5065e-04, -1.1624e-04, -6.3008e-05,\n",
      "         3.0387e-03, -3.2444e-04,  9.0144e-05,  2.0273e-03,  1.7506e-03,\n",
      "         5.8138e-04, -8.2438e-04,  1.4582e-04, -8.1069e-04, -2.3417e-04,\n",
      "         2.8333e-03,  6.9061e-04,  2.0148e-03, -6.7706e-04,  3.1809e-03,\n",
      "         4.2363e-03,  3.1945e-03,  4.8426e-05, -3.9496e-05, -2.0230e-04,\n",
      "        -8.6994e-04,  1.9345e-03,  2.0247e-03,  1.1520e-03,  1.6678e-04,\n",
      "        -6.8743e-04,  1.1120e-03, -8.4832e-04, -3.2302e-03, -3.8753e-03,\n",
      "         6.8540e-04, -4.3991e-04,  2.5917e-03, -4.3010e-04,  1.9279e-03,\n",
      "        -1.8919e-04, -3.8855e-03, -8.9763e-04, -4.3663e-05, -1.0408e-03,\n",
      "         8.0555e-04,  1.5399e-03,  2.2809e-04,  1.9603e-04,  2.6714e-03,\n",
      "        -1.6191e-03, -1.6663e-03,  3.5133e-03])), ('encoder.1.running_var', tensor([0.1205, 0.1445, 0.1821, 0.0651, 0.1402, 0.0836, 0.1391, 0.1505, 0.1532,\n",
      "        0.1119, 0.0826, 0.0913, 0.0342, 0.0454, 0.0426, 0.0695, 0.0576, 0.1153,\n",
      "        0.0560, 0.0886, 0.0899, 0.1077, 0.1012, 0.0877, 0.0795, 0.0480, 0.0742,\n",
      "        0.0820, 0.0631, 0.0530, 0.1142, 0.0561, 0.1110, 0.0809, 0.1432, 0.0818,\n",
      "        0.0434, 0.1021, 0.0302, 0.0788, 0.1344, 0.0779, 0.0796, 0.1592, 0.0233,\n",
      "        0.1171, 0.0262, 0.0505, 0.0875, 0.0476, 0.0634, 0.1022, 0.0473, 0.0363,\n",
      "        0.0692, 0.0659, 0.0879, 0.1708, 0.0938, 0.1101, 0.0781, 0.0231, 0.1452,\n",
      "        0.1154, 0.1344, 0.0413, 0.0730, 0.0255, 0.0700, 0.0777, 0.0575, 0.0748,\n",
      "        0.0594, 0.0912, 0.0435, 0.1110, 0.0766, 0.1051, 0.0492, 0.0478, 0.1014,\n",
      "        0.0962, 0.0545, 0.1010, 0.0796, 0.1338, 0.1750, 0.0590])), ('encoder.1.num_batches_tracked', tensor(1558)), ('encoder.4.weight', tensor([[-5.3252e-04,  8.3917e-05,  4.1623e-04,  ...,  3.7148e-03,\n",
      "          5.4487e-04,  4.2090e-04],\n",
      "        [-6.3347e-03,  7.4192e-05,  4.1067e-05,  ...,  1.8614e-04,\n",
      "         -1.9622e-04, -1.5293e-02],\n",
      "        [-7.2266e-05,  1.1596e-03, -3.3832e-05,  ..., -2.3459e-03,\n",
      "          3.8829e-02, -1.9243e-03],\n",
      "        ...,\n",
      "        [-1.1491e-02,  1.5015e-03, -8.6128e-07,  ..., -9.9261e-04,\n",
      "          9.9969e-06,  3.3641e-02],\n",
      "        [ 1.0828e-02, -9.9259e-04, -1.9458e-04,  ...,  2.7099e-02,\n",
      "         -5.1304e-03, -2.8688e-04],\n",
      "        [ 3.5344e-02, -1.5648e-04, -5.4374e-03,  ...,  2.1391e-02,\n",
      "         -4.8626e-04, -1.7173e-04]])), ('encoder.4.bias', tensor([ 7.0426e-09,  3.1595e-07,  1.0645e-08,  1.6008e-07,  8.1095e-07,\n",
      "         7.7367e-08, -5.9093e-08, -7.2618e-07, -1.6149e-07,  8.8931e-07,\n",
      "        -3.2668e-07,  8.8611e-07, -9.6208e-08,  7.0404e-07, -8.6391e-07,\n",
      "        -1.5458e-07, -2.0225e-07,  1.1651e-07,  1.9837e-07, -7.0844e-07,\n",
      "         4.0226e-08,  4.9211e-07, -3.1108e-07, -2.9405e-07, -7.2555e-07,\n",
      "        -2.5612e-07, -8.2510e-09,  3.6699e-07,  5.7761e-07,  3.1185e-08,\n",
      "         6.9982e-07, -2.1384e-07, -1.8946e-07,  2.2484e-07, -5.6921e-08,\n",
      "         8.4936e-08,  2.2703e-08, -2.8157e-08,  3.0899e-07, -1.1560e-07,\n",
      "        -3.2822e-07, -1.8476e-07, -1.0978e-07, -1.5414e-07,  5.3982e-07,\n",
      "         2.1086e-07,  1.1734e-07, -6.0175e-07,  1.0761e-07, -2.5429e-07,\n",
      "         5.3943e-07,  2.9907e-08, -8.8291e-07,  4.9113e-07,  5.3447e-09,\n",
      "        -2.7451e-07, -8.8362e-07,  3.4094e-07, -2.4714e-08, -2.4887e-07,\n",
      "        -8.6114e-08, -1.9967e-07, -8.1146e-07,  3.3422e-08,  6.2471e-08,\n",
      "         2.2992e-07, -1.9462e-07,  1.1013e-08, -2.6555e-07,  2.2509e-07,\n",
      "         1.9425e-07,  6.1623e-07,  1.3897e-07,  5.0212e-08, -3.2244e-07,\n",
      "        -5.0404e-07,  2.7645e-08, -2.1630e-07, -3.8840e-09,  5.7506e-07,\n",
      "         2.6304e-07, -2.3578e-07,  1.3022e-08, -2.5012e-07,  3.7660e-07,\n",
      "        -1.9213e-07,  5.2153e-09, -5.9606e-07,  5.8560e-10,  3.0218e-07,\n",
      "        -2.2534e-07, -2.7774e-08,  1.7136e-07,  1.5871e-08, -1.8353e-07,\n",
      "         2.1187e-09,  1.6605e-07, -8.6122e-07,  2.4084e-07,  7.1286e-07,\n",
      "         8.1217e-08, -1.3744e-07, -6.1798e-07,  5.7695e-07,  4.3801e-07,\n",
      "         2.3658e-07,  6.8007e-08, -7.9612e-08, -8.7213e-07, -1.8489e-07,\n",
      "         2.5306e-07, -3.8544e-09, -5.7923e-07, -3.1313e-07, -2.5302e-07,\n",
      "         1.1930e-07, -4.5980e-07, -8.1433e-07, -5.8596e-07,  8.0223e-07,\n",
      "         3.6528e-09,  9.2227e-08,  2.2577e-07,  1.0548e-07, -6.9367e-07,\n",
      "        -2.9226e-07,  6.5624e-08, -1.6327e-07, -6.4084e-07,  7.1789e-07,\n",
      "        -1.5691e-07, -9.7006e-07,  5.0965e-07, -9.5671e-07, -3.6060e-07,\n",
      "         1.1671e-07, -5.6540e-09,  2.3665e-07, -1.2050e-08,  3.5073e-07,\n",
      "        -7.0128e-08, -3.3407e-07,  3.5756e-07,  6.2935e-08,  7.2404e-07,\n",
      "         8.5674e-07,  7.2268e-07, -2.6905e-07,  6.0949e-07, -2.2534e-07,\n",
      "         4.3702e-07, -1.6851e-07,  1.8948e-07, -1.8283e-07,  2.4667e-07,\n",
      "         5.8916e-07, -7.2541e-07,  7.3186e-08,  2.1037e-07, -3.2170e-07,\n",
      "         7.9308e-07, -1.1710e-07,  1.1601e-07,  5.9992e-07, -7.0798e-07,\n",
      "        -2.6074e-07,  8.5032e-08, -1.4566e-07, -5.9308e-09,  1.2974e-06,\n",
      "        -2.5045e-07,  3.0910e-07,  2.3182e-07,  9.7067e-07,  2.4810e-08,\n",
      "        -2.8648e-07, -1.9781e-07,  3.5173e-07,  1.0210e-08,  8.7307e-07,\n",
      "        -2.6823e-07,  3.1841e-07, -1.0182e-07,  4.2078e-07,  6.2975e-08,\n",
      "        -3.0869e-07,  2.0834e-07,  9.7089e-07,  2.1338e-07,  7.2330e-07,\n",
      "         6.9131e-07,  1.2896e-06, -1.4124e-07, -7.9488e-07, -7.1769e-08,\n",
      "        -4.3943e-08, -2.0147e-07,  8.8942e-08, -3.9471e-08, -8.7809e-07,\n",
      "         6.2558e-07, -6.0476e-07,  2.2608e-07, -5.5152e-07,  2.5065e-07,\n",
      "        -8.8437e-08, -1.2001e-07,  4.6356e-08, -3.6533e-07,  1.2035e-08,\n",
      "         8.3682e-08, -7.0095e-07, -7.8955e-07,  2.2359e-07, -3.1912e-07,\n",
      "        -5.0889e-07, -8.1383e-08, -1.9417e-07,  5.4364e-07, -7.2783e-07,\n",
      "         1.1100e-08, -1.5856e-07,  4.2815e-07,  7.2865e-07,  9.2198e-09,\n",
      "         6.0937e-08,  1.0178e-07, -2.5862e-07,  8.8820e-08,  8.7837e-07,\n",
      "        -8.5955e-07,  2.4792e-07, -1.9029e-07, -9.8120e-07,  5.0368e-07,\n",
      "        -3.0579e-07,  1.9865e-07,  1.5906e-08,  1.3338e-07,  7.3716e-07,\n",
      "         5.0976e-07,  8.5929e-08, -6.1969e-07,  1.1415e-07, -3.0153e-07,\n",
      "         1.7912e-07, -1.6833e-07,  2.0868e-07,  1.6042e-07, -1.0065e-07,\n",
      "         1.6723e-07, -2.7843e-07, -4.4054e-09, -9.5783e-07,  7.8920e-07,\n",
      "         2.5851e-07])), ('encoder.5.weight', tensor([0.7812, 0.7599, 0.7830, 0.7706, 0.7014, 0.7494, 0.7759, 0.8322, 0.7759,\n",
      "        0.8402, 0.7854, 0.8465, 0.7727, 0.7114, 0.6656, 0.7434, 0.7071, 0.7029,\n",
      "        0.7125, 0.6953, 0.6818, 0.7911, 0.7398, 0.8167, 0.8002, 0.7434, 0.8066,\n",
      "        0.7954, 0.7589, 0.7683, 0.7946, 0.7280, 0.7733, 0.6756, 0.7538, 0.7668,\n",
      "        0.7865, 0.7449, 0.7942, 0.7980, 0.7883, 0.6839, 0.8235, 0.8082, 0.7601,\n",
      "        0.8565, 0.7416, 0.8083, 0.7446, 0.7041, 0.8787, 0.6943, 0.7853, 0.7485,\n",
      "        0.8194, 0.7937, 0.7408, 0.7538, 0.7055, 0.7319, 0.7761, 0.6818, 0.8077,\n",
      "        0.7512, 0.7194, 0.7326, 0.8653, 0.7605, 0.7464, 0.8078, 0.7136, 0.6360,\n",
      "        0.7308, 0.7495, 0.8459, 0.8024, 0.7552, 0.7799, 0.8070, 0.6890, 0.7315,\n",
      "        0.8385, 0.7548, 0.7627, 0.6998, 0.8127, 0.7357, 0.7004, 0.6870, 0.7381,\n",
      "        0.7186, 0.7748, 0.8520, 0.7675, 0.7653, 0.7211, 0.8012, 0.7587, 0.7008,\n",
      "        0.7356, 0.8117, 0.7565, 0.7129, 0.7231, 0.8086, 0.8071, 0.7504, 0.7230,\n",
      "        0.7662, 0.7471, 0.7529, 0.8839, 0.7177, 0.8082, 0.7594, 0.7004, 0.6463,\n",
      "        0.8231, 0.7295, 0.7825, 0.8144, 0.6981, 0.8096, 0.7710, 0.7007, 0.8208,\n",
      "        0.7672, 0.8456, 0.8066, 0.7361, 0.6937, 0.7799, 0.8110, 0.6935, 0.8335,\n",
      "        0.7346, 0.7026, 0.7721, 0.6593, 0.7597, 0.7995, 0.6816, 0.7968, 0.7452,\n",
      "        0.7336, 0.7527, 0.7446, 0.7576, 0.7000, 0.7646, 0.7868, 0.7234, 0.7295,\n",
      "        0.7068, 0.7718, 0.8650, 0.7791, 0.7556, 0.7949, 0.7569, 0.7404, 0.7772,\n",
      "        0.7469, 0.6684, 0.7743, 0.7490, 0.7797, 0.8556, 0.8411, 0.7126, 0.8243,\n",
      "        0.7838, 0.8779, 0.7299, 0.7616, 0.6784, 0.8448, 0.8253, 0.7119, 0.7068,\n",
      "        0.7902, 0.8120, 0.7511, 0.7340, 0.7695, 0.7538, 0.7540, 0.7547, 0.7926,\n",
      "        0.8051, 0.7957, 0.8016, 0.7614, 0.6890, 0.7887, 0.8387, 0.7774, 0.7176,\n",
      "        0.8252, 0.7675, 0.8171, 0.6869, 0.7682, 0.7709, 0.7469, 0.7867, 0.8391,\n",
      "        0.7106, 0.7320, 0.7456, 0.7551, 0.7888, 0.7746, 0.7652, 0.7650, 0.7510,\n",
      "        0.7746, 0.7615, 0.7368, 0.7595, 0.8127, 0.7638, 0.7165, 0.6980, 0.7699,\n",
      "        0.7310, 0.7394, 0.7774, 0.7891, 0.8014, 0.7594, 0.7722, 0.7770, 0.7887,\n",
      "        0.7273, 0.7894, 0.7475, 0.7631, 0.6707, 0.7173, 0.8137, 0.7111, 0.6993,\n",
      "        0.7429, 0.7627, 0.8233, 0.7910, 0.7673, 0.8153, 0.7312, 0.8053, 0.8116,\n",
      "        0.7246, 0.7187, 0.7738, 0.7282])), ('encoder.5.bias', tensor([-1.6108e-06,  1.7782e-06, -5.1328e-06, -1.3684e-06,  8.9933e-07,\n",
      "         9.3955e-08, -5.1896e-07, -1.9975e-06, -6.1479e-07,  1.1031e-06,\n",
      "         1.1649e-07,  1.1943e-07, -5.6847e-08, -1.4920e-06,  3.7995e-07,\n",
      "        -3.7211e-07,  6.0443e-08,  1.3646e-06, -2.7080e-07,  8.3355e-07,\n",
      "        -1.7658e-06,  8.0970e-08,  2.7128e-06,  1.2873e-07,  1.0726e-06,\n",
      "         1.8354e-06,  1.4814e-06,  6.5032e-08,  3.7207e-07, -6.5511e-06,\n",
      "        -1.4770e-06, -7.0520e-08,  1.7030e-06,  5.1640e-07,  3.3953e-07,\n",
      "         3.1788e-08,  3.7681e-07, -3.5908e-06,  1.8414e-06, -1.9364e-06,\n",
      "         8.0270e-07, -7.0185e-07, -4.0566e-07,  1.0232e-06, -4.1546e-08,\n",
      "        -9.8984e-06,  1.8157e-07, -4.2091e-07,  1.4281e-06,  1.3730e-07,\n",
      "        -4.1893e-06, -1.0490e-06, -6.3135e-08,  4.4939e-07, -4.5149e-06,\n",
      "        -7.3403e-07, -6.3809e-07, -6.4684e-08,  3.5408e-07, -1.0639e-07,\n",
      "         3.4498e-06, -2.1608e-07,  6.5042e-08,  1.7627e-07,  3.6072e-07,\n",
      "         8.4895e-07,  1.7187e-06, -3.0126e-06,  1.5424e-06,  1.9714e-06,\n",
      "        -1.5146e-06,  1.9019e-07, -2.1495e-07,  2.6988e-07, -1.3168e-06,\n",
      "        -2.7213e-07,  8.2795e-07,  7.2363e-07, -2.2902e-06,  3.7817e-07,\n",
      "         4.2505e-07, -4.2509e-07, -1.5717e-06, -5.1909e-07, -1.3630e-07,\n",
      "         4.0773e-07, -7.4538e-08,  1.6664e-07, -1.3427e-07, -6.0805e-09,\n",
      "        -2.2061e-06,  8.7466e-07,  1.1324e-06, -9.2577e-07,  6.5058e-07,\n",
      "         4.0355e-07,  2.5396e-07, -7.8491e-07, -1.3348e-06,  2.4429e-06,\n",
      "        -1.6004e-06,  1.2510e-06,  2.3448e-07,  5.9528e-07, -7.4849e-06,\n",
      "        -1.0034e-06, -2.4072e-07,  5.7502e-07, -2.8047e-07,  6.5525e-06,\n",
      "        -8.4076e-07, -1.0543e-06, -2.0848e-06, -5.6905e-07,  5.7190e-07,\n",
      "        -3.8780e-07,  1.2165e-07, -2.5149e-06,  1.8428e-06, -5.7916e-07,\n",
      "         1.8658e-09, -5.3643e-07,  3.4267e-06, -1.7817e-06,  3.4971e-10,\n",
      "         5.7930e-07,  6.4352e-07, -3.5629e-06,  8.4860e-07,  2.2192e-07,\n",
      "         2.5855e-07, -2.6476e-05,  1.1697e-06, -2.6247e-07, -1.0849e-05,\n",
      "        -2.5050e-06, -7.1059e-07, -2.4153e-06, -6.3468e-07,  1.1070e-07,\n",
      "        -2.6080e-06, -1.2095e-07, -1.2845e-07,  6.9558e-08,  2.8489e-07,\n",
      "        -5.7727e-07, -7.9217e-07, -1.6639e-07, -8.8028e-07, -3.9135e-07,\n",
      "        -1.9104e-06,  4.1555e-07, -1.2856e-06,  4.1520e-07, -6.7485e-07,\n",
      "        -1.0835e-06, -3.2201e-06,  1.4567e-06,  1.3567e-06,  9.5771e-07,\n",
      "        -1.2156e-06, -9.0418e-07,  1.6068e-06, -3.1534e-07,  4.0837e-07,\n",
      "         2.8213e-06,  3.2320e-07, -6.0228e-06, -3.9629e-07, -2.9928e-07,\n",
      "        -9.8537e-07,  1.2458e-06, -1.2463e-06, -1.0088e-06, -1.2154e-06,\n",
      "         1.0072e-07,  1.2796e-06,  8.3271e-07, -4.9634e-07, -4.3040e-07,\n",
      "        -9.6103e-07, -1.2135e-06, -6.3644e-07,  1.2495e-06, -8.4639e-07,\n",
      "         2.6628e-06, -9.8875e-08, -2.6222e-07, -1.6498e-06, -1.4089e-06,\n",
      "        -1.5741e-06, -5.8962e-07, -3.8877e-06, -2.2519e-07,  5.2471e-07,\n",
      "        -1.2271e-06, -2.3440e-06, -1.3886e-06,  2.1114e-07, -9.4140e-07,\n",
      "        -1.0744e-06,  4.1400e-07, -5.7311e-07,  1.8018e-06,  7.0259e-08,\n",
      "        -1.6909e-06, -3.9368e-06, -1.6913e-06, -1.6053e-06,  9.2677e-07,\n",
      "        -1.6963e-06, -4.5108e-07, -5.6637e-07, -1.1837e-07,  3.4022e-06,\n",
      "         7.3493e-07,  1.1431e-07,  9.8647e-07,  4.4343e-07, -2.6166e-07,\n",
      "         2.5272e-07, -4.4763e-07, -1.1599e-06, -1.2526e-07, -2.0468e-06,\n",
      "         1.7148e-07,  2.7267e-06, -5.5769e-06,  2.9877e-07, -1.5170e-06,\n",
      "         2.3253e-06, -1.0779e-06,  2.2947e-09,  3.1334e-07,  3.7879e-07,\n",
      "         1.3522e-07,  2.1218e-07, -1.8250e-06, -1.2075e-06, -1.3690e-06,\n",
      "         5.5482e-07, -4.7922e-07, -1.5365e-07,  7.3811e-07,  1.0884e-06,\n",
      "        -1.2015e-05, -3.8428e-07, -6.1729e-07, -4.2662e-05, -3.2511e-07,\n",
      "         1.0510e-07,  1.9939e-06, -3.7623e-07, -7.1852e-07, -2.9625e-08,\n",
      "         5.3330e-07])), ('encoder.5.running_mean', tensor([ 0.1079,  0.0407, -0.0821, -0.0223, -0.0420,  0.0994, -0.1417,  0.1711,\n",
      "         0.0301,  0.0341,  0.0898,  0.0588,  0.1105,  0.0675,  0.0778,  0.0485,\n",
      "        -0.0040,  0.0565,  0.0221, -0.0121,  0.0082,  0.0530,  0.0484,  0.0871,\n",
      "         0.0400,  0.0769, -0.0544, -0.0270,  0.1100, -0.1241, -0.0347,  0.0473,\n",
      "        -0.1150,  0.0514,  0.0191,  0.0149,  0.1038,  0.0169,  0.0172, -0.0961,\n",
      "         0.0321,  0.0035,  0.1368, -0.0294,  0.0985,  0.1730,  0.0357,  0.0473,\n",
      "         0.0386, -0.0505, -0.0164,  0.0098,  0.0399,  0.0780,  0.0571, -0.0264,\n",
      "         0.0259,  0.0703,  0.0123,  0.0203, -0.0982, -0.0511,  0.0676,  0.1102,\n",
      "        -0.0592,  0.0172,  0.0200,  0.1130,  0.0667,  0.0424,  0.0418,  0.0261,\n",
      "         0.0643,  0.0415, -0.1005,  0.1015,  0.1273,  0.0582, -0.0483,  0.0418,\n",
      "         0.0411,  0.0108,  0.0057,  0.0792,  0.0586,  0.0479,  0.0604,  0.0251,\n",
      "         0.0446,  0.1186, -0.0804,  0.0360,  0.0682, -0.1016, -0.0599,  0.0406,\n",
      "         0.0630,  0.0500, -0.0202,  0.0332,  0.0181,  0.0013,  0.1062,  0.0777,\n",
      "        -0.0484,  0.0141,  0.0034,  0.0788, -0.1234,  0.1057,  0.0201, -0.0208,\n",
      "         0.0011, -0.1040, -0.0068, -0.0600,  0.0533,  0.0879, -0.0386,  0.0419,\n",
      "         0.0436,  0.0549,  0.0755,  0.0991,  0.0022,  0.1033,  0.0526,  0.1203,\n",
      "         0.0486,  0.0412,  0.0670, -0.0987,  0.1184,  0.0180, -0.0484,  0.1102,\n",
      "         0.0237,  0.1178,  0.0048, -0.0085, -0.0406,  0.0429,  0.0325,  0.0488,\n",
      "         0.0709, -0.0066,  0.0616,  0.1333,  0.0155,  0.0502, -0.0075,  0.0391,\n",
      "        -0.0300,  0.1135,  0.0578,  0.0691,  0.1040,  0.0584,  0.0626,  0.0428,\n",
      "         0.0618,  0.0550,  0.0447, -0.0717,  0.0060, -0.0882,  0.0923, -0.0318,\n",
      "         0.1386,  0.0479,  0.1141,  0.0630,  0.1353,  0.0298,  0.1001,  0.0428,\n",
      "        -0.0702, -0.0928,  0.0300,  0.0592,  0.1032,  0.0655,  0.0403,  0.0307,\n",
      "         0.0234,  0.0880,  0.0359,  0.0284,  0.0413, -0.1260,  0.0356, -0.0158,\n",
      "         0.0336,  0.0167, -0.0451,  0.1194, -0.0146,  0.0679, -0.0994, -0.0087,\n",
      "         0.0522, -0.0402,  0.0667,  0.0224,  0.0828,  0.1026, -0.0382, -0.1010,\n",
      "        -0.0245,  0.0570,  0.0129,  0.1445,  0.0546,  0.0720,  0.0574,  0.0531,\n",
      "         0.0588, -0.0104,  0.0481,  0.0362,  0.0052,  0.0229,  0.0414,  0.0689,\n",
      "         0.0915,  0.0487, -0.0184,  0.0368,  0.0635, -0.1035, -0.0494,  0.1109,\n",
      "        -0.0079,  0.0241,  0.0318,  0.0613,  0.0379, -0.0897,  0.0140,  0.0203,\n",
      "        -0.0510, -0.0676,  0.0016, -0.0687,  0.0883, -0.0850, -0.1515, -0.0147,\n",
      "        -0.2139,  0.0603,  0.0844,  0.0719,  0.0253,  0.0631,  0.0335,  0.0275])), ('encoder.5.running_var', tensor([0.0183, 0.0082, 0.0119, 0.0130, 0.0038, 0.0086, 0.0085, 0.0287, 0.0074,\n",
      "        0.0340, 0.0124, 0.0370, 0.0221, 0.0056, 0.0060, 0.0086, 0.0043, 0.0059,\n",
      "        0.0050, 0.0021, 0.0034, 0.0084, 0.0115, 0.0156, 0.0095, 0.0101, 0.0131,\n",
      "        0.0147, 0.0096, 0.0113, 0.0134, 0.0075, 0.0195, 0.0038, 0.0125, 0.0088,\n",
      "        0.0151, 0.0092, 0.0187, 0.0123, 0.0174, 0.0028, 0.0203, 0.0175, 0.0174,\n",
      "        0.0382, 0.0157, 0.0119, 0.0055, 0.0062, 0.0317, 0.0053, 0.0104, 0.0102,\n",
      "        0.0242, 0.0149, 0.0080, 0.0114, 0.0031, 0.0055, 0.0144, 0.0043, 0.0173,\n",
      "        0.0077, 0.0051, 0.0054, 0.0317, 0.0115, 0.0151, 0.0163, 0.0104, 0.0014,\n",
      "        0.0077, 0.0121, 0.0277, 0.0119, 0.0152, 0.0097, 0.0133, 0.0046, 0.0056,\n",
      "        0.0235, 0.0088, 0.0122, 0.0048, 0.0184, 0.0072, 0.0043, 0.0045, 0.0153,\n",
      "        0.0069, 0.0084, 0.0289, 0.0141, 0.0082, 0.0082, 0.0223, 0.0120, 0.0055,\n",
      "        0.0091, 0.0134, 0.0076, 0.0072, 0.0195, 0.0139, 0.0219, 0.0089, 0.0092,\n",
      "        0.0110, 0.0141, 0.0158, 0.0249, 0.0065, 0.0199, 0.0115, 0.0048, 0.0034,\n",
      "        0.0288, 0.0090, 0.0096, 0.0130, 0.0056, 0.0148, 0.0124, 0.0071, 0.0142,\n",
      "        0.0121, 0.0336, 0.0166, 0.0070, 0.0045, 0.0103, 0.0188, 0.0039, 0.0246,\n",
      "        0.0133, 0.0062, 0.0248, 0.0029, 0.0027, 0.0163, 0.0058, 0.0171, 0.0052,\n",
      "        0.0087, 0.0043, 0.0063, 0.0159, 0.0069, 0.0071, 0.0179, 0.0090, 0.0074,\n",
      "        0.0086, 0.0137, 0.0288, 0.0118, 0.0088, 0.0244, 0.0071, 0.0111, 0.0110,\n",
      "        0.0105, 0.0021, 0.0124, 0.0093, 0.0075, 0.0160, 0.0233, 0.0060, 0.0296,\n",
      "        0.0148, 0.0305, 0.0063, 0.0106, 0.0036, 0.0179, 0.0224, 0.0028, 0.0088,\n",
      "        0.0159, 0.0157, 0.0078, 0.0059, 0.0095, 0.0097, 0.0048, 0.0077, 0.0158,\n",
      "        0.0185, 0.0155, 0.0102, 0.0161, 0.0031, 0.0080, 0.0293, 0.0119, 0.0101,\n",
      "        0.0204, 0.0167, 0.0144, 0.0031, 0.0097, 0.0121, 0.0078, 0.0145, 0.0156,\n",
      "        0.0056, 0.0038, 0.0115, 0.0079, 0.0129, 0.0103, 0.0087, 0.0114, 0.0069,\n",
      "        0.0103, 0.0082, 0.0102, 0.0103, 0.0159, 0.0130, 0.0072, 0.0042, 0.0159,\n",
      "        0.0093, 0.0095, 0.0143, 0.0148, 0.0142, 0.0095, 0.0157, 0.0090, 0.0101,\n",
      "        0.0102, 0.0062, 0.0136, 0.0142, 0.0028, 0.0041, 0.0128, 0.0091, 0.0046,\n",
      "        0.0111, 0.0112, 0.0187, 0.0145, 0.0037, 0.0183, 0.0054, 0.0202, 0.0139,\n",
      "        0.0099, 0.0124, 0.0104, 0.0070])), ('encoder.5.num_batches_tracked', tensor(1558)), ('encoder.8.weight', tensor([[-1.4425e-05, -3.2165e-03, -9.6809e-04,  ..., -2.1779e-04,\n",
      "          1.0893e-03,  9.4377e-04],\n",
      "        [ 2.6994e-03, -4.0408e-03, -2.1151e-04,  ...,  6.3219e-03,\n",
      "          5.7933e-03, -9.0627e-06],\n",
      "        [ 5.9858e-03, -4.8379e-06, -2.6689e-03,  ..., -4.1514e-05,\n",
      "          1.7327e-04, -6.2620e-04],\n",
      "        ...,\n",
      "        [-1.4564e-04, -2.4225e-03,  7.6952e-04,  ...,  9.4810e-05,\n",
      "          4.5630e-04, -8.4086e-05],\n",
      "        [ 2.9099e-04, -3.9523e-04, -1.0639e-03,  ..., -2.1119e-05,\n",
      "          1.1362e-05,  4.4906e-06],\n",
      "        [-1.0330e-03, -2.7733e-03, -7.5742e-05,  ..., -2.5829e-05,\n",
      "         -6.6350e-04, -4.1168e-05]])), ('encoder.8.bias', tensor([-1.2284e-07,  8.6797e-07,  7.9254e-07, -3.6017e-07, -2.8739e-07,\n",
      "         1.9394e-07, -6.1040e-08, -5.7654e-08,  6.3535e-07, -5.2937e-08,\n",
      "        -1.2301e-07, -5.5020e-08, -2.3725e-07, -1.0230e-07, -1.4965e-07,\n",
      "         2.2702e-07,  8.1774e-07, -2.8659e-07,  9.7142e-07,  1.1382e-07,\n",
      "        -6.5664e-08,  5.6629e-08,  2.2706e-07,  2.8807e-07,  1.1065e-07,\n",
      "        -1.9569e-07, -6.4650e-08,  6.9663e-07,  4.5110e-07, -8.5881e-08,\n",
      "        -2.8455e-07, -1.7342e-07,  2.7850e-07,  1.1383e-07, -2.0591e-07,\n",
      "        -5.0509e-08, -2.3183e-07,  1.0503e-07,  1.1214e-07,  1.1252e-07,\n",
      "         1.0254e-07,  9.5610e-07,  4.9971e-08, -5.7942e-08, -1.7966e-07,\n",
      "        -3.8542e-08,  1.1258e-07, -9.7743e-09,  4.5406e-07,  1.9647e-07,\n",
      "         1.7370e-07,  7.0912e-07,  4.6976e-08, -1.3762e-07,  2.0663e-07,\n",
      "         7.0339e-07,  4.1041e-08,  9.8536e-07, -7.1911e-07, -1.1943e-07,\n",
      "         5.9268e-08,  1.8249e-07,  2.4789e-07,  9.7102e-08, -1.8807e-07,\n",
      "         9.9765e-07,  2.5303e-07, -1.2176e-07, -4.6156e-07,  7.2579e-07,\n",
      "        -1.1558e-07, -9.2965e-09,  2.6785e-07, -1.7365e-07, -2.5358e-07,\n",
      "         2.4753e-07, -2.2074e-07,  1.0847e-09,  1.9071e-07, -5.3235e-07,\n",
      "         2.5818e-07, -1.2400e-07,  7.7445e-08,  1.6852e-07,  9.8235e-07,\n",
      "        -1.9343e-08,  9.4841e-08, -2.1132e-07,  9.0427e-08, -4.1931e-07,\n",
      "         9.8842e-08,  1.6765e-07,  1.2421e-07, -4.7661e-07,  1.0010e-06,\n",
      "        -3.0085e-07, -2.0452e-07, -4.5107e-07, -8.6377e-07,  2.5147e-07,\n",
      "        -4.0233e-07, -3.3508e-07, -4.9449e-07, -2.6797e-07, -1.1303e-07,\n",
      "         3.6975e-07,  7.3399e-07,  2.4788e-07,  2.4457e-07,  4.3209e-07,\n",
      "        -2.1649e-07, -7.9836e-09,  6.6424e-08, -1.8063e-08,  9.6656e-08,\n",
      "         1.6016e-07,  1.2923e-06, -1.2796e-06, -2.1269e-07,  6.1916e-07,\n",
      "        -2.8801e-07,  8.8281e-08,  2.6662e-07,  2.5883e-07,  1.3785e-06,\n",
      "         5.4088e-08, -7.1849e-08,  2.1154e-07])), ('encoder.9.weight', tensor([0.7643, 0.8539, 0.7733, 0.7415, 0.8877, 0.9237, 0.8565, 0.8637, 0.8138,\n",
      "        0.8422, 0.8418, 0.8407, 0.7792, 0.8057, 0.8814, 0.8895, 0.7830, 0.8062,\n",
      "        0.7841, 0.9020, 0.8445, 0.7745, 0.8414, 0.7974, 0.8263, 0.8189, 0.7680,\n",
      "        0.8418, 0.8064, 0.8479, 0.8854, 0.8290, 0.8890, 0.8237, 0.8075, 0.8038,\n",
      "        0.8334, 0.8627, 0.8375, 0.8221, 0.7998, 0.9128, 0.8054, 0.9026, 0.7587,\n",
      "        0.8049, 0.8537, 0.8214, 0.7831, 0.8757, 0.8493, 0.8456, 0.8651, 0.7845,\n",
      "        0.7996, 0.7724, 0.8300, 0.8195, 0.8463, 0.8095, 0.7594, 0.9189, 0.7949,\n",
      "        0.8408, 0.8101, 0.7832, 0.8577, 0.8488, 0.8797, 0.8490, 0.8506, 0.8787,\n",
      "        0.7916, 0.7921, 0.8767, 0.8246, 0.8336, 0.8403, 0.8318, 0.7766, 0.8351,\n",
      "        0.8449, 0.8069, 0.8670, 0.8085, 0.8829, 0.8261, 0.8639, 0.7450, 0.8055,\n",
      "        0.8370, 0.8568, 0.8681, 0.8504, 0.8254, 0.9097, 0.8647, 0.7950, 0.8414,\n",
      "        0.7792, 0.7664, 0.8628, 0.8294, 0.8656, 0.8185, 0.8209, 0.8741, 0.8171,\n",
      "        0.7914, 0.8679, 0.8329, 0.8757, 0.8594, 0.8436, 0.7944, 0.8075, 0.7369,\n",
      "        0.8170, 0.8914, 0.8634, 0.7946, 0.8682, 0.8198, 0.8461, 0.9233, 0.7546,\n",
      "        0.7893, 0.7594])), ('encoder.9.bias', tensor([ 4.9968e-08, -7.3009e-07, -1.2611e-06, -1.0326e-06, -5.6068e-07,\n",
      "         1.1943e-05, -5.8946e-06,  1.2948e-07,  7.0950e-06, -5.6731e-06,\n",
      "        -2.2955e-06,  3.4853e-07, -1.0451e-07, -2.6830e-06, -7.9736e-07,\n",
      "        -2.3259e-06, -6.8727e-07, -8.7648e-07, -7.0788e-08,  5.0747e-08,\n",
      "        -1.7376e-06, -2.8553e-06,  6.2847e-06,  1.3985e-06, -1.3489e-06,\n",
      "        -4.6939e-07,  1.6368e-06,  1.5520e-06,  1.9089e-06, -9.0693e-03,\n",
      "        -6.0149e-04, -5.8409e-06, -2.2044e-03,  8.0630e-07,  6.1805e-06,\n",
      "        -3.2937e-06, -3.8022e-06,  2.0666e-06, -4.8067e-06,  5.2267e-07,\n",
      "         3.8239e-06, -4.5505e-06,  2.7483e-07, -1.7436e-03,  3.2065e-06,\n",
      "        -2.7745e-06, -9.9668e-08, -2.2756e-06,  5.7319e-07,  1.1240e-06,\n",
      "        -2.1902e-06, -3.8024e-06,  9.3162e-07,  3.6138e-07,  8.3400e-07,\n",
      "         1.7370e-07, -1.3063e-06,  2.8838e-06,  1.3030e-07, -3.1185e-07,\n",
      "         3.5142e-06, -5.8978e-06,  3.0721e-07,  7.4514e-07,  4.6702e-06,\n",
      "         2.4753e-07, -7.4835e-06, -1.0981e-06,  1.0091e-06, -4.1226e-06,\n",
      "        -1.6844e-06, -1.8560e-06, -1.0595e-06,  2.7390e-06, -9.4680e-07,\n",
      "         4.1617e-07, -1.4812e-06, -4.0080e-09, -1.0081e-06, -8.3518e-07,\n",
      "        -1.7044e-06, -4.0276e-06,  2.9419e-06,  3.1490e-07, -1.5111e-07,\n",
      "        -6.0845e-06,  3.9340e-06, -6.9396e-07, -8.3442e-07,  1.6260e-08,\n",
      "         2.3554e-06,  2.6811e-06, -3.3529e-06, -2.8048e-06, -7.6666e-07,\n",
      "        -2.9890e-05, -7.0412e-04,  2.7360e-07,  2.6873e-07, -2.1049e-07,\n",
      "         6.4069e-06, -1.7298e-06,  1.0019e-06, -1.2046e-06, -5.4568e-07,\n",
      "         8.8254e-07,  1.8629e-06,  7.4747e-06, -1.8112e-07, -4.7446e-03,\n",
      "        -4.7797e-06, -7.3457e-04, -3.1213e-06,  1.4125e-06,  9.2059e-07,\n",
      "        -6.2537e-06, -1.7405e-07, -6.8394e-07, -5.2407e-07, -2.8051e-06,\n",
      "         2.2002e-06,  6.6468e-07,  3.7052e-07,  2.1002e-06, -8.1448e-03,\n",
      "        -3.4726e-07, -7.3041e-08,  3.5225e-06])), ('encoder.9.running_mean', tensor([ 0.0256, -0.0473,  0.0204,  0.0282, -0.0553, -0.1103,  0.0628,  0.1222,\n",
      "         0.0436,  0.0196, -0.0065,  0.1376,  0.0481,  0.0826, -0.0228,  0.0438,\n",
      "         0.0083,  0.0473,  0.1055, -0.0498,  0.0113,  0.0493,  0.0566,  0.0217,\n",
      "         0.0670,  0.1091,  0.0903,  0.0390,  0.1591, -0.0476, -0.0854,  0.0740,\n",
      "        -0.0337,  0.0930,  0.0538,  0.0656, -0.1577, -0.0051,  0.1421,  0.0764,\n",
      "         0.0572, -0.0776,  0.0886,  0.0131,  0.0476, -0.0233, -0.0591, -0.0037,\n",
      "         0.0275,  0.0972,  0.0358, -0.0447,  0.2246,  0.0626,  0.0399,  0.0617,\n",
      "         0.0497, -0.0060,  0.0292,  0.1162, -0.0626,  0.0339,  0.0736,  0.0181,\n",
      "         0.0893,  0.0062, -0.0756, -0.0226,  0.1366, -0.0053,  0.0842,  0.1097,\n",
      "        -0.0218,  0.0982,  0.1115,  0.0488,  0.0868, -0.0288,  0.0943,  0.1106,\n",
      "         0.0115, -0.0446,  0.0767,  0.0718,  0.1041,  0.0469,  0.0334,  0.0828,\n",
      "         0.0711,  0.1545, -0.0032, -0.0826, -0.1875,  0.1043,  0.0414,  0.0840,\n",
      "        -0.0962,  0.0859,  0.0175,  0.0242,  0.0653, -0.0868, -0.1117,  0.0257,\n",
      "         0.0586, -0.0092,  0.0192,  0.0898, -0.0732, -0.0969,  0.0342, -0.0492,\n",
      "         0.0019,  0.1128,  0.1169, -0.0267,  0.0186, -0.0520,  0.0035,  0.0859,\n",
      "         0.0525,  0.1065,  0.0899,  0.1381, -0.0500, -0.0455, -0.0004,  0.0237])), ('encoder.9.running_var', tensor([0.0130, 0.0167, 0.0219, 0.0093, 0.0437, 0.0486, 0.0473, 0.0336, 0.0313,\n",
      "        0.0229, 0.0161, 0.0444, 0.0193, 0.0233, 0.0483, 0.0478, 0.0225, 0.0201,\n",
      "        0.0199, 0.0426, 0.0181, 0.0177, 0.0234, 0.0214, 0.0283, 0.0266, 0.0220,\n",
      "        0.0260, 0.0394, 0.0468, 0.0516, 0.0257, 0.0532, 0.0265, 0.0206, 0.0276,\n",
      "        0.0336, 0.0417, 0.0515, 0.0168, 0.0282, 0.0557, 0.0158, 0.0370, 0.0085,\n",
      "        0.0215, 0.0333, 0.0189, 0.0256, 0.0502, 0.0310, 0.0208, 0.0420, 0.0134,\n",
      "        0.0255, 0.0200, 0.0311, 0.0274, 0.0189, 0.0220, 0.0113, 0.0541, 0.0169,\n",
      "        0.0362, 0.0247, 0.0211, 0.0333, 0.0352, 0.0410, 0.0244, 0.0387, 0.0455,\n",
      "        0.0141, 0.0286, 0.0402, 0.0242, 0.0277, 0.0206, 0.0278, 0.0195, 0.0186,\n",
      "        0.0292, 0.0230, 0.0400, 0.0267, 0.0321, 0.0213, 0.0482, 0.0150, 0.0207,\n",
      "        0.0244, 0.0319, 0.0310, 0.0444, 0.0172, 0.0513, 0.0317, 0.0246, 0.0364,\n",
      "        0.0193, 0.0182, 0.0405, 0.0349, 0.0430, 0.0303, 0.0243, 0.0443, 0.0157,\n",
      "        0.0181, 0.0365, 0.0215, 0.0546, 0.0353, 0.0340, 0.0238, 0.0165, 0.0083,\n",
      "        0.0270, 0.0468, 0.0441, 0.0230, 0.0470, 0.0289, 0.0409, 0.0760, 0.0077,\n",
      "        0.0155, 0.0102])), ('encoder.9.num_batches_tracked', tensor(1558)), ('encoder.12.weight', tensor([[ 4.8959e-04,  2.9097e-04,  1.2390e-04,  ...,  2.3705e-07,\n",
      "         -3.7881e-05, -2.6770e-04],\n",
      "        [ 3.0008e-02, -3.5504e-06,  4.6540e-03,  ..., -1.5129e-07,\n",
      "          5.4612e-02,  7.5689e-06],\n",
      "        [ 4.8263e-05,  2.2477e-02, -1.8609e-02,  ..., -1.2774e-04,\n",
      "          1.1678e-02,  1.9827e-04],\n",
      "        ...,\n",
      "        [ 1.9248e-02, -1.4132e-03, -1.9298e-02,  ..., -3.2522e-03,\n",
      "          1.4319e-02, -2.3643e-03],\n",
      "        [ 1.6650e-02,  6.1885e-03, -2.9866e-04,  ..., -8.5363e-04,\n",
      "          1.5006e-03,  1.9970e-03],\n",
      "        [-4.5033e-03,  6.0924e-03,  2.3677e-05,  ...,  3.4341e-03,\n",
      "         -1.3845e-02, -2.3837e-04]])), ('encoder.12.bias', tensor([-4.2068e-08,  5.2312e-07,  1.1175e-07, -2.4494e-09,  1.1647e-08,\n",
      "         8.0501e-07,  8.6767e-08,  1.6900e-07,  6.7868e-08,  1.3281e-07,\n",
      "        -2.0011e-07, -1.7078e-07, -9.0794e-08,  1.6486e-07, -1.0444e-07,\n",
      "        -3.3340e-07,  1.2943e-06,  2.0864e-07,  2.7737e-08, -1.0304e-07,\n",
      "        -4.2712e-07,  8.8209e-07,  9.8953e-07,  3.4318e-07, -3.6556e-07,\n",
      "        -1.0052e-07,  4.7357e-07,  1.5382e-07,  3.6859e-07,  2.8718e-07,\n",
      "         2.6920e-07, -9.6789e-08, -5.5208e-08, -2.0598e-07, -1.9317e-07,\n",
      "         3.6394e-07,  2.6228e-07,  1.2309e-07,  1.2051e-07,  6.6765e-08,\n",
      "         2.1630e-07, -2.0573e-08,  1.9219e-07, -6.9962e-07,  6.1490e-07,\n",
      "         5.3400e-08,  3.1435e-07,  4.4408e-08,  3.3876e-09,  1.1422e-08,\n",
      "        -5.8601e-08, -5.1779e-07,  1.2160e-08,  1.0050e-07, -8.7376e-08,\n",
      "         7.9131e-08, -2.3771e-08,  8.7273e-07,  8.7077e-07,  4.5790e-08,\n",
      "        -3.2860e-09,  1.9192e-07, -3.2854e-07, -3.0091e-07])), ('encoder.13.weight', tensor([1.2410, 1.3262, 1.3930, 1.2893, 1.2778, 1.3024, 1.3093, 1.1765, 1.2258,\n",
      "        1.2876, 1.2569, 1.3787, 1.3105, 1.2389, 1.3296, 1.3393, 1.3541, 1.2133,\n",
      "        1.2647, 1.4846, 1.3615, 1.3060, 1.3391, 1.2274, 1.3117, 1.3632, 1.2629,\n",
      "        1.2872, 1.2615, 1.3433, 1.2983, 1.2987, 1.3461, 1.2142, 1.3685, 1.2826,\n",
      "        1.3873, 1.1844, 1.1926, 1.1895, 1.3750, 1.2184, 1.3437, 1.2285, 1.3343,\n",
      "        1.3501, 1.3027, 1.2685, 1.3950, 1.2907, 1.1623, 1.3764, 1.3104, 1.3533,\n",
      "        1.2650, 1.4211, 1.3008, 1.2996, 1.2688, 1.2553, 1.3351, 1.2538, 1.1595,\n",
      "        1.3048])), ('encoder.13.bias', tensor([0.1536, 0.2515, 0.2817, 0.2618, 0.1615, 0.2399, 0.2358, 0.1569, 0.0969,\n",
      "        0.2068, 0.1126, 0.2833, 0.1899, 0.2112, 0.2187, 0.2588, 0.2905, 0.1455,\n",
      "        0.1932, 0.3252, 0.2608, 0.2386, 0.3260, 0.1091, 0.2146, 0.2912, 0.1766,\n",
      "        0.1534, 0.1215, 0.2593, 0.2570, 0.1734, 0.2365, 0.1679, 0.2720, 0.1840,\n",
      "        0.3413, 0.1066, 0.0820, 0.1849, 0.2894, 0.1347, 0.2666, 0.1856, 0.2795,\n",
      "        0.3099, 0.1511, 0.1495, 0.2689, 0.2331, 0.0724, 0.2841, 0.2880, 0.2848,\n",
      "        0.1400, 0.3162, 0.2194, 0.1981, 0.2346, 0.1457, 0.2713, 0.2142, 0.1739,\n",
      "        0.2147])), ('encoder.13.running_mean', tensor([ 0.2352, -0.1090,  0.0439,  0.1202,  0.1507,  0.0294,  0.1204,  0.1483,\n",
      "         0.2917,  0.2305,  0.2177, -0.0112,  0.0021,  0.0040, -0.0680,  0.0280,\n",
      "        -0.0367,  0.1132, -0.0091,  0.0391,  0.0999,  0.0217,  0.0542,  0.2423,\n",
      "         0.0891,  0.0713,  0.0533,  0.2351,  0.1399, -0.0065, -0.1495,  0.0204,\n",
      "         0.1371,  0.2253,  0.0337,  0.1032,  0.0462,  0.1016,  0.2261,  0.2121,\n",
      "         0.0232,  0.1980,  0.1380,  0.0703,  0.0778,  0.1159,  0.1173,  0.1237,\n",
      "         0.0468,  0.0100,  0.1608,  0.0024, -0.1890,  0.0725,  0.0576,  0.1006,\n",
      "         0.1031,  0.1444,  0.0310,  0.0464,  0.0439, -0.0870,  0.0402,  0.1661])), ('encoder.13.running_var', tensor([0.1926, 0.3314, 0.1081, 0.1902, 0.0835, 0.2098, 0.2401, 0.0750, 0.1982,\n",
      "        0.1971, 0.1322, 0.1748, 0.1087, 0.0599, 0.1705, 0.2383, 0.2490, 0.1397,\n",
      "        0.0570, 0.1975, 0.1708, 0.2674, 0.1216, 0.1577, 0.2821, 0.2777, 0.1389,\n",
      "        0.2140, 0.2179, 0.1734, 0.2260, 0.1936, 0.0948, 0.2778, 0.1289, 0.1654,\n",
      "        0.1899, 0.0572, 0.1093, 0.2301, 0.2032, 0.1190, 0.2142, 0.0945, 0.2101,\n",
      "        0.0787, 0.2487, 0.1436, 0.1057, 0.0909, 0.1544, 0.1786, 0.1146, 0.1358,\n",
      "        0.1208, 0.4104, 0.1631, 0.1690, 0.2152, 0.1680, 0.0831, 0.0720, 0.0481,\n",
      "        0.2426])), ('encoder.13.num_batches_tracked', tensor(1558)), ('classifiers.0.weight', tensor([[-1.5800e-07,  1.0562e-01,  1.8923e-01, -6.6288e-02, -8.0377e-07,\n",
      "         -1.9103e-08, -7.3117e-07, -2.2561e-06,  1.9989e-07, -2.6171e-06,\n",
      "          2.3194e-01,  2.7033e-01, -5.6689e-07, -2.7707e-07, -2.6843e-07,\n",
      "          1.4354e-01, -4.2619e-02,  1.8081e-01, -1.7001e-06, -5.0146e-03,\n",
      "         -9.7533e-07,  5.4934e-08,  4.2399e-06,  7.5720e-08,  2.0264e-01,\n",
      "          7.9069e-07,  3.7910e-02,  5.6997e-07, -2.2226e-06, -1.7868e-06,\n",
      "         -2.4971e-04, -1.8174e-07,  1.6856e-02, -8.7731e-07, -7.3686e-07,\n",
      "          1.2060e-01,  5.7173e-02, -3.2865e-02, -6.0833e-07, -3.8787e-02,\n",
      "          7.0995e-02,  1.3542e-07,  2.8809e-02, -1.8779e-06, -3.8230e-05,\n",
      "         -4.7879e-02, -6.3384e-07, -1.4757e-06, -7.6800e-07, -6.8352e-07,\n",
      "          4.5883e-07,  7.1998e-07,  6.2722e-02, -1.1221e-02,  6.8196e-07,\n",
      "         -1.3308e-06,  2.2598e-07, -8.9792e-04,  3.4618e-01,  5.9754e-02,\n",
      "          2.9905e-07,  7.4107e-07,  1.2685e-06, -3.0268e-07],\n",
      "        [-1.8790e-01, -2.6210e-02, -7.4401e-02,  1.9004e-03,  1.5569e-01,\n",
      "          2.2778e-06,  9.2142e-06,  6.1206e-06,  3.4871e-01,  2.0018e-01,\n",
      "          1.0779e-02,  1.1240e-01,  2.0333e-01,  1.8438e-01, -1.5289e-02,\n",
      "          2.3273e-01, -2.0172e-01, -2.4246e-02,  1.4361e-01,  2.3352e-01,\n",
      "         -2.2846e-02,  1.7559e-03,  3.0740e-01,  2.4252e-01, -5.4818e-02,\n",
      "          2.8650e-01,  9.0258e-03,  1.5191e-01,  2.1171e-01, -1.4868e-03,\n",
      "          1.1952e-01, -7.6739e-02,  2.6560e-01,  2.5408e-04,  8.9666e-02,\n",
      "          2.9791e-01,  2.7880e-01,  1.5609e-02,  1.2065e-01,  3.8086e-02,\n",
      "         -8.7307e-02, -1.2288e-02,  4.3619e-01,  3.0591e-02, -1.8342e-01,\n",
      "          1.6949e-02, -4.9506e-08,  3.2208e-01,  1.2723e-01,  5.7554e-02,\n",
      "          1.1337e-07,  2.0624e-02, -7.7995e-02,  1.1583e-01, -1.1641e-01,\n",
      "          2.5048e-01,  2.9663e-01,  2.6171e-01,  4.2234e-02, -1.1233e-01,\n",
      "         -8.5346e-03,  9.3290e-06, -2.0806e-01, -3.9718e-03],\n",
      "        [ 2.9566e-07,  2.1587e-07, -9.8100e-08,  1.3255e-08,  4.6789e-08,\n",
      "         -1.1465e-07, -1.5035e-07, -2.1317e-07, -3.3898e-07, -1.2921e-07,\n",
      "         -4.4533e-07,  5.2227e-08,  1.6316e-08,  1.6397e-08,  1.3274e-07,\n",
      "          2.9882e-07, -4.5580e-07, -9.4052e-08,  1.2163e-07,  3.3913e-07,\n",
      "          5.2351e-07, -1.7668e-07, -1.3003e-06, -4.5328e-07, -3.7051e-07,\n",
      "          3.8340e-07, -2.2418e-07, -1.8220e-07, -1.0827e-06,  9.9278e-08,\n",
      "         -4.8506e-07, -2.8382e-08,  4.5845e-07, -1.1836e-08,  2.7566e-07,\n",
      "         -1.1750e-07, -1.2284e-07,  1.2827e-07,  7.8346e-08, -6.4691e-08,\n",
      "          5.7179e-08,  2.9101e-07, -1.8677e-07, -4.0518e-08, -1.6640e-08,\n",
      "         -2.9274e-07, -4.9205e-08, -1.5041e-07, -2.1545e-07, -1.5396e-07,\n",
      "          7.0605e-08, -8.9301e-07,  6.1644e-07, -9.2641e-08, -7.3616e-07,\n",
      "         -3.0042e-07, -3.2387e-07, -1.0796e-07, -9.2657e-08, -5.3694e-07,\n",
      "         -2.4862e-07,  2.8741e-07, -6.0403e-08, -5.5589e-08],\n",
      "        [ 1.9785e-01, -1.5788e-02,  8.5606e-02,  1.6588e-02,  1.3708e-02,\n",
      "          1.6578e-02, -1.6678e-01,  2.7540e-01,  1.6027e-06,  3.4773e-01,\n",
      "          1.0582e-03, -4.0781e-02, -6.4613e-06,  2.5151e-07,  9.3017e-03,\n",
      "         -7.1493e-02, -1.2379e-01, -2.0071e-01, -4.4421e-06,  4.1487e-01,\n",
      "          2.7612e-02,  3.1890e-01, -2.9403e-07,  6.0522e-02,  2.4424e-01,\n",
      "         -7.4140e-02,  2.2852e-01, -1.1640e-02, -6.1150e-07,  2.7445e-05,\n",
      "         -6.6362e-04,  9.8958e-05, -1.1776e-01,  1.9313e-01, -7.7631e-07,\n",
      "          1.1917e-01,  2.4420e-01, -8.7252e-06,  2.5278e-01, -5.9622e-07,\n",
      "          3.0118e-01,  1.9361e-01, -8.0927e-06,  1.3599e-01,  2.3480e-02,\n",
      "         -7.0026e-02,  1.7032e-01, -4.5206e-05, -6.6516e-03,  6.5577e-07,\n",
      "          1.8176e-07, -6.6176e-07,  7.5623e-03, -2.0296e-01,  3.1352e-07,\n",
      "         -6.2517e-02, -1.7304e-01,  3.3223e-01, -3.2706e-02,  1.7453e-01,\n",
      "          2.1800e-01,  2.0192e-04,  1.5797e-01,  8.0473e-03],\n",
      "        [ 4.2798e-06, -4.4865e-02, -3.7423e-03, -3.1829e-06, -6.8082e-06,\n",
      "          1.1511e-02,  1.8116e-01, -5.9871e-06,  2.5996e-07, -7.7795e-06,\n",
      "         -2.9305e-06,  2.1117e-01,  2.4469e-01, -1.5449e-01,  4.2254e-01,\n",
      "         -6.5270e-02,  1.6928e-01,  1.1537e-07, -7.0222e-02, -1.4292e-01,\n",
      "         -5.3543e-04, -1.1066e-06,  1.0661e-01, -3.9285e-05,  2.3643e-07,\n",
      "         -3.3258e-06, -2.3370e-05,  7.7880e-02, -4.9066e-06,  4.1195e-02,\n",
      "         -2.8556e-05, -4.0671e-07, -1.3410e-01, -2.5103e-06, -2.4126e-06,\n",
      "         -2.4451e-02, -1.1309e-01,  3.0272e-01,  9.8512e-08,  2.0710e-01,\n",
      "          1.5956e-01, -5.9164e-03, -4.6881e-06, -6.1807e-07, -3.4251e-02,\n",
      "          8.5143e-09, -8.1023e-07,  9.5701e-07,  2.0403e-01, -2.1650e-03,\n",
      "         -1.2666e-01,  1.0688e-01, -5.5423e-06,  3.8264e-01,  5.5976e-02,\n",
      "          2.3288e-01, -4.1192e-02,  2.2115e-07, -3.1458e-04, -1.4235e-01,\n",
      "         -1.2680e-06, -4.9172e-06,  1.5020e-07,  8.0572e-02],\n",
      "        [-1.0217e-06, -9.7614e-07,  3.1677e-07, -4.3221e-07, -4.0986e-07,\n",
      "         -3.6129e-07, -1.5944e-08, -3.0170e-07,  2.4267e-08,  1.7058e-07,\n",
      "         -1.5688e-06,  1.2676e-07,  6.1733e-07,  3.6025e-07, -3.2058e-07,\n",
      "         -1.9327e-06, -1.5365e-06,  5.1833e-07,  1.1440e-07,  5.4855e-07,\n",
      "         -1.0368e-06, -1.5417e-07, -1.2754e-06, -1.1543e-06, -7.0799e-07,\n",
      "          8.3840e-07,  4.1456e-07,  4.3533e-07, -4.2464e-07, -6.0873e-07,\n",
      "         -7.2240e-07, -4.0173e-07, -4.1374e-07,  1.4501e-07,  7.6683e-07,\n",
      "         -1.0276e-07, -4.5016e-07,  4.3871e-07,  2.1495e-07, -2.1906e-07,\n",
      "         -6.9207e-08,  5.8944e-07, -9.2696e-08,  1.3191e-08, -7.6188e-07,\n",
      "         -1.2478e-06,  1.6985e-07, -8.0559e-07, -2.4893e-07,  3.2070e-07,\n",
      "          1.1139e-07, -8.0237e-09,  4.9310e-07, -2.8787e-07, -7.8362e-08,\n",
      "         -1.3272e-06, -2.3688e-07,  3.7073e-07, -4.6264e-07, -5.5180e-08,\n",
      "          3.8352e-07,  5.5933e-07, -7.6191e-07, -2.4618e-07],\n",
      "        [-8.3004e-02,  2.5972e-01,  1.6038e-01,  2.1002e-07, -1.2446e-06,\n",
      "          3.8391e-01, -3.3806e-02, -8.2575e-02,  4.6464e-02, -6.0004e-08,\n",
      "         -8.6282e-02,  2.2910e-02,  1.7380e-01, -1.1340e-01, -7.0382e-07,\n",
      "         -1.5515e-02,  4.1616e-03, -2.6222e-02,  2.2958e-01, -1.5053e-06,\n",
      "          2.3154e-01, -1.9053e-01, -3.0495e-06, -1.6274e-01, -8.6247e-02,\n",
      "         -5.0485e-02, -6.0033e-02,  1.8199e-01,  9.2120e-02,  1.7861e-01,\n",
      "          2.9787e-01,  3.5958e-01,  2.4395e-01, -3.9669e-02, -9.4261e-02,\n",
      "         -2.0300e-02, -8.5787e-02, -4.6712e-06,  2.2178e-02, -3.9971e-02,\n",
      "          1.7455e-01, -2.5375e-06,  2.3062e-06,  1.2910e-02,  2.4503e-01,\n",
      "          2.6304e-02,  3.2482e-01, -3.1369e-02,  6.3158e-02,  1.9828e-07,\n",
      "          1.9249e-01,  2.6497e-01,  2.6772e-01, -4.2330e-02,  4.0921e-03,\n",
      "          4.5561e-02, -8.2542e-07, -1.2207e-02, -1.6397e-01, -3.5340e-02,\n",
      "          4.5431e-02,  2.2140e-01,  4.2559e-03, -1.9768e-01],\n",
      "        [-3.5402e-02,  1.8268e-01,  4.0941e-06,  1.6903e-06, -1.9628e-04,\n",
      "          8.3942e-02,  2.7648e-01,  1.2959e-01, -6.1314e-07, -9.6854e-02,\n",
      "         -2.6716e-05, -6.0839e-02, -9.9997e-07,  1.6153e-02, -2.3529e-06,\n",
      "          3.7057e-08,  7.8688e-02,  1.7846e-01, -1.9753e-06, -3.5682e-03,\n",
      "          3.2592e-01, -1.4763e-05,  5.9716e-02,  5.4655e-03,  1.0169e-01,\n",
      "          4.3522e-02, -1.1025e-01, -1.2351e-05, -6.4921e-02, -1.0657e-01,\n",
      "         -1.6144e-06, -9.9634e-07, -1.8298e-06,  2.7371e-01,  3.4708e-01,\n",
      "         -4.4172e-07, -5.0831e-07, -3.5058e-06, -9.8557e-07, -1.3676e-05,\n",
      "          4.7267e-07,  2.4092e-02, -1.4776e-06, -4.9816e-05,  6.7175e-02,\n",
      "          9.2316e-08, -7.6476e-02,  2.1898e-01, -1.0513e-05,  1.1691e-02,\n",
      "         -2.4426e-07, -2.8588e-02,  1.2953e-06,  1.5021e-07, -1.0781e-07,\n",
      "         -3.5002e-05, -4.9780e-07, -2.2973e-06,  4.7671e-02,  2.0848e-01,\n",
      "         -1.5057e-05, -4.4216e-06, -1.9628e-02,  4.1617e-01],\n",
      "        [-3.8359e-08, -6.3826e-07, -1.2364e-06,  3.2643e-08, -2.4212e-07,\n",
      "         -2.1561e-07, -2.4470e-07,  1.8763e-08,  2.2209e-07, -4.7512e-07,\n",
      "         -2.9844e-07, -1.4562e-06, -2.5492e-07, -6.4299e-08, -2.4051e-07,\n",
      "         -4.9484e-08, -1.7610e-08, -1.0059e-09, -3.8133e-07, -9.9612e-08,\n",
      "         -2.8218e-08, -4.1024e-08, -2.5199e-07, -8.4323e-07, -3.6663e-07,\n",
      "         -8.8445e-07, -7.9641e-08, -7.6922e-07, -6.6080e-08, -4.4351e-07,\n",
      "          2.2243e-07,  2.2624e-07,  4.1560e-07, -1.7523e-08,  7.5508e-08,\n",
      "         -1.0007e-06, -1.9485e-07,  1.8412e-07,  6.4099e-07, -4.9549e-07,\n",
      "         -1.7177e-07, -2.4341e-07, -1.8494e-07, -1.0303e-07, -7.9754e-07,\n",
      "         -1.3328e-06,  3.3032e-07,  3.4164e-07, -2.7075e-07,  3.6481e-08,\n",
      "         -5.8197e-07, -3.2837e-07,  8.6738e-07, -1.3330e-08, -1.6937e-07,\n",
      "          1.7452e-07, -4.9755e-08, -3.3299e-07,  5.8511e-07,  2.6755e-07,\n",
      "         -3.2690e-07, -6.7327e-08, -7.6854e-08,  9.7800e-08],\n",
      "        [-2.5853e-07,  2.9770e-07, -1.3690e-07,  9.0931e-08, -3.2221e-07,\n",
      "          3.9437e-08, -5.0043e-07, -2.8414e-07,  6.1707e-08, -3.1235e-07,\n",
      "         -7.3744e-07, -2.6055e-07,  2.0984e-07, -3.0709e-07,  1.0607e-07,\n",
      "         -1.1412e-08, -7.9665e-07, -4.8441e-07, -2.3158e-07, -3.2576e-07,\n",
      "          3.5260e-08, -9.5708e-07, -7.7075e-07, -1.5974e-07, -1.0137e-07,\n",
      "         -8.0244e-07,  2.8771e-07, -2.0062e-07,  1.9523e-08, -1.4463e-07,\n",
      "         -1.6929e-06,  5.1696e-09,  2.5072e-07, -1.3704e-07, -6.7213e-08,\n",
      "         -2.0654e-08, -1.1866e-07,  9.3946e-08, -1.3028e-09, -4.5287e-07,\n",
      "          6.6909e-08, -2.9500e-07,  5.9204e-08, -1.9426e-07, -1.4064e-07,\n",
      "          1.6355e-07, -6.5596e-08,  4.6934e-07,  1.7604e-07,  2.8845e-07,\n",
      "         -8.3250e-08, -3.4629e-07, -1.1076e-06, -1.0011e-07, -6.1486e-07,\n",
      "         -8.4288e-08, -6.8215e-07,  7.8886e-07, -1.2109e-06, -7.9712e-07,\n",
      "         -6.0573e-07, -7.1815e-07, -1.8047e-08,  5.0784e-08]])), ('classifiers.0.bias', tensor([ 2.6805e-07,  1.0764e-01, -3.8360e-02,  8.9726e-06, -9.6779e-07,\n",
      "        -9.8295e-02,  5.7989e-07, -5.5182e-06, -3.2144e-02, -4.4915e-02])), ('classifiers.1.weight', tensor([[ 6.6861e-04, -1.3404e-01,  3.0635e-01,  1.0282e-01,  4.7100e-02,\n",
      "         -3.3297e-07, -4.2048e-02,  7.4035e-03,  3.7949e-01,  2.2634e-02,\n",
      "          3.1895e-05,  1.6740e-01, -8.3201e-02,  1.0309e-04,  9.6822e-02,\n",
      "          2.7604e-01, -3.5385e-07, -1.9160e-03,  1.5409e-05,  3.1061e-01,\n",
      "          3.1018e-04,  4.5585e-02,  5.8097e-02, -5.1445e-07, -1.1445e-01,\n",
      "          2.8227e-03,  3.6205e-01, -1.2415e-01,  1.2734e-02, -1.1041e-01,\n",
      "          7.8540e-03, -1.6680e-01,  2.2646e-01,  1.7291e-01,  1.4583e-01,\n",
      "         -1.7202e-01,  2.1985e-01,  8.4803e-06,  7.5420e-03, -9.3505e-05,\n",
      "         -1.7852e-01, -3.1837e-02,  1.9883e-01,  8.9557e-02, -1.7721e-01,\n",
      "          2.7775e-03,  1.8407e-01,  3.4456e-02,  2.6807e-01, -6.0030e-02,\n",
      "          1.6824e-07, -1.4436e-06, -1.2092e-01,  1.3816e-01, -9.2479e-02,\n",
      "         -7.3287e-02,  2.7125e-01,  2.4562e-01, -2.1510e-02, -2.7098e-06,\n",
      "         -8.0007e-02, -2.6124e-04,  1.8930e-02,  1.1842e-01],\n",
      "        [ 1.5736e-01,  1.3451e-01,  1.2108e-01,  3.7334e-02,  6.8561e-03,\n",
      "         -1.5661e-07, -1.0369e-01,  1.9238e-05,  3.6796e-06,  6.7950e-02,\n",
      "          2.0164e-03,  2.0592e-01,  2.7636e-07,  7.2719e-02, -1.9834e-01,\n",
      "          2.0751e-01,  9.3436e-08,  1.5523e-01, -1.5711e-05,  3.4546e-01,\n",
      "         -1.1406e-01,  8.9789e-02,  1.6608e-06, -7.9935e-06,  3.8993e-01,\n",
      "         -8.6203e-02,  9.3534e-02, -2.7246e-07, -7.8450e-02,  1.7157e-02,\n",
      "         -4.4482e-02,  2.6437e-02,  1.2287e-02,  1.1908e-02, -2.3121e-02,\n",
      "          1.9187e-01,  2.6191e-01, -1.1701e-01, -1.5718e-01, -1.9393e-06,\n",
      "          4.0645e-01,  1.1115e-02, -3.4980e-08,  1.6572e-02,  9.5320e-07,\n",
      "          2.0757e-02, -1.1428e-01, -1.8473e-01, -1.5614e-01,  1.1707e-01,\n",
      "         -1.9611e-07, -3.5662e-02,  2.2082e-02, -1.1308e-02,  4.8487e-02,\n",
      "         -1.6383e-01,  1.0941e-05, -3.3084e-02,  1.4027e-01,  2.9506e-01,\n",
      "         -2.8377e-06,  2.2945e-04,  1.2787e-01, -1.5003e-01],\n",
      "        [ 3.8480e-06, -9.4606e-02, -3.4981e-06,  9.8756e-02, -2.5458e-02,\n",
      "          3.1607e-07,  1.4931e-01,  3.1952e-06,  8.6822e-08,  2.5237e-01,\n",
      "         -2.7858e-07,  1.9739e-01,  3.8347e-01,  2.0692e-01,  1.4898e-01,\n",
      "         -1.0818e-01, -1.1445e-01,  7.6816e-03, -6.2921e-07, -4.9920e-02,\n",
      "         -1.1701e-01, -6.2594e-06,  2.3170e-01,  2.4671e-01, -1.5930e-01,\n",
      "          1.0534e-02,  5.3658e-07,  4.1254e-07,  2.2769e-08, -5.7903e-02,\n",
      "         -5.1449e-02, -1.9470e-01, -9.5849e-02, -3.1500e-02, -6.3621e-07,\n",
      "          1.2999e-01, -4.9324e-07,  6.3037e-02,  1.1594e-01,  1.2331e-03,\n",
      "          1.3538e-01,  7.9903e-02,  3.2593e-02, -5.0979e-06, -1.6723e-01,\n",
      "          4.1548e-02, -1.9353e-01, -2.3557e-02,  7.1746e-02,  1.5746e-01,\n",
      "          3.8907e-02,  2.2797e-01,  1.0554e-02,  9.1656e-02,  2.6996e-01,\n",
      "          3.8781e-01,  2.4431e-02, -1.1964e-01,  1.7792e-01, -3.1955e-02,\n",
      "          1.8346e-02,  3.3839e-03, -3.9535e-02,  1.2930e-02],\n",
      "        [-2.5561e-07, -2.6694e-07,  4.6079e-07,  3.3267e-07, -2.0172e-06,\n",
      "         -2.2775e-08, -8.0602e-07, -4.4006e-07, -1.4172e-07, -1.4015e-06,\n",
      "          2.6981e-07,  5.1726e-07, -3.5903e-07, -1.1563e-07, -4.9378e-07,\n",
      "          4.5555e-07, -8.2603e-08,  1.3579e-07,  8.5699e-08,  2.9013e-08,\n",
      "         -6.3634e-07, -3.6187e-07, -1.5865e-07, -3.8956e-07, -2.5400e-09,\n",
      "         -7.3212e-07,  4.5216e-07, -5.0278e-07, -3.9181e-07,  5.9998e-07,\n",
      "         -5.2996e-08, -1.0981e-06, -2.5469e-07,  4.1226e-07,  2.7220e-07,\n",
      "          6.8943e-07, -2.8290e-07,  4.1540e-07,  4.0428e-07, -6.2981e-07,\n",
      "         -2.6433e-07, -2.9531e-07, -4.8280e-07,  2.1872e-07,  2.4010e-07,\n",
      "         -1.2992e-07,  3.4478e-08, -5.8034e-09, -1.2655e-06,  5.5929e-07,\n",
      "          6.1504e-07, -1.9147e-06,  1.1769e-07,  2.8908e-07, -1.8214e-06,\n",
      "         -1.3960e-07, -1.4826e-07,  2.3682e-08,  1.6537e-08, -5.7716e-07,\n",
      "          1.0761e-09, -1.1027e-06,  2.6785e-07, -3.8172e-07],\n",
      "        [ 2.9375e-08,  4.7656e-07, -5.2010e-07, -2.9456e-07, -2.0697e-07,\n",
      "          3.0901e-07,  4.2272e-08, -7.9320e-07,  5.5916e-08, -4.1169e-07,\n",
      "          2.1798e-07, -3.8898e-07, -7.2634e-08, -7.6314e-07, -7.5131e-07,\n",
      "         -2.0160e-07, -1.6135e-06, -3.2253e-07,  2.0206e-07, -7.4344e-08,\n",
      "          1.4082e-07, -1.1028e-06, -4.4310e-08,  1.7992e-07, -2.9355e-07,\n",
      "          7.3350e-08,  1.9485e-07, -2.9370e-08, -3.2949e-07, -1.4495e-08,\n",
      "         -5.6698e-08, -5.2824e-07, -1.0573e-06, -4.1704e-07,  1.5109e-07,\n",
      "         -2.3219e-07, -1.1534e-07,  4.7853e-07,  1.0728e-08, -1.9978e-07,\n",
      "          4.7011e-07, -2.5174e-07, -1.6565e-07, -2.4302e-06,  2.9548e-09,\n",
      "         -2.3860e-07, -3.1222e-07,  4.8568e-08,  1.6359e-07, -2.4064e-07,\n",
      "          6.5560e-08,  2.2240e-07,  4.2639e-07, -7.1121e-07, -1.9279e-08,\n",
      "          1.0687e-07,  4.0879e-07,  1.2193e-07, -1.4366e-07, -7.8853e-07,\n",
      "         -4.5740e-07, -3.9800e-07,  1.8472e-07, -2.0485e-09],\n",
      "        [-1.8240e-02,  1.0393e-01, -6.2722e-02, -2.7139e-01,  4.9444e-02,\n",
      "          3.9703e-01,  5.9621e-02, -1.7250e-02,  3.5225e-07, -9.0591e-02,\n",
      "          6.2565e-08,  2.2507e-01, -1.3062e-01, -1.2710e-02,  2.8754e-01,\n",
      "         -3.3697e-02,  1.2312e-01,  4.8595e-07,  3.1486e-01, -5.8852e-07,\n",
      "          1.5056e-01, -7.0828e-02,  3.0992e-01, -2.4768e-01, -2.5547e-02,\n",
      "         -1.1798e-06, -7.8425e-08,  1.2537e-01, -5.1199e-02,  2.7069e-01,\n",
      "          2.2326e-01,  9.1404e-02, -9.7886e-02, -2.9059e-05, -2.1069e-02,\n",
      "         -1.6590e-01, -1.5677e-01,  1.0772e-01,  1.8342e-02,  2.7455e-01,\n",
      "         -1.5642e-04, -2.5008e-02, -1.5725e-01, -1.1292e-01,  5.3779e-02,\n",
      "         -1.9662e-02,  2.8405e-05, -5.0454e-04,  1.1795e-02, -8.9590e-05,\n",
      "          2.0419e-01,  2.1891e-01,  1.0047e-02,  8.1639e-03,  1.6867e-01,\n",
      "          1.1930e-01, -3.2800e-07, -1.9530e-06, -8.7839e-05,  1.3345e-07,\n",
      "          1.0035e-01,  2.0719e-01,  5.6662e-02,  1.5644e-01],\n",
      "        [-1.2613e-06,  8.8454e-02,  5.1218e-06,  6.2609e-07, -2.7623e-04,\n",
      "         -5.0147e-06,  3.5485e-01,  2.0754e-02, -2.4516e-02, -1.4590e-01,\n",
      "          6.4714e-07, -3.6543e-07, -8.1658e-07,  9.5361e-02,  2.5143e-06,\n",
      "          3.5853e-07,  2.0974e-01,  3.7538e-01, -2.9785e-06, -1.3224e-07,\n",
      "          1.7714e-01,  7.3300e-02, -7.3592e-04,  5.5304e-07,  1.1043e-01,\n",
      "          5.3809e-06, -6.6619e-02,  1.4455e-06, -3.9025e-02, -2.1137e-01,\n",
      "          2.7074e-03, -6.8338e-06,  7.2787e-06,  2.1082e-02,  2.6841e-01,\n",
      "         -1.7028e-07, -1.6805e-01, -7.5019e-06, -1.0300e-06, -9.3508e-02,\n",
      "         -3.1639e-02,  5.0982e-02, -5.9423e-02, -1.1438e-05,  2.1023e-01,\n",
      "          1.2613e-06, -2.0409e-02,  1.4754e-02, -9.5796e-02,  7.8465e-05,\n",
      "         -1.5518e-06, -4.1468e-04, -2.0670e-05, -7.9436e-03, -1.5988e-06,\n",
      "         -3.2557e-04, -2.8995e-03, -8.4892e-06,  3.9244e-07,  2.6119e-01,\n",
      "         -8.6518e-02, -1.4669e-02, -3.3763e-02,  4.0747e-01],\n",
      "        [ 2.4195e-08, -1.4227e-06,  3.5165e-02,  2.2225e-01,  1.0418e-01,\n",
      "          1.2806e-06,  1.1432e-06, -9.2651e-07, -6.7392e-02, -1.8406e-01,\n",
      "         -1.0999e-07, -1.9110e-02,  3.4236e-01, -1.2956e-01, -1.7056e-01,\n",
      "         -4.3163e-06, -1.3967e-02, -3.8982e-07,  1.0752e-02,  1.9307e-01,\n",
      "          5.8380e-07, -6.2015e-05, -1.5494e-01, -5.9600e-07,  4.6326e-07,\n",
      "          5.0002e-06, -8.1784e-02,  3.1823e-01,  2.3651e-01,  3.2843e-01,\n",
      "          3.5312e-07, -1.0082e-06,  3.5637e-01, -5.7086e-02, -3.2057e-04,\n",
      "          6.5297e-08,  2.1896e-05,  4.4685e-03, -1.7649e-02,  5.5598e-03,\n",
      "          1.1246e-05, -3.8010e-06,  1.0526e-02,  5.1084e-02, -8.5172e-07,\n",
      "          2.0632e-01,  1.5548e-01,  1.3408e-01, -1.4439e-01, -2.9293e-07,\n",
      "         -5.7297e-07, -1.0057e-02, -1.3405e-02,  6.1511e-05, -1.2454e-01,\n",
      "          1.5945e-01, -4.1783e-02, -5.8006e-02,  1.0645e-02,  2.9028e-07,\n",
      "          1.0996e-07, -1.8599e-06, -4.3183e-02, -1.6875e-06],\n",
      "        [ 1.8321e-01, -1.2493e-01, -6.7313e-07, -1.0836e-02, -7.4272e-07,\n",
      "         -1.3193e-08, -4.1067e-07,  1.8421e-01, -6.8537e-02, -2.5878e-06,\n",
      "          3.1882e-01, -1.6403e-07,  4.8075e-08, -1.1121e-05, -4.3920e-07,\n",
      "          3.2605e-07, -6.9277e-02, -1.3215e-06, -1.7111e-06, -1.3716e-06,\n",
      "          1.0924e-01, -3.1199e-03, -5.5424e-03,  3.2883e-01, -7.8200e-07,\n",
      "         -1.3954e-01, -1.0763e-06, -1.0712e-06, -2.3455e-07,  1.4971e-07,\n",
      "         -6.8196e-02, -3.8997e-02, -1.3940e-03,  1.5526e-01, -1.6459e-02,\n",
      "         -2.5713e-02, -4.5951e-06,  2.3658e-07,  2.2217e-01, -1.3678e-06,\n",
      "          2.7013e-04,  5.2002e-02, -1.0701e-07, -6.2758e-07,  1.9106e-01,\n",
      "         -1.5281e-02,  8.5893e-08,  5.8685e-08, -1.5752e-07, -3.9028e-05,\n",
      "          7.0647e-02, -1.2587e-02,  9.3751e-08, -9.9646e-07, -6.0385e-06,\n",
      "         -1.1322e-06,  5.2601e-08,  8.1356e-02, -1.4363e-06,  1.8284e-07,\n",
      "          5.6354e-02, -4.0648e-07,  2.5933e-05, -3.4932e-07],\n",
      "        [ 8.4940e-08, -8.4967e-07,  5.4891e-07, -2.7125e-07, -6.3431e-07,\n",
      "         -4.3917e-07, -8.8378e-07, -1.0801e-06,  8.5918e-08, -3.7005e-07,\n",
      "          3.2586e-08,  4.7712e-07, -4.8538e-08, -4.4052e-07, -5.4534e-07,\n",
      "          6.8779e-07, -1.0226e-06, -5.9542e-08,  1.6160e-07,  2.2361e-07,\n",
      "          5.2112e-08,  1.4907e-07, -5.5367e-07,  4.3677e-07, -1.4573e-06,\n",
      "          9.5270e-08, -1.8976e-07,  9.6206e-07, -3.3849e-07, -2.2529e-07,\n",
      "         -7.3750e-08, -1.1933e-06, -8.7151e-07, -6.4244e-07, -2.0463e-06,\n",
      "         -4.0644e-07, -1.5486e-06, -1.3017e-07,  1.6801e-07, -7.1595e-07,\n",
      "          3.4955e-07,  2.6619e-07,  4.7648e-07, -1.6511e-07, -3.0793e-07,\n",
      "          3.6994e-09, -1.1508e-06, -2.2411e-07,  1.1268e-07,  3.3497e-07,\n",
      "         -3.5489e-07,  5.1022e-07,  1.3879e-07, -4.4955e-07, -1.0860e-07,\n",
      "         -4.8540e-07, -4.4933e-07, -6.5994e-08, -8.6830e-08, -1.6701e-07,\n",
      "          3.7994e-09,  4.9916e-09, -3.2774e-07, -2.6916e-07]])), ('classifiers.1.bias', tensor([ 2.7726e-02,  1.9510e-02,  2.8206e-02, -1.9353e-01, -1.0464e-01,\n",
      "         4.4304e-02, -1.5671e-06, -4.3614e-08, -1.4059e-07, -2.4927e-01])), ('classifiers.2.weight', tensor([[ 4.0802e-01, -2.8636e-07, -1.3437e-01,  1.2092e-01,  5.3528e-04,\n",
      "          4.0811e-02, -8.4410e-07,  5.2290e-02,  1.0790e-07,  3.4290e-02,\n",
      "          9.4959e-02, -4.0412e-02, -5.3716e-02,  1.0071e-01, -5.2924e-06,\n",
      "         -4.4608e-06,  4.2709e-07, -1.0847e-01, -1.2655e-07,  1.1831e-02,\n",
      "          4.1253e-05,  3.9732e-02, -7.9138e-07,  2.7987e-01,  1.4831e-01,\n",
      "         -2.2757e-06, -5.8342e-06, -2.7820e-07,  3.6859e-07, -8.2367e-08,\n",
      "         -8.1851e-02,  1.3804e-01, -1.1216e-01,  7.4303e-02, -1.8965e-02,\n",
      "         -8.7077e-07,  1.8033e-03, -1.1567e-02,  2.2916e-01, -8.0221e-07,\n",
      "          9.9850e-02,  2.9369e-01, -2.7739e-02,  1.1623e-01,  3.0540e-01,\n",
      "          1.2079e-01, -1.0213e-05, -6.3115e-06, -1.2680e-03,  2.7181e-01,\n",
      "         -6.1831e-07, -2.4438e-07, -1.1731e-01, -2.4209e-01,  1.0759e-01,\n",
      "         -8.7187e-02, -6.6736e-02,  2.4500e-02, -1.0599e-01, -5.9813e-02,\n",
      "          3.7883e-02, -7.9242e-07,  9.3363e-02, -1.0901e-01],\n",
      "        [-1.3383e-06, -9.1690e-09, -2.9313e-07, -5.4135e-08, -8.9342e-07,\n",
      "         -5.1158e-07, -3.0321e-07, -1.1475e-06,  5.5041e-09, -3.7007e-07,\n",
      "          3.1658e-07, -7.5917e-07,  3.1106e-08,  3.6285e-07, -5.6671e-07,\n",
      "          2.0730e-07,  9.5385e-08, -5.0037e-07,  1.8636e-08, -4.2357e-08,\n",
      "          3.0458e-07, -3.3734e-08, -4.4328e-07, -2.6910e-07,  1.5942e-07,\n",
      "         -6.7532e-09, -7.2886e-07, -4.5010e-07, -1.1569e-06, -3.7836e-07,\n",
      "         -2.9705e-07, -6.7434e-07,  7.1821e-09, -1.4050e-06,  2.0224e-07,\n",
      "         -8.4895e-07,  3.3662e-07, -7.3283e-08,  4.0363e-07, -2.1821e-08,\n",
      "         -1.6155e-07,  3.0622e-08, -1.8704e-08, -4.9217e-07, -3.9557e-08,\n",
      "          1.5436e-08,  9.0547e-07, -5.1619e-07, -1.7400e-06, -5.6690e-07,\n",
      "         -3.0024e-07, -3.4596e-07, -4.7381e-07, -4.1158e-07, -4.4713e-08,\n",
      "          1.9482e-08, -1.8233e-06, -4.9938e-07,  2.7709e-07, -1.0878e-07,\n",
      "          1.7428e-07, -2.4730e-07,  2.5590e-07,  2.0258e-07],\n",
      "        [-5.8506e-06,  4.9351e-02,  2.0579e-01, -2.0588e-01,  4.3160e-02,\n",
      "         -4.5678e-07, -4.8980e-07, -8.5257e-07, -1.8400e-02, -9.7914e-06,\n",
      "          3.6623e-02,  5.0692e-02, -1.9017e-02, -4.1330e-06, -6.7127e-02,\n",
      "          2.4987e-01, -9.6907e-07,  6.3659e-02, -3.1602e-06, -6.0556e-02,\n",
      "         -1.1768e-01,  1.2737e-01, -2.0140e-02, -4.9111e-07,  1.7430e-01,\n",
      "          8.5734e-07,  1.1514e-01, -8.2459e-02, -3.1251e-03,  4.4180e-02,\n",
      "         -1.2047e-01, -4.1920e-06, -6.5794e-05, -1.6474e-01, -4.1692e-07,\n",
      "          2.8261e-01,  2.6207e-01, -5.7941e-02, -1.5690e-07,  3.9253e-07,\n",
      "          2.2322e-01, -1.2484e-01, -5.9134e-04,  6.2090e-03, -2.3585e-06,\n",
      "         -1.6381e-02, -1.3180e-01, -9.1754e-02, -1.5189e-02, -7.9378e-02,\n",
      "         -2.5079e-07, -1.1727e-06,  2.6449e-01, -1.9139e-01, -5.0579e-07,\n",
      "         -4.5557e-08,  3.4491e-06, -5.5211e-04,  2.3812e-01,  3.4499e-01,\n",
      "         -2.3262e-06,  1.7338e-01,  1.2253e-01, -3.1066e-06],\n",
      "        [-6.5855e-05, -8.7964e-07, -2.2738e-01,  1.2716e-02,  3.7223e-01,\n",
      "          7.6355e-07, -2.8478e-07, -1.1726e-01,  1.7536e-01, -6.3469e-02,\n",
      "          1.8065e-06, -1.3069e-01,  6.6822e-02,  2.2917e-02, -2.5536e-06,\n",
      "          3.6297e-02,  8.9219e-08, -1.5926e-06,  2.1665e-02,  6.5697e-02,\n",
      "         -3.8778e-02,  1.9839e-01, -3.8091e-06, -5.3735e-06, -1.4900e-02,\n",
      "          1.2956e-01,  1.0144e-07,  2.3698e-01,  6.6281e-05,  5.6590e-02,\n",
      "          1.2874e-06, -3.9019e-07,  2.2441e-01,  2.7451e-04,  2.3992e-02,\n",
      "          1.8898e-07,  8.2879e-02,  1.1099e-01, -8.8634e-04,  1.6923e-02,\n",
      "         -1.9722e-01,  3.8795e-06,  1.3478e-01,  2.5228e-01,  9.8719e-08,\n",
      "          1.2224e-03,  7.0137e-02,  4.1014e-01,  4.8638e-06, -5.4078e-06,\n",
      "          5.9679e-09,  2.9872e-07, -7.6920e-02,  2.8613e-02,  1.4216e-07,\n",
      "          1.1016e-02,  6.1580e-06, -1.4445e-01,  1.0107e-02,  4.0498e-07,\n",
      "         -5.2236e-08, -2.5837e-02, -2.6888e-02,  2.0013e-07],\n",
      "        [-3.5615e-02, -1.1989e-06,  1.5665e-01, -5.3302e-06, -1.0992e-01,\n",
      "         -3.5323e-07,  1.6846e-06, -2.0529e-06, -3.2368e-06, -4.0013e-03,\n",
      "         -7.3519e-07, -3.9599e-02,  2.2906e-01, -4.3242e-06, -3.5949e-02,\n",
      "         -2.8536e-06, -1.4548e-06, -5.6715e-07, -4.4319e-07, -7.4285e-02,\n",
      "          5.9681e-03, -2.5044e-06, -4.6763e-06, -8.9973e-07,  1.0695e-06,\n",
      "         -1.4661e-02, -1.5043e-08,  1.4733e-01,  6.2933e-02,  2.4785e-01,\n",
      "          5.8136e-07, -6.0141e-08,  1.5831e-01, -5.2903e-06,  1.7560e-07,\n",
      "         -3.2622e-02, -1.1726e-01, -4.5636e-02, -3.4622e-06, -1.5290e-07,\n",
      "          2.5703e-02,  4.0188e-07, -1.7525e-01,  1.9957e-01,  2.1957e-07,\n",
      "          1.7500e-01,  2.7504e-01, -1.2794e-02, -2.4554e-02,  1.7500e-07,\n",
      "          6.1863e-07, -2.1375e-07,  1.4495e-01, -1.1254e-01, -1.2773e-06,\n",
      "          5.6039e-07, -4.3098e-03, -3.1639e-02, -1.6229e-02, -2.2334e-02,\n",
      "         -2.4247e-06,  1.6359e-06, -2.3607e-07, -4.8480e-06],\n",
      "        [ 2.9916e-08, -9.1276e-07, -1.1434e-07, -3.1619e-06,  3.1158e-07,\n",
      "         -2.5220e-07, -4.8183e-07,  7.5827e-07,  2.0764e-07,  1.1083e-07,\n",
      "         -1.1269e-06, -1.5470e-06, -8.0016e-07,  5.2009e-08, -2.5625e-07,\n",
      "          4.3674e-07, -4.7549e-07,  5.3966e-07,  2.0868e-07, -1.4582e-06,\n",
      "          2.4608e-07,  1.9789e-07, -3.8006e-07, -5.4089e-07,  9.6799e-08,\n",
      "         -2.7556e-07, -2.8619e-07, -5.7300e-07, -1.0763e-06, -4.6454e-07,\n",
      "         -1.0213e-07, -5.8000e-08, -1.8722e-08, -6.7509e-07,  5.4365e-07,\n",
      "         -2.3502e-07, -1.8882e-07, -6.0943e-08,  4.7236e-07, -6.3851e-07,\n",
      "         -1.8289e-06,  1.9815e-07, -2.2272e-08,  9.7396e-07, -3.6785e-08,\n",
      "         -3.4354e-06,  3.3244e-07,  5.4771e-07, -1.3891e-07, -5.2016e-07,\n",
      "         -6.8831e-07,  1.9587e-07,  4.6511e-08, -9.2056e-08, -2.6651e-08,\n",
      "         -3.2939e-07, -1.3941e-07, -9.7424e-07,  5.2919e-07, -3.0743e-07,\n",
      "          5.6010e-07, -1.3758e-06, -1.0504e-06, -2.8910e-07],\n",
      "        [-1.6278e-02,  5.6377e-08,  4.4274e-06,  1.7885e-01, -1.0224e-01,\n",
      "          6.1352e-07, -2.6351e-08, -4.2711e-06,  3.4908e-01,  1.0115e-06,\n",
      "         -8.8728e-02, -9.0568e-07, -7.2189e-02, -7.1709e-06, -1.4504e-07,\n",
      "         -6.2235e-02, -1.3456e-06, -1.0401e-02,  6.2793e-03, -6.5200e-02,\n",
      "          5.7603e-02, -1.0474e-01,  6.7066e-02, -7.0189e-04, -5.1581e-08,\n",
      "          9.2705e-02, -6.1139e-07,  1.3285e-06,  1.4367e-01, -4.4436e-02,\n",
      "         -3.6601e-02, -9.7285e-07, -7.6346e-02, -2.4882e-06, -1.6930e-06,\n",
      "         -4.6691e-08,  1.6517e-07, -1.1873e-02,  4.7381e-02, -3.2402e-07,\n",
      "         -1.3055e-01, -2.2706e-06,  2.0242e-01, -3.0441e-07, -4.3934e-07,\n",
      "          9.3422e-06,  1.2621e-01, -6.0919e-06,  2.3883e-01,  2.5755e-07,\n",
      "         -7.3824e-09, -6.3463e-08, -6.9381e-07,  1.7053e-02, -1.8755e-07,\n",
      "         -4.6428e-04,  1.3300e-01,  2.1727e-01, -8.5555e-02,  1.7671e-07,\n",
      "         -1.8598e-06, -3.8254e-07,  9.5965e-08,  5.5504e-07],\n",
      "        [ 8.7349e-02, -2.0039e-01,  2.6485e-01,  7.2456e-08, -7.3230e-07,\n",
      "          2.5911e-06,  7.2755e-08,  1.0551e-02,  1.1073e-02,  7.9787e-03,\n",
      "          5.7983e-02,  3.7341e-01,  6.3369e-02, -6.7928e-06,  4.1277e-01,\n",
      "          7.2398e-02, -1.6775e-06, -1.5359e-01,  6.6002e-06,  3.1316e-02,\n",
      "         -3.2266e-02,  1.2085e-02, -3.1119e-05, -1.4221e-07, -4.2962e-02,\n",
      "         -1.1773e-01,  3.7287e-01, -1.0510e-01, -2.6054e-02, -1.8651e-01,\n",
      "          2.5257e-03, -4.9532e-06,  2.3269e-02, -9.5949e-06,  1.5219e-01,\n",
      "         -2.6574e-05,  3.1161e-01,  1.9391e-01, -5.5878e-02, -1.4243e-06,\n",
      "         -1.2779e-02, -4.6323e-03, -8.2634e-02, -2.5187e-01, -7.8957e-02,\n",
      "          1.1602e-01,  1.5113e-01,  1.4677e-02,  1.0341e-01,  9.0071e-07,\n",
      "          2.7382e-07, -4.3194e-02,  1.5815e-01,  1.0398e-01,  2.7080e-01,\n",
      "          1.1972e-01,  6.1117e-03,  2.5255e-01, -5.2636e-02, -6.2563e-03,\n",
      "          1.5806e-02, -8.9996e-02,  3.9776e-05,  2.7127e-02],\n",
      "        [-1.6594e-01,  1.2444e-03, -1.6790e-01, -1.0818e-01,  1.2598e-06,\n",
      "          7.1547e-02,  4.6293e-01,  6.3675e-02, -7.2935e-05,  6.8725e-02,\n",
      "         -2.0845e-02, -6.2696e-07, -3.4966e-02,  1.2498e-01,  2.5431e-01,\n",
      "         -2.7592e-02,  4.0388e-01,  1.3753e-01, -1.2981e-01, -3.0379e-01,\n",
      "          1.8215e-02, -1.1033e-05,  2.9106e-01,  2.4790e-01, -9.1523e-07,\n",
      "          3.8042e-01, -3.3154e-07,  1.8964e-01, -9.9362e-02,  3.1335e-04,\n",
      "          4.1710e-07, -1.6234e-06, -1.5138e-03,  6.8930e-06,  3.0703e-01,\n",
      "          2.4226e-01, -6.3131e-07,  3.6394e-04,  6.7946e-06,  3.1316e-01,\n",
      "         -1.5166e-01,  2.4970e-01,  1.0239e-06, -6.3514e-02,  2.8685e-02,\n",
      "         -1.4628e-01, -9.6425e-04,  1.3150e-06, -3.9480e-02,  2.8751e-01,\n",
      "          7.8662e-07,  1.3337e-01,  9.8238e-07,  8.7140e-04, -2.6985e-02,\n",
      "          2.1806e-01,  7.2385e-06, -2.0379e-01,  1.4509e-01,  1.3291e-01,\n",
      "         -2.0253e-02,  3.7579e-02, -5.8395e-08,  2.2871e-01],\n",
      "        [-6.8925e-02,  3.2880e-01, -1.0628e-01, -6.4312e-02, -1.6085e-02,\n",
      "          3.2960e-01, -1.8720e-06, -1.1064e-01, -5.5478e-07, -1.2929e-01,\n",
      "          3.2986e-07,  8.1378e-04, -1.4117e-01, -2.3990e-06,  3.9159e-02,\n",
      "         -3.7205e-07,  2.2008e-01, -1.2326e-06,  2.8349e-01, -4.8921e-04,\n",
      "          6.6972e-08, -3.7990e-07, -1.2122e-06,  2.6131e-09, -6.1501e-02,\n",
      "         -9.9789e-07, -1.0135e-06, -6.2241e-07, -1.1627e-07,  1.4207e-07,\n",
      "          2.8073e-01,  3.1650e-01, -2.2700e-06, -2.9533e-06, -3.4968e-02,\n",
      "         -2.9724e-07, -1.9925e-03, -6.6222e-07, -2.4723e-07,  1.9521e-07,\n",
      "         -6.1810e-02, -6.5267e-07, -7.3454e-03,  5.5342e-07,  2.3135e-02,\n",
      "         -6.0584e-02, -3.4488e-08, -8.6559e-07,  1.6772e-01,  1.0668e-01,\n",
      "          2.4510e-01,  3.4360e-01,  2.0250e-01, -1.4512e-01,  2.3303e-01,\n",
      "         -7.4517e-02, -5.6878e-02, -2.1144e-07, -1.3798e-01, -8.6688e-02,\n",
      "          7.9067e-02,  1.8196e-01,  3.6011e-02, -1.6794e-06]])), ('classifiers.2.bias', tensor([ 1.6482e-07, -1.6562e-01, -3.6815e-06,  8.8795e-07, -3.0985e-06,\n",
      "        -2.1482e-01, -1.1176e-06,  2.0293e-02,  8.2437e-02,  1.1526e-06])), ('classifiers.3.weight', tensor([[-2.6867e-03, -3.2814e-02,  2.0047e-04,  2.8051e-04,  3.8181e-02,\n",
      "          6.0599e-06,  1.9040e-01,  6.6852e-05, -1.3259e-01,  7.0292e-06,\n",
      "         -1.5780e-05, -1.2306e-01,  1.7783e-01,  1.8512e-04,  1.0403e-01,\n",
      "         -4.2909e-06,  4.6429e-06, -1.8073e-02, -5.9998e-07,  2.7389e-01,\n",
      "         -1.4658e-02,  1.0323e-01,  5.0361e-02,  2.2318e-04,  4.2421e-02,\n",
      "          1.4256e-01, -4.4562e-07,  4.9561e-01,  2.1897e-01,  5.4712e-01,\n",
      "         -1.5289e-01,  2.7003e-07,  8.3370e-02, -6.5924e-02, -1.5403e-01,\n",
      "          1.1637e-01,  3.1942e-03,  1.0057e-01, -5.7175e-07,  4.3617e-01,\n",
      "          1.0188e-01, -1.8100e-01,  6.1333e-02,  3.4848e-01, -7.4603e-02,\n",
      "          2.6577e-02,  3.8171e-02,  1.3835e-04, -2.5531e-01, -1.7096e-02,\n",
      "          2.4806e-07,  3.4996e-01,  2.9885e-01,  2.0151e-02, -1.4306e-01,\n",
      "          2.8217e-01, -1.1494e-01, -1.3919e-01,  3.4754e-02,  3.2022e-02,\n",
      "         -1.4457e-05,  3.5635e-01,  1.5972e-01,  1.4215e-05],\n",
      "        [ 3.6985e-02, -7.1159e-07,  1.6806e-02, -4.3536e-05,  1.6016e-06,\n",
      "         -5.9232e-07, -5.7981e-05,  3.0817e-02,  7.4483e-06,  7.0569e-02,\n",
      "         -8.9783e-03, -5.1266e-06,  2.2473e-06, -4.4332e-06,  3.1677e-01,\n",
      "         -6.1200e-02, -8.4034e-06, -3.6014e-02, -1.0874e-06,  5.7106e-02,\n",
      "         -5.2414e-06,  6.3889e-02, -1.6151e-05,  2.2174e-07,  4.1586e-02,\n",
      "         -3.0219e-06,  3.0430e-01, -2.8927e-06, -2.8516e-07, -2.4832e-07,\n",
      "          3.5276e-06,  8.3696e-08, -8.2544e-02,  4.7918e-02,  2.2957e-02,\n",
      "         -2.1969e-07,  1.5457e-01, -1.3249e-04, -1.7072e-06,  1.9407e-07,\n",
      "         -1.2127e-01, -7.2368e-07, -4.2835e-03, -5.3149e-02,  1.9154e-06,\n",
      "         -1.5286e-02,  3.3738e-01, -6.7342e-02, -7.1677e-02,  9.2226e-07,\n",
      "          2.1698e-08, -1.6809e-07, -2.5633e-06, -3.6249e-02,  6.3358e-07,\n",
      "         -5.3352e-02, -3.5249e-02,  2.2539e-01,  2.5743e-07, -6.0974e-02,\n",
      "         -2.0640e-06, -1.0227e-06,  7.2232e-02,  1.8379e-01],\n",
      "        [-1.2780e-05,  1.5082e-01,  5.8192e-05, -2.1144e-02,  6.4653e-03,\n",
      "          1.8228e-01,  1.4904e-01,  2.7997e-04,  8.9865e-02, -8.5441e-04,\n",
      "          2.4923e-02,  3.6409e-01, -7.7289e-02,  2.3820e-01, -7.1811e-06,\n",
      "          1.7480e-01,  2.5997e-01, -1.6604e-06,  1.0357e-01,  2.2345e-01,\n",
      "         -1.2364e-06,  6.1368e-06, -8.9005e-02, -1.3187e-01,  8.2996e-02,\n",
      "         -5.2144e-02, -5.6246e-02, -4.1960e-02, -3.9632e-07,  1.4747e-01,\n",
      "          4.4993e-01,  4.9825e-01,  9.9144e-02,  2.6843e-01,  5.2944e-02,\n",
      "         -5.9374e-02,  1.0731e-02,  1.0002e-04, -2.0870e-03, -2.0007e-06,\n",
      "          4.4890e-02,  6.5310e-02, -1.7833e-02, -9.6042e-02,  2.5303e-01,\n",
      "          2.8134e-01, -1.2214e-01,  7.4184e-04,  7.7721e-02,  3.4244e-01,\n",
      "          1.2792e-01,  2.2449e-01,  9.3530e-06,  1.3346e-03,  1.1651e-01,\n",
      "         -1.3555e-01,  3.4580e-02,  5.4260e-02, -1.8102e-06, -1.2324e-04,\n",
      "          2.6464e-01,  1.8866e-01,  3.0412e-02,  1.3379e-01],\n",
      "        [ 8.6551e-07, -3.3780e-08, -1.0965e-01,  1.2201e-01, -2.1021e-01,\n",
      "         -5.7819e-06,  8.6286e-03, -2.3518e-03,  6.7174e-02,  3.0762e-02,\n",
      "         -1.3378e-01,  3.5498e-02,  1.0661e-01,  8.0085e-02,  1.7258e-01,\n",
      "         -1.4067e-01, -9.7794e-02,  1.3458e-02, -6.3626e-07, -2.4205e-01,\n",
      "         -1.4044e-06, -2.8847e-06,  5.4159e-02,  1.0634e-03, -1.5459e-01,\n",
      "          3.6691e-01, -2.4694e-02, -1.8988e-01,  2.8693e-01, -1.6224e-01,\n",
      "         -1.2994e-02, -1.1700e-07, -3.2583e-02, -4.1051e-06, -9.2785e-02,\n",
      "          2.0642e-02, -5.2654e-05,  7.7761e-03,  2.7641e-01, -4.6306e-02,\n",
      "          2.5085e-01,  1.7777e-01,  2.9860e-01, -2.0200e-03, -1.7420e-02,\n",
      "          1.7926e-02,  8.6054e-06, -5.4177e-07,  3.8866e-01,  2.0230e-01,\n",
      "         -1.0396e-01, -1.1363e-01,  1.0538e-01,  1.4896e-01,  2.2317e-01,\n",
      "          3.7732e-01,  3.0114e-01,  2.2875e-03,  1.7919e-01, -9.7345e-07,\n",
      "          1.3381e-01, -4.8427e-06, -1.6500e-01,  2.6027e-03],\n",
      "        [-7.2377e-07, -1.9042e-07, -9.2513e-07, -6.1250e-07, -8.5668e-08,\n",
      "         -4.2280e-07, -9.0629e-08, -6.4233e-08,  1.8500e-07, -1.3063e-07,\n",
      "          1.4518e-07,  1.5826e-07,  1.3778e-07,  3.5261e-07,  8.1989e-08,\n",
      "          1.5708e-07, -8.1578e-08, -6.6484e-07,  2.4104e-07, -2.4735e-07,\n",
      "          3.3566e-07, -4.0579e-07, -4.4571e-07,  9.0950e-08,  4.6477e-07,\n",
      "         -5.0202e-08, -3.1169e-07,  5.8091e-08, -3.7907e-07, -8.7504e-08,\n",
      "         -1.4644e-07, -3.4028e-07,  4.0026e-07, -7.5763e-07, -1.1644e-06,\n",
      "          1.2917e-07, -2.0656e-07, -1.7501e-07,  1.8308e-07,  2.7997e-07,\n",
      "         -2.6852e-07, -7.2059e-07,  3.0396e-07, -5.9608e-07, -1.9111e-07,\n",
      "         -1.4863e-07,  1.9580e-08, -5.9320e-07, -8.9659e-07,  2.0796e-07,\n",
      "          2.9642e-07, -1.2581e-06, -2.3594e-06,  6.1876e-07, -3.1670e-08,\n",
      "          4.9275e-07, -3.8939e-07, -5.5353e-07, -4.6811e-07, -3.0069e-07,\n",
      "          3.9431e-07, -2.8921e-07, -2.6880e-07, -5.4205e-07],\n",
      "        [ 5.0712e-02,  3.0143e-01,  6.9373e-03,  1.5098e-03, -1.7121e-01,\n",
      "          1.2728e-06,  2.2743e-07,  1.0515e-01, -1.5387e-04, -2.3244e-02,\n",
      "          3.4018e-01,  1.2146e-01,  1.5495e-08, -6.2322e-02,  1.2190e-06,\n",
      "          1.0407e-01,  8.7794e-06,  2.4854e-01, -2.4881e-01, -7.8656e-02,\n",
      "          2.9797e-01, -2.7007e-02, -2.0559e-01,  3.5126e-01,  2.0635e-01,\n",
      "          1.4289e-01, -1.3454e-06,  1.5789e-06, -1.4318e-01, -5.6659e-08,\n",
      "         -3.6622e-06, -1.0179e-06,  1.8849e-03, -3.2998e-02,  3.3694e-02,\n",
      "          8.0911e-03, -1.1755e-05, -1.8874e-02,  2.3694e-01, -1.8346e-02,\n",
      "          5.7346e-02,  9.6016e-02,  3.0434e-04, -1.4755e-07,  3.9587e-01,\n",
      "         -4.5492e-06, -1.9995e-08, -1.7645e-01, -1.7693e-06, -1.3822e-01,\n",
      "         -4.0248e-07, -3.9177e-02,  8.1204e-02, -5.1752e-07, -1.8103e-01,\n",
      "         -1.1467e-06, -1.6178e-05,  1.3799e-04,  3.7490e-01,  3.0050e-01,\n",
      "          2.4274e-01, -6.2106e-03, -6.6861e-06,  4.2898e-02],\n",
      "        [ 6.7394e-08,  3.5710e-07, -1.5547e-06,  3.6130e-07, -5.7665e-07,\n",
      "          4.2572e-07,  7.2088e-08, -1.6236e-06,  8.0072e-07, -8.4517e-08,\n",
      "         -7.1220e-09, -7.0028e-07,  7.0607e-07,  5.4958e-07,  5.1978e-07,\n",
      "          2.3637e-08, -1.7655e-06,  6.1688e-08,  6.7641e-07,  4.0973e-07,\n",
      "          3.8232e-07, -3.8681e-07, -1.4144e-06, -5.2612e-07, -6.6506e-07,\n",
      "         -2.9841e-07,  2.9581e-07, -1.8830e-07,  7.8415e-07,  1.4610e-08,\n",
      "         -1.0164e-07, -1.4832e-07, -2.0503e-08,  4.1157e-07,  5.9497e-07,\n",
      "         -9.4032e-07,  1.1045e-07,  5.6273e-07, -9.1666e-08, -5.6407e-07,\n",
      "         -6.1488e-07,  1.3841e-07, -3.8347e-07,  3.6008e-07, -4.4530e-07,\n",
      "         -3.5713e-07,  2.5812e-07, -1.4962e-07, -9.6014e-08,  5.2858e-07,\n",
      "         -2.3856e-07,  2.7360e-07, -8.9240e-08, -1.8690e-07, -6.4268e-07,\n",
      "          5.9689e-07,  3.4741e-07, -6.0378e-08, -7.1791e-07,  1.1874e-07,\n",
      "         -2.4715e-07,  5.5433e-09,  1.1380e-07,  5.9548e-07],\n",
      "        [-1.2656e-01,  2.7526e-07, -2.7665e-01, -1.9643e-04,  2.3554e-01,\n",
      "         -4.3942e-07, -1.2631e-03, -5.4897e-05,  2.5792e-01, -4.9747e-05,\n",
      "          5.3815e-06, -2.7562e-07, -8.1489e-07,  1.3992e-01, -1.0255e-05,\n",
      "          9.9347e-02, -1.8912e-06, -1.0090e-06, -3.8209e-08,  5.8808e-02,\n",
      "         -3.9686e-07,  1.7372e-01, -9.8782e-02, -1.2291e-06, -4.9651e-03,\n",
      "          2.2082e-07,  5.7134e-06, -1.6305e-06, -5.5637e-07,  2.2304e-07,\n",
      "          1.0524e-06,  1.4263e-07,  1.7087e-02,  1.3558e-01,  1.6791e-01,\n",
      "         -2.3823e-08, -2.9206e-03, -2.0899e-06, -2.4186e-06, -6.0137e-08,\n",
      "         -4.2305e-06, -7.8622e-02,  2.2640e-01,  3.2606e-01, -1.3309e-06,\n",
      "         -2.2015e-01, -1.9816e-06,  1.3425e-01,  1.7672e-05, -5.4044e-06,\n",
      "         -4.8232e-09, -9.4783e-07,  9.9700e-07, -4.5971e-05, -1.4501e-07,\n",
      "         -1.9590e-03,  5.6484e-02, -1.0426e-02, -1.6865e-06,  5.3111e-07,\n",
      "         -1.2370e-06, -4.6565e-07, -2.7570e-05, -5.1961e-02],\n",
      "        [-1.7797e-06, -3.6030e-07, -3.9852e-07, -2.3188e-08,  5.9116e-07,\n",
      "         -3.2743e-07,  1.0359e-07, -9.3774e-07,  1.7893e-07,  4.4721e-07,\n",
      "         -2.7780e-07, -4.8206e-07,  2.6693e-07,  5.1352e-07, -6.3791e-07,\n",
      "         -2.6733e-07, -5.8775e-08, -1.2845e-06, -3.6307e-07, -5.9451e-07,\n",
      "          3.1740e-08,  1.9820e-07,  1.6100e-07, -3.0427e-07,  4.9073e-07,\n",
      "          6.8475e-07, -8.2369e-07, -1.2899e-07, -1.5412e-07, -9.8312e-08,\n",
      "         -4.1456e-07,  8.8944e-07,  2.2843e-08, -1.4286e-07,  6.2355e-08,\n",
      "          5.1496e-07,  5.3438e-09,  4.9768e-08,  7.1744e-08,  6.9730e-07,\n",
      "          8.4103e-07, -5.5403e-08,  6.0473e-07, -1.7615e-07, -5.2068e-07,\n",
      "         -4.0322e-07, -1.0195e-07, -1.4975e-06, -1.3484e-07, -1.4980e-06,\n",
      "         -2.8183e-07, -9.2809e-08, -1.0369e-06,  1.0265e-06,  6.0354e-08,\n",
      "          2.3679e-07, -4.3565e-07,  3.4772e-07,  2.9882e-08, -2.9482e-07,\n",
      "         -1.0814e-06,  7.4408e-08, -2.2584e-07,  3.5422e-07],\n",
      "        [ 2.6767e-07, -6.5118e-07, -3.2110e-07, -1.3670e-07, -4.0170e-07,\n",
      "         -1.4546e-07, -8.0691e-08, -9.7474e-07,  4.5376e-07,  1.2250e-07,\n",
      "         -1.6311e-07,  2.4459e-07,  7.2959e-07,  5.1998e-07, -3.4922e-07,\n",
      "         -8.8515e-07, -4.6568e-07, -8.8674e-07, -4.8507e-07, -5.3295e-07,\n",
      "         -5.0352e-07, -6.6661e-07,  4.1314e-07, -4.2739e-10, -1.4820e-07,\n",
      "         -1.1686e-06,  1.5045e-07, -1.9772e-08, -2.0377e-07, -1.2594e-07,\n",
      "         -1.0840e-07, -4.0783e-07,  4.7687e-07,  3.4438e-07,  3.1142e-07,\n",
      "         -9.6190e-07, -7.4935e-07,  4.0533e-07,  5.3568e-07,  8.2135e-09,\n",
      "         -2.2390e-07, -2.4095e-07,  6.3303e-07,  6.0301e-07, -6.2329e-09,\n",
      "          7.1449e-07, -4.1232e-07,  2.1289e-07,  1.1003e-08, -1.7345e-06,\n",
      "         -3.5154e-07, -7.2201e-08, -6.2683e-07, -2.5477e-07, -6.3490e-07,\n",
      "         -1.9818e-07, -1.4645e-06,  4.1992e-09, -2.0948e-07, -1.6742e-07,\n",
      "          5.1820e-07,  5.0173e-07,  2.2870e-07,  6.3008e-08]])), ('classifiers.3.bias', tensor([ 1.2063e-01, -3.3600e-07,  1.2183e-01,  6.8486e-02, -1.0720e-01,\n",
      "         5.0871e-05, -1.3774e-01,  9.5619e-07, -9.6133e-02, -9.5016e-02])), ('classifiers.4.weight', tensor([[-1.0514e-07, -2.7433e-07, -2.0289e-07, -9.4963e-07,  1.9149e-07,\n",
      "          2.8184e-07, -2.1631e-07, -3.9037e-07,  3.1729e-08,  3.7311e-07,\n",
      "         -4.3048e-07, -4.4494e-07,  2.7564e-07, -2.5218e-07, -4.6946e-07,\n",
      "         -1.1385e-06, -1.4799e-08, -7.6956e-08,  1.1955e-07, -4.0118e-07,\n",
      "         -7.6637e-08, -9.6235e-07, -5.2359e-07,  1.0927e-07, -6.0363e-08,\n",
      "          5.4797e-07,  2.1126e-07,  4.4976e-08,  2.2934e-07,  3.6187e-08,\n",
      "         -2.7794e-07, -9.6352e-08,  3.6824e-07,  1.5438e-08, -1.3827e-06,\n",
      "          4.7355e-07,  2.3792e-08,  6.4600e-07, -7.4875e-07, -1.3432e-06,\n",
      "         -1.2270e-07, -5.6856e-07, -1.5021e-07, -1.1205e-06, -1.1320e-06,\n",
      "         -6.1513e-07, -2.0326e-07,  1.5115e-08, -3.5238e-08, -3.0674e-08,\n",
      "          1.5278e-07,  3.3993e-07,  3.3909e-07, -1.5601e-06,  1.8819e-07,\n",
      "          3.4594e-07,  3.4547e-07,  5.1665e-08,  1.5188e-08, -2.6127e-07,\n",
      "         -9.7523e-08, -3.5808e-07,  3.5049e-07,  5.8754e-08],\n",
      "        [ 2.3407e-06, -1.3889e-05,  1.5116e-01,  9.5767e-03, -2.8778e-01,\n",
      "         -7.2360e-02,  2.3867e-01, -2.2949e-02, -1.8838e-01, -5.4871e-02,\n",
      "         -6.9968e-06,  1.1406e-01,  9.2736e-02,  1.0657e-01,  3.5139e-01,\n",
      "         -3.7239e-07,  2.9543e-01,  5.6241e-02, -6.8159e-03, -3.3163e-01,\n",
      "          6.5145e-07, -1.3221e-01,  1.5826e-01, -1.3043e-01,  1.7975e-01,\n",
      "         -8.1513e-02, -4.5721e-02,  3.1126e-01,  1.8934e-01,  2.1410e-01,\n",
      "          3.8201e-06, -9.1532e-02,  7.7244e-02,  1.8786e-01,  2.6522e-01,\n",
      "         -2.1914e-01, -1.1806e-01,  2.4839e-01, -6.8358e-07,  6.1827e-02,\n",
      "          2.9601e-01,  2.7217e-01, -2.5579e-01,  1.8633e-03, -4.5820e-04,\n",
      "          2.6907e-01,  1.0701e-01,  9.4173e-02,  4.6154e-02,  1.3019e-05,\n",
      "         -3.0378e-07,  7.6697e-02,  2.3804e-01,  2.3277e-01,  6.9971e-02,\n",
      "          2.7748e-01, -2.4243e-02, -1.4457e-01, -1.7214e-01, -4.4652e-05,\n",
      "         -1.2096e-06, -3.7554e-06, -6.4055e-03,  1.3428e-01],\n",
      "        [ 1.5666e-01, -1.8217e-07,  2.5252e-01, -3.1467e-08, -2.6275e-06,\n",
      "         -1.1376e-07, -1.7906e-07,  1.3273e-01,  6.7618e-06,  1.6673e-01,\n",
      "         -3.0240e-02,  5.6866e-07, -1.2942e-01, -1.5366e-02,  8.4969e-02,\n",
      "         -3.2352e-06, -9.7202e-07, -8.3556e-07, -4.1911e-07,  2.2002e-02,\n",
      "         -2.5300e-06,  1.1257e-01, -8.1643e-07, -9.9454e-08, -5.1604e-05,\n",
      "         -7.4196e-08,  2.0729e-01,  1.3622e-07, -4.8479e-07, -6.4252e-07,\n",
      "          1.9276e-06, -2.2492e-06, -5.4359e-02, -3.5712e-07,  3.6128e-02,\n",
      "         -1.3741e-07,  4.1625e-02, -7.9940e-07, -1.5834e-06, -1.9700e-07,\n",
      "         -1.0397e-02, -8.4864e-07, -9.6699e-02, -1.8714e-01, -1.9615e-06,\n",
      "         -6.5337e-06,  3.1658e-01,  9.0322e-07, -1.2337e-06, -2.5379e-02,\n",
      "          1.1219e-07, -5.1960e-07,  5.4598e-08, -2.5218e-07, -1.3247e-06,\n",
      "         -3.8119e-08, -8.3426e-07,  5.6076e-02, -7.3456e-08, -7.7934e-02,\n",
      "          2.2004e-07, -3.2679e-06,  1.4083e-01,  1.5097e-01],\n",
      "        [-7.2340e-07, -2.3647e-07, -8.1101e-07, -5.2491e-07, -2.8027e-07,\n",
      "         -9.9298e-06,  9.1546e-06,  1.5740e-01, -3.3617e-02,  1.7021e-01,\n",
      "         -2.0385e-07, -2.2322e-06,  2.1264e-02,  1.1710e-01, -2.6636e-06,\n",
      "         -3.5821e-02, -6.7297e-05, -1.8100e-02, -1.2643e-06, -9.4536e-02,\n",
      "         -1.0408e-06, -1.9889e-07,  1.8357e-01,  2.2965e-01, -1.8967e-07,\n",
      "          1.0878e-02, -1.4785e-06,  2.3403e-01, -3.4574e-07, -9.8138e-03,\n",
      "         -6.8884e-03, -1.1075e-07, -5.6411e-06, -9.9486e-07, -7.6958e-07,\n",
      "          1.0540e-01, -1.0698e-01, -4.3569e-07, -1.3022e-03,  2.2729e-01,\n",
      "         -5.3862e-02, -3.3048e-02, -4.5910e-02,  4.2292e-02, -7.8320e-05,\n",
      "         -6.5678e-07, -6.5520e-03, -3.2139e-06, -2.4986e-02,  5.3869e-02,\n",
      "          1.0150e-01,  3.3624e-01, -1.4364e-06, -3.5982e-03, -7.3224e-07,\n",
      "          2.2568e-01, -3.0482e-03, -3.2780e-02, -3.1515e-05,  1.0498e-06,\n",
      "          2.7915e-07,  3.7643e-02,  2.1130e-07, -1.1286e-01],\n",
      "        [-3.4231e-08,  1.4118e-01, -1.6303e-02,  3.3753e-01, -2.5094e-02,\n",
      "          7.5801e-02,  3.3885e-02, -2.4125e-01,  1.7442e-03,  4.7852e-03,\n",
      "         -1.2879e-02, -8.0147e-02,  3.4818e-02,  1.8402e-01, -6.4639e-02,\n",
      "         -5.2129e-02, -2.8841e-02,  3.3518e-01,  1.7265e-01,  7.9836e-02,\n",
      "          1.1738e-01, -9.7665e-02,  1.8474e-01,  3.8145e-02, -3.5936e-02,\n",
      "          4.2265e-01, -2.7391e-01,  6.4893e-02,  2.1153e-01,  8.5493e-02,\n",
      "         -1.4143e-06, -5.2533e-02,  2.8164e-03, -1.7872e-01,  8.5145e-02,\n",
      "          1.9418e-03,  6.4630e-02,  5.3124e-02,  2.7353e-01,  1.3549e-01,\n",
      "         -3.6928e-02,  1.9001e-01,  1.4366e-01,  2.6328e-07,  2.4498e-01,\n",
      "          8.9434e-02,  2.7112e-02,  1.1626e-01,  7.1256e-02,  6.8983e-03,\n",
      "         -9.3868e-02, -2.0455e-01, -1.1284e-01,  2.0500e-01, -1.5794e-01,\n",
      "          1.7424e-01,  2.6618e-01,  1.0666e-06,  2.0110e-01,  2.0078e-01,\n",
      "         -4.8174e-02, -1.5045e-01, -1.1169e-02,  2.2130e-02],\n",
      "        [-1.8191e-07, -1.3733e-02,  2.5139e-01,  1.7797e-06, -1.0466e-01,\n",
      "         -7.1039e-07, -5.3106e-07,  5.9002e-08, -4.2250e-02,  6.4550e-07,\n",
      "          6.4862e-02,  3.1582e-01, -3.7018e-07, -4.2807e-06, -2.0050e-05,\n",
      "          2.5064e-01, -3.3212e-06, -5.3543e-02,  3.0607e-03,  1.3034e-01,\n",
      "         -1.9491e-06, -2.3247e-01, -1.8657e-07,  3.8145e-08, -5.3897e-05,\n",
      "         -1.9876e-07,  5.4447e-02, -8.5798e-03, -2.4439e-02, -8.9095e-04,\n",
      "          9.7607e-02, -1.3142e-06,  2.1707e-01, -1.1083e-06, -7.8519e-07,\n",
      "         -1.4058e-02,  4.1531e-02,  1.4623e-01, -2.0034e-07, -5.3675e-07,\n",
      "         -7.9850e-02, -1.8037e-07,  4.3280e-07, -5.5480e-03, -1.9442e-07,\n",
      "          1.5351e-02, -1.8210e-01, -2.6847e-06, -2.8384e-05, -3.9836e-07,\n",
      "          1.0468e-07,  1.2119e-07, -3.2632e-06,  2.4174e-01, -6.7990e-06,\n",
      "         -3.3177e-04,  2.8050e-01,  5.4195e-02,  3.5234e-05, -2.2274e-03,\n",
      "         -1.9604e-02,  1.1448e-07, -4.3015e-02, -1.1385e-06],\n",
      "        [ 7.4913e-02, -1.3242e-02, -3.0138e-05,  2.0790e-01,  8.0035e-02,\n",
      "          5.9633e-02, -2.9471e-02,  2.1978e-02, -5.6580e-02,  1.0198e-01,\n",
      "          1.8080e-01, -1.3089e-01, -7.1821e-04, -2.4360e-06, -1.9035e-01,\n",
      "         -8.7342e-02,  3.1563e-07, -1.1585e-01, -1.0371e-06,  2.2147e-03,\n",
      "          1.2345e-01,  1.0474e-04, -6.0900e-05,  1.9181e-01,  2.2040e-01,\n",
      "         -6.0875e-03, -5.1062e-02, -9.1842e-02, -1.2780e-07, -7.2276e-08,\n",
      "         -2.8893e-06,  1.5302e-02, -8.7600e-06,  2.9934e-01, -3.9262e-02,\n",
      "         -2.5448e-02,  6.3049e-06, -2.8526e-06,  1.0768e-01, -3.2411e-02,\n",
      "          3.1510e-01,  3.1922e-01, -4.6633e-02,  2.1558e-04,  1.1293e-01,\n",
      "         -3.9051e-09, -3.1878e-06, -4.4327e-02, -1.7305e-03,  2.7041e-01,\n",
      "         -2.3372e-02, -7.8550e-07, -1.8179e-01, -7.8245e-03,  9.6771e-02,\n",
      "         -3.2432e-02, -8.5062e-02,  5.4597e-02, -4.3075e-07,  3.6258e-07,\n",
      "          3.0731e-01, -1.0759e-06,  2.3546e-01, -1.7597e-01],\n",
      "        [-8.0431e-02,  4.2640e-01,  7.6687e-02, -8.7886e-02, -3.3822e-06,\n",
      "          2.1189e-01, -1.8044e-01, -1.2693e-06, -1.6033e-01, -9.1052e-02,\n",
      "          1.5748e-01,  2.5332e-01, -2.9754e-02, -1.0680e-03, -5.3460e-02,\n",
      "          4.0874e-02,  1.7271e-01,  1.0746e-01,  1.9043e-01, -5.7745e-07,\n",
      "          2.5826e-02,  1.6048e-05,  3.2055e-02, -1.5394e-01,  2.9325e-01,\n",
      "         -9.8235e-02,  1.4118e-01, -1.7368e-01, -1.2012e-05,  5.4020e-02,\n",
      "          1.1994e-01,  2.7542e-01, -8.4973e-05, -1.3777e-01, -2.4494e-02,\n",
      "          2.6102e-01,  8.4481e-02, -1.6383e-06,  1.4761e-02, -8.0654e-06,\n",
      "          1.5847e-01, -2.0839e-01, -3.8472e-02,  5.7270e-05,  1.1751e-01,\n",
      "          2.6053e-02, -1.3083e-01, -4.2699e-02,  8.7441e-02, -3.5770e-06,\n",
      "          2.0505e-01,  2.9921e-01,  4.1426e-01, -1.3796e-01,  9.2559e-03,\n",
      "         -9.5316e-05,  1.3771e-02, -9.0760e-03,  1.6449e-02,  2.5799e-01,\n",
      "          6.3726e-02,  4.2857e-01,  1.6757e-01, -7.8853e-02],\n",
      "        [-2.9437e-07, -8.1239e-07, -1.8632e-01, -6.2286e-07,  8.9832e-02,\n",
      "          4.2114e-07, -9.0577e-02, -3.8457e-05,  9.3122e-02, -5.1716e-05,\n",
      "          3.3888e-06,  3.3196e-07, -2.6733e-06,  7.1644e-02, -6.6914e-06,\n",
      "          2.6443e-01,  3.5212e-07, -2.6477e-06, -5.0008e-07,  4.5878e-06,\n",
      "         -2.6438e-03,  1.2218e-01, -1.6040e-06, -2.5054e-07, -6.5603e-02,\n",
      "         -9.6215e-08,  6.8497e-06, -1.0922e-06, -6.0610e-07, -4.5457e-02,\n",
      "         -6.2703e-07,  5.5685e-08,  5.4981e-02,  1.2058e-01,  2.4903e-01,\n",
      "          1.8663e-07, -3.8056e-03,  1.9000e-07,  1.3406e-07, -1.1059e-06,\n",
      "          1.7417e-07, -4.3963e-02,  1.5197e-01,  2.2306e-01, -1.1790e-06,\n",
      "         -1.2633e-01,  2.9431e-07,  1.5737e-01,  4.6701e-05, -3.5781e-06,\n",
      "          5.9974e-08, -1.8883e-06, -4.0374e-07, -6.2601e-07, -1.3800e-06,\n",
      "          9.2663e-08,  7.3435e-02, -3.1879e-05, -1.9612e-06,  5.0797e-07,\n",
      "         -3.5778e-07, -3.0094e-06, -3.7138e-05, -1.5932e-03],\n",
      "        [-7.2994e-08,  5.2048e-07, -5.2547e-07,  3.4321e-08, -1.4872e-06,\n",
      "          8.9668e-07, -2.8919e-07, -5.7272e-07, -3.4905e-07, -2.2606e-07,\n",
      "         -5.6557e-07,  2.6332e-07, -4.7736e-07,  5.3695e-08,  1.2878e-07,\n",
      "         -3.0581e-07, -1.1122e-06,  2.0845e-07,  8.8117e-07, -2.0787e-07,\n",
      "          2.7572e-07, -7.3068e-07, -6.8518e-08, -2.1508e-06,  3.4621e-07,\n",
      "         -1.3622e-06, -6.8331e-08,  6.4930e-07, -6.1880e-07, -3.0950e-07,\n",
      "         -6.7161e-07, -1.8246e-07,  8.1945e-07,  3.2052e-07, -3.0240e-07,\n",
      "          3.4087e-08, -1.3051e-07, -3.2710e-07, -5.1643e-07, -3.3649e-07,\n",
      "         -3.6890e-07, -1.7127e-07, -3.9486e-07, -4.1717e-07, -6.9271e-07,\n",
      "         -2.8857e-07, -1.0229e-08,  2.7958e-07,  6.0237e-07, -8.6518e-07,\n",
      "          3.0889e-07, -8.4733e-07,  5.8676e-07, -4.3525e-07,  5.3898e-08,\n",
      "         -4.2687e-07, -2.8911e-07, -1.1499e-06, -1.0225e-06,  1.8852e-07,\n",
      "          1.7247e-07, -8.4283e-07,  7.9338e-08, -4.2344e-07]])), ('classifiers.4.bias', tensor([-2.0095e-01,  7.7850e-02, -6.4157e-07,  1.0162e-06,  1.2561e-02,\n",
      "        -2.9451e-06, -9.3684e-08,  7.5349e-03,  5.1149e-07, -2.6785e-01])), ('classifiers.5.weight', tensor([[-7.9007e-02,  2.3519e-01, -3.3188e-07,  4.3201e-06, -1.2275e-06,\n",
      "          3.1244e-01,  8.6144e-04, -6.7629e-02, -1.2891e-06, -1.6050e-01,\n",
      "         -1.1381e-01,  1.8436e-02, -2.0880e-01, -1.8024e-08, -3.0390e-02,\n",
      "          1.8718e-08,  2.5990e-01,  7.3133e-03,  2.8591e-01, -5.1180e-06,\n",
      "          3.3288e-01, -2.6108e-02,  4.2334e-02,  1.3016e-01, -7.0792e-02,\n",
      "          1.1104e-02, -9.3826e-02, -8.1030e-02, -1.0318e-08, -1.8694e-06,\n",
      "          9.2515e-02,  3.6703e-01, -5.9509e-02, -7.9216e-08,  1.1068e-01,\n",
      "         -2.2972e-02, -7.7061e-02, -2.1666e-02,  9.5510e-03, -4.5934e-02,\n",
      "         -1.7493e-03, -2.6435e-01, -2.0630e-06, -6.8667e-07,  2.1524e-01,\n",
      "          1.2089e-02, -2.1706e-07,  3.2958e-02,  1.4748e-01,  3.8890e-02,\n",
      "          2.4957e-01,  2.2264e-01,  1.1208e-06,  9.8598e-02,  1.4809e-01,\n",
      "          6.8115e-07, -1.1962e-01, -1.2400e-01,  2.2137e-01,  2.9379e-01,\n",
      "          2.0441e-01,  1.0130e-01,  6.5801e-07,  1.1983e-01],\n",
      "        [-1.3368e-07, -8.2438e-07, -1.7656e-07,  9.8118e-07,  5.9073e-08,\n",
      "          5.7916e-07, -1.1060e-06,  3.8521e-07,  1.0543e-07, -3.0304e-07,\n",
      "          7.0631e-08, -7.5063e-07, -9.1262e-07,  2.3708e-07, -4.4327e-07,\n",
      "          1.2973e-07, -4.6523e-07,  2.0095e-07,  4.1978e-07,  9.1261e-08,\n",
      "          9.2978e-08, -4.4927e-07, -1.2414e-07, -8.3168e-07,  1.2809e-08,\n",
      "         -2.1175e-06,  1.9515e-07, -4.0278e-07, -7.9443e-07, -7.1314e-07,\n",
      "         -9.2394e-07, -7.6138e-08, -1.5148e-06,  4.5767e-07,  2.7027e-07,\n",
      "         -9.8267e-07, -4.6994e-07,  5.5905e-07,  4.5040e-07, -2.2682e-07,\n",
      "         -3.6924e-07, -2.4041e-08, -5.7931e-07,  3.3087e-07,  1.6596e-07,\n",
      "          7.1511e-08,  1.0015e-06,  4.2065e-07, -6.5567e-07,  2.2264e-07,\n",
      "         -3.8433e-08, -9.7439e-07,  2.6875e-07,  6.0798e-09,  5.4153e-07,\n",
      "         -3.7381e-07, -7.0515e-08, -5.9071e-07, -1.2634e-07, -4.3425e-07,\n",
      "          3.8448e-07,  1.6405e-07, -8.3669e-07,  9.0332e-07],\n",
      "        [ 2.1614e-07, -1.1320e-06, -9.5188e-07,  2.3041e-07, -3.4281e-07,\n",
      "          5.7696e-07, -1.1434e-06,  4.0140e-07,  3.9154e-07, -7.8138e-07,\n",
      "         -5.1604e-07, -4.3855e-07,  1.9130e-07, -1.1585e-06, -2.8535e-07,\n",
      "         -4.7371e-07, -4.8992e-08,  2.3938e-07, -1.3750e-06,  6.1139e-07,\n",
      "         -1.4520e-07, -3.0618e-08,  1.5668e-07, -1.9008e-06,  9.2589e-07,\n",
      "         -1.3687e-07,  6.9145e-07,  9.5636e-09,  2.9802e-07, -4.9578e-08,\n",
      "         -1.3746e-06,  1.0152e-07,  9.6916e-08,  2.9421e-08, -1.6539e-07,\n",
      "         -5.6269e-07, -5.2207e-07, -3.5643e-07, -1.2472e-07, -3.7841e-07,\n",
      "          2.1159e-07, -1.1593e-07,  3.0477e-07, -5.7390e-07, -1.2609e-06,\n",
      "          1.0952e-07,  8.3920e-07,  1.6828e-07, -4.8261e-08, -1.1618e-06,\n",
      "          1.8350e-07, -1.4530e-06,  2.0001e-07, -5.3915e-07,  3.0509e-07,\n",
      "         -1.4078e-06,  4.3405e-07,  4.2728e-07, -5.6481e-07, -5.9126e-07,\n",
      "          7.4976e-08,  2.6822e-07, -1.5287e-07, -5.1546e-08],\n",
      "        [-3.6485e-07,  2.3548e-02,  2.0243e-01, -1.3930e-01, -1.9700e-04,\n",
      "         -9.6712e-07, -4.9929e-03, -1.5449e-06, -5.1196e-06, -5.5841e-02,\n",
      "          9.6014e-02,  2.0420e-01,  2.4498e-02, -3.3489e-06, -6.5141e-03,\n",
      "          4.5942e-01, -1.2074e-06,  4.2989e-03,  1.1213e-05,  9.7787e-03,\n",
      "         -9.7904e-02, -1.2984e-01, -1.5057e-01, -6.4769e-07,  1.1312e-01,\n",
      "         -4.8362e-02,  2.1628e-01, -1.0849e-06, -3.8017e-06, -4.8007e-02,\n",
      "          1.4324e-01,  2.0416e-07,  1.3256e-01, -7.3549e-07,  1.7783e-07,\n",
      "          8.4644e-03,  9.9973e-02,  1.7864e-05, -7.0470e-07, -8.8252e-07,\n",
      "          1.5737e-03, -6.0010e-07,  1.2265e-01, -3.9800e-02, -9.8432e-07,\n",
      "          1.0930e-01, -4.1996e-02, -2.5095e-02, -9.2436e-06, -1.3805e-06,\n",
      "         -2.3600e-07, -2.4035e-06,  2.7807e-02,  2.5119e-01, -6.2555e-08,\n",
      "         -1.3168e-01,  1.0724e-01,  1.9442e-02,  3.9106e-01,  3.2086e-02,\n",
      "         -7.8939e-07,  3.6957e-07, -2.1887e-06, -9.0015e-07],\n",
      "        [ 2.8950e-01, -2.5883e-06,  1.1638e-01, -5.9456e-06,  1.3671e-01,\n",
      "         -3.2635e-06,  8.5684e-02,  2.8205e-01,  8.3293e-02,  1.5854e-04,\n",
      "         -3.0063e-01, -1.1831e-01, -1.1537e-01,  3.4683e-01,  7.4929e-02,\n",
      "          1.3344e-01,  1.0675e-01, -3.8663e-02, -8.2145e-06,  2.7905e-01,\n",
      "         -2.1427e-01,  4.8670e-01, -2.3198e-07, -1.1992e-01,  3.5987e-01,\n",
      "         -6.6719e-02,  2.1078e-01, -4.2092e-05,  3.5612e-02, -2.3328e-07,\n",
      "          6.4858e-02,  1.1958e-01,  6.1255e-02,  2.4609e-01,  3.3765e-01,\n",
      "          1.9093e-04,  1.9648e-01, -1.3053e-03, -1.1346e-01, -8.3192e-02,\n",
      "          1.4468e-01,  2.6188e-02,  4.9820e-02,  2.8892e-01,  2.0362e-07,\n",
      "         -8.4601e-03,  1.1197e-02,  1.3627e-01,  2.9544e-06,  5.9437e-02,\n",
      "         -5.5966e-07, -1.9045e-06, -2.8014e-02, -3.0986e-02,  2.4465e-07,\n",
      "         -1.1832e-01, -3.9298e-02, -9.6461e-07, -1.4839e-01,  3.5863e-02,\n",
      "         -3.5321e-02,  8.2028e-02,  8.6950e-02,  2.4882e-01],\n",
      "        [ 1.9470e-01,  2.4375e-07, -1.7219e-01,  2.2102e-01,  2.2360e-01,\n",
      "          3.6062e-07, -3.9342e-02,  5.7961e-08, -2.2119e-02, -4.5869e-02,\n",
      "         -1.8316e-06,  6.3811e-03,  3.4788e-01, -1.3996e-06,  4.8727e-02,\n",
      "          4.4415e-07, -1.0127e-01, -1.1039e-01,  6.9226e-02,  9.1232e-02,\n",
      "         -1.8141e-01, -2.1239e-02, -3.2073e-02, -1.0741e-01, -2.1669e-09,\n",
      "          4.7466e-02, -8.6179e-07,  1.1337e-01,  4.7862e-02,  6.6908e-02,\n",
      "         -1.0624e-01, -5.9865e-08,  6.1590e-03, -1.0947e-06, -3.8932e-07,\n",
      "         -1.3288e-08,  9.9936e-06,  1.4875e-01, -2.5090e-06,  4.5824e-02,\n",
      "         -5.8354e-07, -5.9774e-02,  1.6075e-01, -1.6360e-02,  1.7567e-07,\n",
      "          2.4878e-01, -3.8033e-05,  3.5812e-01,  2.0274e-01, -5.5739e-02,\n",
      "         -9.5737e-07, -7.6716e-02, -2.0235e-03,  1.6180e-01,  1.4374e-01,\n",
      "          2.8328e-01, -2.0212e-01, -1.1519e-03,  3.7810e-02, -6.5098e-07,\n",
      "          1.2129e-01, -7.1100e-07, -3.6363e-02, -1.5075e-06],\n",
      "        [-1.8738e-02, -6.1149e-02, -3.8714e-07, -5.1259e-03, -3.1462e-02,\n",
      "          3.9654e-06,  1.5290e-01,  1.7365e-06, -3.6960e-07,  1.8274e-01,\n",
      "          8.3764e-07, -7.1212e-02,  1.5565e-03,  3.6716e-01,  8.9093e-02,\n",
      "         -8.9496e-07,  1.9466e-01,  8.0435e-04, -2.7976e-06, -6.7736e-02,\n",
      "         -1.7181e-01, -1.9043e-07,  2.3844e-01,  1.1237e-01, -5.3315e-02,\n",
      "          2.3066e-01, -4.4042e-07,  1.6883e-01, -1.8804e-02,  1.8275e-01,\n",
      "          1.1673e-02,  1.3920e-06, -1.6841e-02, -7.5888e-02, -1.2892e-01,\n",
      "          8.0368e-02, -6.7375e-07,  8.1551e-03,  1.6711e-01,  3.1513e-01,\n",
      "         -1.1725e-01,  4.2085e-02,  1.1010e-05,  2.1353e-06, -1.1898e-01,\n",
      "         -6.3162e-02, -8.9513e-07, -2.9982e-04, -8.5220e-02,  1.5107e-01,\n",
      "          2.9502e-07,  2.8704e-01, -2.1063e-03,  1.4467e-01,  4.7173e-07,\n",
      "          3.3710e-01,  4.8238e-03, -1.9100e-06,  3.4814e-02, -4.4281e-06,\n",
      "          1.1131e-07, -5.5508e-07, -3.8997e-02,  5.1109e-02],\n",
      "        [ 2.4099e-01, -1.1705e-06, -3.3129e-08,  1.4085e-06, -5.9265e-07,\n",
      "          6.9691e-10, -2.2906e-06,  2.0479e-01, -1.2189e-06,  4.0062e-07,\n",
      "          1.2758e-01, -6.9751e-02, -1.1424e-06,  8.8680e-07, -8.7235e-02,\n",
      "         -2.6605e-07, -1.0997e-06, -8.9798e-02, -4.1730e-07,  6.5729e-08,\n",
      "          3.2474e-02, -6.8591e-08,  6.0238e-07,  3.1374e-01, -1.1285e-06,\n",
      "         -7.8312e-02, -2.0081e-08, -6.5419e-08, -2.8241e-07, -1.3576e-06,\n",
      "         -5.6363e-07, -4.2821e-02, -3.6538e-07,  8.2352e-02, -3.1273e-07,\n",
      "         -2.2363e-07, -2.1147e-02, -1.5215e-02,  1.9697e-01, -1.2522e-06,\n",
      "          3.1397e-04,  1.1079e-02, -2.0567e-08,  3.6953e-07,  1.5112e-01,\n",
      "          9.9142e-08,  2.2084e-07, -1.7118e-07, -7.5622e-07, -1.1000e-06,\n",
      "          9.6100e-02, -3.9086e-07, -1.0398e-01, -5.8707e-07, -4.8718e-07,\n",
      "          6.0452e-08, -3.0717e-02,  2.0566e-01, -2.8041e-07, -4.9448e-07,\n",
      "          3.1714e-01,  9.3685e-08,  5.4767e-04, -7.7833e-07],\n",
      "        [-6.5042e-07, -3.6691e-07, -1.3296e-06, -2.6162e-08, -3.0659e-07,\n",
      "          5.3233e-07, -8.2162e-07, -8.8119e-07,  2.2374e-07, -3.6831e-07,\n",
      "          1.3761e-07, -8.9717e-07,  6.0857e-07, -6.5127e-07,  2.3369e-07,\n",
      "          2.9568e-08, -1.0961e-07,  4.9073e-08,  2.7895e-07,  2.3478e-07,\n",
      "         -1.1452e-07, -7.3400e-07,  1.0236e-07, -1.9717e-07,  5.9180e-07,\n",
      "          5.7999e-08,  1.4205e-07,  3.1331e-07, -1.5853e-07, -1.0754e-07,\n",
      "          2.6957e-07,  8.2860e-07,  2.5224e-07, -4.7161e-08,  1.9623e-07,\n",
      "         -5.4098e-07, -1.7670e-06,  4.8466e-08,  2.1033e-07, -3.8177e-07,\n",
      "          7.4554e-07, -2.0472e-07, -3.9127e-07,  9.6310e-08, -1.1991e-07,\n",
      "          6.7393e-08, -1.1031e-08, -3.1930e-08, -4.7017e-08, -1.1202e-06,\n",
      "         -5.2678e-07, -1.6184e-06, -1.1435e-07,  3.7833e-07, -9.4892e-07,\n",
      "         -6.5973e-08, -1.4598e-06,  2.2587e-07,  2.4829e-07, -2.2451e-07,\n",
      "          5.7817e-07, -1.2100e-06, -4.4823e-07,  4.4629e-07],\n",
      "        [-4.5087e-02,  7.9167e-08,  2.3564e-01,  2.1205e-01, -8.0439e-02,\n",
      "         -1.3753e-06,  1.1682e-06, -4.5086e-02,  2.4151e-01, -9.5765e-02,\n",
      "          4.3277e-07, -2.2243e-01,  5.1999e-02, -2.0092e-06, -2.2275e-07,\n",
      "         -1.0940e-01, -1.1155e-06, -4.7938e-02, -6.8562e-03, -1.5421e-01,\n",
      "          4.6357e-02, -2.1635e-01,  2.2180e-02, -5.1531e-07, -5.4399e-07,\n",
      "          2.2908e-03, -1.2076e-02,  6.9664e-02,  4.1581e-01,  3.5391e-02,\n",
      "         -2.8649e-07,  1.7183e-07,  2.9010e-01, -2.6566e-06,  6.6600e-07,\n",
      "         -3.5145e-07, -2.2541e-01, -1.8028e-01,  6.5094e-02, -1.3486e-07,\n",
      "          2.1711e-02, -1.2919e-02,  2.1909e-01,  5.1097e-02, -5.4294e-07,\n",
      "          2.4191e-01,  3.2089e-01, -3.1823e-04,  1.1504e-01, -9.3687e-07,\n",
      "         -1.1908e-06, -2.8155e-07,  3.8292e-02,  2.2606e-06, -4.6816e-07,\n",
      "          1.1278e-06,  5.3660e-02,  2.6061e-01, -1.0761e-01, -5.2898e-07,\n",
      "         -1.6695e-06,  2.3947e-06, -9.3016e-07,  1.4544e-06]])), ('classifiers.5.bias', tensor([ 3.0121e-07, -1.0843e-01, -1.3373e-01,  3.1883e-02,  5.8203e-02,\n",
      "        -3.9034e-07,  3.6056e-06, -1.3492e-06, -1.3793e-01,  3.0441e-02])), ('classifiers.6.weight', tensor([[ 3.5581e-07, -4.6388e-07, -1.0203e-06, -3.3222e-07, -8.7249e-08,\n",
      "          3.2144e-08, -1.7668e-06, -3.0913e-08,  1.3153e-07, -1.4684e-07,\n",
      "         -7.2424e-07, -9.2480e-07,  4.8598e-07, -8.3818e-07, -3.2743e-08,\n",
      "          9.1757e-08, -5.6561e-07, -1.5580e-07, -3.0178e-07,  4.2722e-07,\n",
      "          8.0708e-08, -2.0187e-07, -4.8667e-07, -3.8642e-07,  3.6089e-07,\n",
      "          4.0685e-07, -3.2078e-07,  6.7895e-08, -4.3881e-07, -1.0284e-07,\n",
      "         -4.7429e-07, -4.1496e-07, -1.6950e-06,  2.0570e-07, -3.7622e-07,\n",
      "         -1.7864e-07, -4.9963e-07,  2.1632e-07, -1.4134e-07,  5.3275e-07,\n",
      "         -6.9404e-08,  7.9844e-08, -1.4316e-06, -5.6732e-07, -2.8861e-08,\n",
      "         -7.8141e-08,  7.6626e-08,  1.9789e-07,  1.4298e-07, -1.4147e-06,\n",
      "         -6.5741e-08, -3.7307e-07,  7.9647e-08, -1.5480e-07,  1.1332e-07,\n",
      "          1.8196e-07,  1.0112e-07, -1.1035e-06,  3.0861e-08, -5.3455e-07,\n",
      "          1.6226e-07,  2.7236e-07, -5.3214e-07,  2.9332e-07],\n",
      "        [-1.7395e-06,  1.9688e-01, -1.5559e-04,  2.2574e-03, -2.4166e-07,\n",
      "          1.2546e-01,  1.0210e-02, -8.6707e-07,  1.7565e-07, -2.1242e-06,\n",
      "         -1.5679e-06, -8.4737e-02, -1.3674e-05, -7.3809e-08,  1.0803e-07,\n",
      "         -1.2958e-04,  5.3587e-02,  3.2661e-01, -1.3564e-06, -4.5810e-07,\n",
      "          3.5781e-01, -1.4658e-07, -2.8377e-02,  2.3928e-02, -1.4088e-06,\n",
      "          3.4842e-02, -2.8927e-07, -4.4675e-07,  4.0960e-07, -8.4238e-02,\n",
      "         -9.6573e-02,  3.0623e-07, -3.5278e-06, -7.4040e-07,  1.7070e-02,\n",
      "         -1.1712e-04, -7.9335e-06, -2.1303e-06, -6.6064e-02,  2.2298e-06,\n",
      "         -6.6862e-03, -2.3724e-05, -1.2540e-06, -1.5214e-07,  3.0667e-01,\n",
      "         -6.6706e-07, -1.3357e-02, -1.9904e-02, -6.6910e-02,  4.1642e-08,\n",
      "          8.0137e-07, -1.9876e-06, -8.1750e-04,  3.6253e-06,  1.4056e-07,\n",
      "          3.7485e-06, -2.9099e-02, -1.2598e-06,  2.2138e-01,  2.3922e-01,\n",
      "         -1.7000e-02, -8.9041e-07, -1.5405e-06,  1.9920e-02],\n",
      "        [-9.1951e-07, -4.2793e-07, -6.9556e-09,  1.8752e-07, -9.2731e-07,\n",
      "          1.0990e-07,  1.2745e-08, -2.2230e-07, -6.0693e-07, -6.6550e-08,\n",
      "         -7.3225e-07, -6.6692e-07,  1.2269e-07, -7.9891e-08,  3.7044e-07,\n",
      "          9.7477e-08, -1.4078e-06, -2.5324e-07, -1.4357e-07, -1.6440e-07,\n",
      "         -1.2072e-07, -7.7132e-08,  2.7306e-08, -1.7735e-06,  1.0887e-06,\n",
      "         -1.3391e-08,  2.8539e-08,  3.3158e-07,  1.8738e-07, -1.1785e-07,\n",
      "          7.0857e-07,  2.9519e-07, -8.7430e-07,  4.1739e-08,  3.2799e-07,\n",
      "         -2.8493e-08, -1.5312e-06, -3.4822e-07, -5.3320e-08, -3.5223e-07,\n",
      "          3.0824e-07, -4.7347e-07,  3.8028e-07, -8.7264e-07, -2.6197e-07,\n",
      "          1.2167e-07,  3.6255e-07, -5.8078e-08, -1.5118e-06,  3.7741e-08,\n",
      "          5.9452e-07, -7.8248e-07, -4.3227e-07,  2.1646e-07, -6.5314e-07,\n",
      "         -2.7574e-07, -2.8232e-06,  2.3452e-07, -1.4661e-06,  1.0289e-07,\n",
      "         -1.8189e-07,  2.3721e-07,  2.8772e-07, -7.2803e-07],\n",
      "        [ 4.8468e-07, -2.5124e-07,  5.1194e-07,  7.6915e-07, -5.6678e-08,\n",
      "         -7.2199e-08, -1.0209e-06, -4.8452e-07, -6.0325e-07, -1.8900e-07,\n",
      "          5.7981e-07, -6.5806e-07,  1.5104e-07,  3.9760e-07, -2.1566e-07,\n",
      "         -1.6977e-06,  1.8737e-07,  5.1780e-08, -2.4256e-07,  7.4736e-08,\n",
      "         -3.2114e-06, -2.2752e-07,  7.2625e-09, -1.0105e-06,  3.5580e-07,\n",
      "         -1.0984e-06, -1.3597e-06,  7.3083e-08, -1.5951e-07, -3.1836e-07,\n",
      "          4.5158e-07,  2.0224e-07, -1.4079e-07,  4.3205e-08,  5.1409e-07,\n",
      "         -1.7772e-07, -2.7664e-07,  8.8117e-08, -6.1773e-08, -5.4475e-07,\n",
      "         -2.8156e-08,  1.1718e-07,  2.6641e-08,  3.7306e-08,  2.1454e-08,\n",
      "          4.1582e-07,  3.6427e-07,  2.2749e-07, -2.2776e-07,  3.2443e-07,\n",
      "         -2.7634e-07, -2.4046e-07,  9.1842e-07, -3.5308e-07, -4.6788e-08,\n",
      "          7.7741e-08, -5.2187e-08, -1.0046e-06, -2.4171e-06, -7.1928e-08,\n",
      "         -1.9907e-07,  3.6762e-07, -2.0150e-07,  3.3884e-07],\n",
      "        [ 1.9422e-02,  1.9553e-01,  8.1017e-02, -3.5003e-02, -2.5251e-02,\n",
      "          3.0477e-01, -9.1780e-02,  6.0240e-02, -4.9731e-02,  1.0978e-01,\n",
      "          4.0764e-01,  3.1284e-01, -6.7030e-06, -6.9452e-02, -7.4480e-02,\n",
      "          2.5002e-01,  4.9444e-03,  1.5734e-02,  2.6521e-01,  5.8681e-02,\n",
      "          2.5436e-01, -3.1297e-07, -4.3762e-02,  5.5090e-02,  1.2166e-01,\n",
      "         -1.3835e-01,  2.5872e-01, -2.8651e-02, -1.2736e-01, -7.2517e-08,\n",
      "          3.9291e-01,  2.3448e-01,  1.1104e-01,  1.4600e-02, -1.6006e-01,\n",
      "          4.2735e-03,  5.2610e-02,  1.4445e-04,  5.1551e-02, -5.7269e-07,\n",
      "          1.4955e-01,  2.0286e-02,  4.4664e-07, -4.7735e-07,  2.4153e-01,\n",
      "          5.6727e-02, -8.1819e-02, -8.3173e-08,  3.6863e-02, -3.8750e-06,\n",
      "          2.3950e-01,  8.8317e-02,  2.8232e-01,  9.5045e-06,  4.8639e-03,\n",
      "         -1.8945e-01,  3.3213e-02,  2.2959e-01,  2.3819e-01,  8.9238e-04,\n",
      "          3.3762e-01,  2.7877e-02,  6.1483e-03, -2.0086e-01],\n",
      "        [ 1.8080e-02, -7.7911e-02,  2.5287e-02, -8.9749e-02,  2.1689e-01,\n",
      "          3.8273e-02,  4.4116e-02,  2.5121e-01,  5.1348e-02,  7.4489e-02,\n",
      "         -2.1486e-01, -8.6744e-02, -7.1750e-03,  2.4442e-01,  3.8009e-02,\n",
      "          2.0421e-01,  1.3196e-01, -7.0743e-07, -7.6072e-02,  3.9232e-01,\n",
      "         -1.0279e-01,  4.9720e-01, -1.4769e-06, -3.5905e-02,  3.3906e-01,\n",
      "         -2.0484e-01,  2.5666e-01, -1.8319e-06,  5.4239e-02, -5.9165e-07,\n",
      "         -6.1091e-07,  2.2080e-01, -4.6419e-02,  4.0472e-01,  1.8827e-01,\n",
      "          1.1718e-01,  3.2173e-01, -9.4991e-02, -2.5111e-01, -8.1293e-06,\n",
      "          1.9780e-01,  2.2837e-01,  6.1600e-05,  2.2128e-01,  2.0978e-07,\n",
      "          4.4157e-06,  1.2931e-01,  8.3382e-02,  2.9506e-06,  6.1183e-02,\n",
      "          3.1542e-08, -2.8851e-06,  3.3732e-05, -2.1631e-01,  1.5773e-05,\n",
      "         -4.5094e-03, -1.1248e-01,  1.3406e-05, -1.4192e-01,  1.4397e-01,\n",
      "         -4.3107e-02,  9.7586e-05,  1.6592e-01,  7.6931e-02],\n",
      "        [ 1.2853e-01, -1.4294e-01, -1.0847e-01,  2.8687e-01,  1.5626e-01,\n",
      "          1.5897e-07, -1.6161e-03, -1.5434e-06, -1.3625e-01, -7.0834e-02,\n",
      "         -4.1972e-06,  1.8476e-01,  1.2105e-01, -4.7808e-07,  1.1080e-01,\n",
      "         -1.9161e-04, -5.2264e-02, -7.4852e-07,  2.4328e-02,  2.5914e-01,\n",
      "         -1.9976e-02, -4.6104e-07, -1.8434e-01, -2.2295e-03, -1.4974e-06,\n",
      "          5.7337e-04,  4.8408e-08,  2.0438e-01,  1.4522e-01,  9.4111e-02,\n",
      "         -1.6493e-01,  7.8864e-08, -8.0549e-07, -3.1639e-02, -1.6237e-01,\n",
      "         -5.3309e-08,  8.1227e-06,  1.8905e-01, -2.0153e-06,  6.1031e-02,\n",
      "          2.9395e-02, -1.7586e-06,  1.0093e-01,  1.9981e-07,  2.0093e-07,\n",
      "          2.9289e-01, -8.8092e-05,  2.6042e-01,  1.0072e-01, -2.6125e-06,\n",
      "          5.9625e-09, -1.8827e-02,  1.1775e-02,  1.1661e-01,  3.0791e-01,\n",
      "          1.4450e-01, -1.2669e-01, -3.2218e-02,  2.3606e-02, -6.9626e-07,\n",
      "          1.0132e-05, -7.8053e-07, -1.9274e-06, -9.4155e-02],\n",
      "        [-1.2636e-02, -1.1274e-01, -8.3828e-02, -6.1447e-02,  3.1113e-06,\n",
      "          5.7803e-06,  8.6580e-02,  5.6176e-02, -6.6901e-07,  3.9508e-01,\n",
      "         -1.4168e-07, -2.5918e-02,  1.5900e-03,  7.0672e-02,  2.2856e-01,\n",
      "          6.4749e-08,  1.2436e-01,  7.7760e-03, -5.2652e-02, -8.9147e-07,\n",
      "         -1.4184e-01, -1.7384e-07,  2.0597e-01,  2.5116e-02, -9.8575e-02,\n",
      "          3.0152e-01, -3.3599e-07,  2.3962e-01,  3.4035e-06,  2.3034e-01,\n",
      "          6.8893e-02, -3.5349e-07, -1.5237e-01,  4.0064e-07, -1.0784e-01,\n",
      "          3.1887e-01,  3.6907e-07,  1.9818e-02,  5.4898e-04,  3.7673e-01,\n",
      "         -7.5198e-07,  1.2001e-01,  7.1027e-06,  1.2279e-06, -5.8411e-06,\n",
      "         -1.8135e-01, -2.2230e-06, -1.8963e-01, -1.0686e-01,  3.0111e-01,\n",
      "          1.5161e-07,  8.8589e-02, -3.7439e-06,  3.9973e-03, -2.4856e-02,\n",
      "          2.9979e-01,  5.4870e-02, -7.1669e-02,  1.2807e-03, -2.9248e-06,\n",
      "         -2.3672e-07,  1.1251e-06, -5.5342e-02,  2.9285e-03],\n",
      "        [-5.9502e-02, -2.7777e-07,  1.6301e-01,  1.2417e-01, -2.0792e-01,\n",
      "          5.4786e-07, -2.4283e-03, -2.8133e-06,  2.7919e-01, -9.0573e-03,\n",
      "          6.9841e-07, -1.4701e-02,  7.3945e-04, -2.0647e-06, -2.0559e-03,\n",
      "         -1.3878e-06, -2.3611e-06,  6.1053e-07, -5.3831e-07, -3.8988e-02,\n",
      "          2.4141e-01, -2.4470e-07,  2.3452e-02, -7.3082e-02, -3.3424e-07,\n",
      "          2.1364e-01,  5.5235e-07,  1.7583e-01,  3.9090e-01,  1.0909e-01,\n",
      "         -2.0500e-07, -4.9363e-08,  2.1372e-01, -1.4499e-06,  4.6897e-08,\n",
      "         -5.6037e-07, -7.5665e-02, -8.4876e-02,  1.2473e-02, -7.6978e-06,\n",
      "          1.2979e-01, -1.8877e-07,  9.0656e-03,  1.3083e-01, -3.1416e-07,\n",
      "          2.3747e-01,  3.4650e-01, -1.8871e-01,  2.7110e-02, -1.7105e-02,\n",
      "         -1.1365e-06, -7.7368e-02,  8.7203e-02,  2.6201e-06, -1.3754e-07,\n",
      "          9.6756e-07,  8.7034e-02,  9.3325e-02, -1.6285e-01, -9.5304e-07,\n",
      "         -5.3727e-07,  2.1478e-06, -1.0716e-06, -1.3603e-01],\n",
      "        [-9.6040e-07, -7.9434e-07, -1.4747e-07, -8.2445e-07, -1.6637e-07,\n",
      "          2.7921e-07, -3.1320e-08, -7.7940e-07,  1.2262e-07, -9.6470e-08,\n",
      "         -3.2493e-07,  5.8963e-07,  5.0605e-07, -1.4540e-06,  3.4718e-07,\n",
      "          3.8418e-07, -4.9722e-07, -2.8091e-07, -1.1904e-07,  7.6691e-07,\n",
      "         -3.0408e-06,  1.1143e-07, -1.2129e-06, -4.3595e-07, -1.1359e-07,\n",
      "         -9.1002e-07,  1.9650e-07, -4.5616e-08, -2.4171e-07, -8.9797e-08,\n",
      "         -7.5476e-08, -5.8242e-07, -2.3294e-07, -8.6976e-08,  3.8023e-08,\n",
      "         -1.9839e-07,  4.9011e-07, -2.8733e-07,  6.8044e-08, -7.6673e-07,\n",
      "         -1.0304e-07,  5.5930e-07, -2.4643e-06,  2.6083e-07,  2.1066e-07,\n",
      "          6.8235e-10,  1.4607e-07, -1.4779e-07,  2.0875e-07,  1.9028e-07,\n",
      "         -3.6203e-07, -8.0495e-07,  8.7447e-08, -5.6621e-07, -2.5727e-07,\n",
      "         -2.9936e-07, -3.2168e-07, -7.9677e-07, -2.4729e-06, -1.4677e-06,\n",
      "         -1.8868e-07,  4.7045e-07, -1.2567e-07,  3.1378e-07]])), ('classifiers.6.bias', tensor([-1.0381e-01, -9.8106e-07, -7.2679e-02, -1.3897e-01,  1.2820e-01,\n",
      "         6.6271e-02, -8.3474e-07,  3.9852e-06,  1.4454e-06, -1.4521e-01])), ('classifiers.7.weight', tensor([[-1.0527e-01, -1.1914e-04, -4.5819e-02,  3.3937e-07, -4.2905e-05,\n",
      "         -1.1296e-01,  3.2446e-01,  6.3076e-02,  4.6190e-02,  9.6414e-02,\n",
      "         -8.5822e-02, -5.0741e-07,  1.9678e-05,  1.6099e-01,  4.1657e-03,\n",
      "          1.6862e-02,  1.9297e-02,  2.3854e-01,  5.5272e-07, -2.7335e-01,\n",
      "         -3.6001e-02,  7.6274e-02,  1.2808e-01,  1.5425e-01,  1.2269e-01,\n",
      "          1.3132e-01,  2.2629e-07, -1.3290e-01, -8.8113e-03, -6.1056e-02,\n",
      "          2.7981e-01, -1.5497e-06, -6.2696e-02,  1.1775e-01,  3.3573e-02,\n",
      "          1.5920e-01, -1.8954e-06, -1.5564e-06,  1.6188e-01, -1.4423e-05,\n",
      "         -5.6114e-04,  2.9167e-01,  3.6216e-02, -2.7805e-03, -3.0442e-02,\n",
      "          1.5282e-06,  2.6549e-06,  1.8466e-01, -1.2270e-06,  1.5197e-01,\n",
      "          4.7571e-07, -1.4655e-01, -7.4111e-02, -3.1663e-07, -3.3771e-07,\n",
      "          6.3469e-03,  1.6193e-01, -9.2311e-02,  1.5653e-02, -8.0055e-07,\n",
      "         -2.3845e-02, -3.9552e-02, -3.9668e-05,  2.5790e-01],\n",
      "        [-1.0566e-01,  3.2210e-01, -1.1978e-07, -3.1467e-02, -6.3955e-09,\n",
      "          3.7634e-01, -9.1133e-02, -2.7160e-02, -4.9065e-08, -7.4994e-08,\n",
      "          8.2480e-07,  3.2879e-06, -2.0399e-02, -8.3528e-07,  1.0255e-07,\n",
      "         -1.4940e-02,  1.3514e-01, -6.6469e-07,  2.0295e-01, -2.9351e-06,\n",
      "          5.9795e-02, -3.2144e-07, -5.2536e-07, -4.3976e-07, -7.6871e-02,\n",
      "         -3.0033e-06, -8.8782e-02, -9.6938e-03, -2.3218e-02,  9.3669e-07,\n",
      "          3.5508e-01,  3.1260e-01, -3.8640e-06,  8.2145e-08, -3.7663e-02,\n",
      "         -1.9874e-02, -9.7028e-02, -3.2014e-02,  1.2020e-01, -1.5983e-07,\n",
      "         -9.8622e-07, -1.3183e-01, -4.7071e-07, -3.1280e-06,  3.1732e-03,\n",
      "         -5.1998e-02, -1.7863e-06, -3.6779e-07,  2.3706e-01, -1.0242e-07,\n",
      "          2.6072e-01,  8.4626e-02,  2.4245e-01, -8.4322e-02,  2.0340e-01,\n",
      "         -2.7489e-05, -1.1978e-07,  8.4599e-02, -1.2568e-05, -1.0348e-02,\n",
      "          1.0151e-01,  1.7550e-01,  1.1322e-07, -5.2085e-02],\n",
      "        [-2.9278e-02,  6.7806e-02, -3.5900e-02,  2.0954e-01,  1.7567e-01,\n",
      "          4.0407e-02,  7.1060e-02, -1.5432e-01,  2.7756e-01, -2.5598e-01,\n",
      "          1.4091e-07, -2.1670e-01, -8.1025e-02, -1.8658e-01,  5.3773e-03,\n",
      "         -1.2877e-01,  6.6462e-02,  3.9942e-02,  8.6798e-02, -7.9225e-03,\n",
      "          2.5836e-01, -4.7886e-06,  2.5630e-02, -6.1059e-05,  4.0115e-07,\n",
      "          3.9383e-01, -9.7400e-02,  2.7261e-01,  6.3595e-02,  1.3048e-01,\n",
      "         -6.5650e-02,  1.1147e-02,  2.8387e-03, -1.0430e-01,  8.4912e-02,\n",
      "         -7.0403e-02,  1.9986e-06,  1.8361e-01,  7.3577e-02,  3.3235e-01,\n",
      "         -2.5129e-02, -1.2232e-02,  2.7055e-01, -1.8420e-07,  1.6049e-01,\n",
      "          1.0292e-01,  1.1150e-01,  5.7963e-02,  1.6136e-01, -1.9967e-01,\n",
      "          2.4732e-07,  3.1277e-02, -2.6158e-01,  4.5005e-01, -7.5257e-02,\n",
      "          1.3883e-01,  5.6673e-02,  8.5135e-02,  1.6102e-01,  2.2197e-01,\n",
      "         -1.2602e-01, -1.3265e-01, -1.4665e-07,  2.9988e-01],\n",
      "        [-5.1055e-07, -3.7642e-07,  2.7489e-01,  1.1789e-02, -3.0234e-02,\n",
      "          8.4280e-07,  2.0587e-06, -9.5489e-02, -5.0065e-06, -4.0394e-03,\n",
      "         -6.1991e-07, -1.0695e-06,  4.7635e-02, -5.6731e-06, -2.0583e-06,\n",
      "         -8.8386e-07, -1.2009e-06,  2.1719e-06,  7.9424e-07, -3.8443e-02,\n",
      "          1.4288e-02, -1.1224e-01, -1.5215e-02, -1.6763e-06, -2.9542e-06,\n",
      "         -4.8770e-04, -1.1664e-02,  2.1076e-01,  3.3351e-01,  1.1330e-01,\n",
      "          9.3217e-07,  1.2266e-07,  9.6172e-02, -3.3933e-06, -1.5374e-06,\n",
      "         -4.9188e-02, -2.3682e-02, -2.4510e-06, -8.7493e-08, -1.1391e-06,\n",
      "          1.1021e-01, -2.3259e-06, -7.3641e-07,  2.4659e-01,  5.0986e-07,\n",
      "          1.1229e-01,  2.1594e-01, -9.1180e-02, -2.3111e-02, -2.5753e-06,\n",
      "          4.5674e-07, -3.8864e-08,  1.2791e-01, -3.5616e-07, -8.4065e-07,\n",
      "          1.4128e-02, -3.4653e-02, -1.0380e-05, -2.6502e-05, -3.2268e-06,\n",
      "          2.9751e-08,  2.5247e-06,  2.9865e-07, -3.4085e-02],\n",
      "        [-5.9035e-07, -7.1322e-02, -7.0014e-07, -1.3727e-06,  3.4670e-06,\n",
      "          4.9940e-07,  8.1147e-06,  4.1732e-02, -1.5935e-07,  1.2642e-01,\n",
      "         -8.2673e-07, -1.7700e-06,  2.6214e-02,  1.8542e-02, -2.3761e-04,\n",
      "         -1.9377e-06, -2.5896e-02, -8.1169e-02, -1.5516e-02,  6.4273e-08,\n",
      "         -2.0084e-02, -6.5269e-07,  1.1865e-01,  1.5514e-01, -1.2516e-07,\n",
      "          2.2126e-01, -1.7925e-02,  3.0293e-01, -7.7476e-02, -2.1996e-02,\n",
      "         -8.9441e-07, -9.3743e-07, -2.2456e-06,  9.2840e-08, -5.9219e-07,\n",
      "          1.8298e-01, -6.1463e-02, -1.6147e-06, -8.1806e-05,  1.3058e-01,\n",
      "         -1.6304e-05, -5.4932e-04,  4.2347e-07,  1.6211e-02,  8.0036e-08,\n",
      "         -4.5531e-02, -1.1972e-01,  5.6755e-07,  1.4311e-08,  2.8587e-02,\n",
      "         -1.0891e-07,  2.5085e-01, -3.6150e-06, -7.7432e-02, -2.3711e-07,\n",
      "          1.8745e-01, -3.4956e-03, -7.3494e-07, -8.0065e-02, -4.3997e-07,\n",
      "         -1.3277e-06,  1.5519e-01, -1.0469e-08,  1.3267e-06],\n",
      "        [-2.2448e-07, -1.3999e-06,  2.1703e-07, -8.4923e-08, -4.2993e-07,\n",
      "          2.8476e-07, -7.7803e-07, -5.6822e-07, -1.1738e-06,  1.2086e-07,\n",
      "          3.1744e-07,  4.6633e-07, -1.1978e-06,  3.6669e-07,  2.2045e-07,\n",
      "          5.9657e-07, -2.4704e-06, -1.5598e-07, -4.0844e-07, -3.7883e-07,\n",
      "          5.1867e-07, -3.1005e-07,  9.1697e-08, -2.4051e-07,  9.4018e-08,\n",
      "         -5.4075e-07,  3.7583e-07,  1.4765e-07,  3.0004e-07, -3.9044e-07,\n",
      "         -2.4269e-07, -5.9179e-07, -1.0013e-06, -1.2163e-08,  5.5558e-07,\n",
      "         -3.7852e-07, -6.0016e-07,  5.5259e-07,  3.0184e-08, -2.7481e-07,\n",
      "         -3.5768e-08,  1.0546e-08,  1.3991e-07,  1.5982e-07, -2.2044e-07,\n",
      "          5.0536e-07,  5.0501e-08,  3.8191e-08, -5.5998e-08,  1.8608e-07,\n",
      "         -1.0309e-07,  6.6710e-08, -2.0092e-06, -8.0371e-07, -2.6183e-07,\n",
      "          5.1662e-07,  6.8620e-07, -2.7865e-08,  1.8606e-08,  4.2220e-07,\n",
      "         -1.4227e-07,  2.7409e-07, -1.4493e-06, -4.4480e-07],\n",
      "        [-8.1724e-02,  1.7758e-02,  2.6327e-01, -7.7809e-02,  3.1886e-01,\n",
      "         -2.1510e-06, -6.8446e-08, -5.6356e-07,  1.5591e-01, -5.4613e-02,\n",
      "          2.0987e-01,  1.6059e-01, -1.6063e-05,  2.0591e-05, -1.8245e-01,\n",
      "          2.8955e-01, -1.5411e-04,  5.8333e-05, -1.9955e-02,  1.0444e-01,\n",
      "         -3.3736e-02,  3.3446e-01, -1.6220e-04, -8.4206e-02,  9.4111e-02,\n",
      "         -1.0255e-01,  1.6699e-01, -3.4621e-02, -1.6364e-01, -2.3390e-07,\n",
      "          1.2889e-01, -1.6437e-06,  2.0757e-01,  2.2654e-05,  7.9499e-04,\n",
      "          2.8380e-01,  9.2415e-02,  2.1831e-05, -5.1265e-03,  2.9407e-06,\n",
      "          8.6742e-02, -3.3668e-04,  2.2777e-01,  1.8865e-01, -1.5517e-01,\n",
      "          3.5391e-03, -1.8160e-01,  1.5040e-01,  8.8834e-02, -2.0821e-01,\n",
      "          1.3195e-07, -1.4401e-06,  2.4942e-01,  2.1132e-01, -1.0477e-07,\n",
      "         -2.7703e-01,  2.6111e-01,  6.8375e-05,  6.4594e-02,  3.2354e-01,\n",
      "         -1.4512e-05,  2.9177e-04,  9.8672e-02, -8.5503e-02],\n",
      "        [-5.1333e-07, -3.2002e-07, -1.2772e-07, -2.4470e-07, -4.6091e-07,\n",
      "          2.6782e-07, -3.5919e-07,  1.3200e-07, -4.4802e-07, -6.1920e-07,\n",
      "          3.8814e-07, -4.2193e-07, -7.9163e-07, -2.0777e-07,  1.1254e-06,\n",
      "          1.3409e-07,  3.4451e-07,  3.0544e-07,  1.9522e-07, -1.2341e-06,\n",
      "          1.4866e-07, -4.1683e-07, -2.3170e-07,  8.0961e-09, -7.2971e-09,\n",
      "         -4.6282e-07, -1.6539e-07,  3.0014e-07, -2.2423e-07, -9.7026e-08,\n",
      "         -3.8839e-07,  4.5451e-08, -7.3403e-07,  4.3130e-07, -1.9833e-07,\n",
      "         -3.3280e-07, -9.9088e-08, -2.2441e-08, -1.6294e-07, -1.6954e-07,\n",
      "          9.3578e-07, -2.1578e-07, -1.5120e-07,  1.5245e-07, -2.6729e-07,\n",
      "         -4.6208e-07,  1.1274e-07, -1.7899e-06,  7.5593e-08, -2.1729e-07,\n",
      "         -1.9074e-08, -8.4292e-07,  3.3190e-07,  5.7683e-08, -2.6851e-08,\n",
      "         -1.3783e-06,  7.9181e-09,  1.3805e-07, -2.5258e-07,  8.5266e-08,\n",
      "          1.8678e-07, -1.9921e-07, -5.6708e-07,  4.6445e-07],\n",
      "        [ 1.5673e-01, -1.4818e-01,  8.6798e-02,  9.9069e-02,  1.1372e-01,\n",
      "         -4.9618e-07, -2.0143e-02,  6.5725e-02,  1.0146e-01,  9.1223e-02,\n",
      "         -1.9959e-01,  4.9815e-02,  1.7514e-01, -9.4542e-07,  3.7710e-01,\n",
      "         -1.3920e-01, -2.4817e-03, -6.6334e-02,  3.9071e-08,  1.1152e-01,\n",
      "         -1.3115e-01,  2.8551e-01, -9.3146e-06, -4.1566e-03,  6.7083e-02,\n",
      "         -8.7969e-02,  5.1056e-02, -2.0282e-01, -9.0505e-02, -4.8052e-02,\n",
      "         -5.4972e-02,  8.0188e-02, -2.3087e-02,  1.2652e-01,  2.2672e-04,\n",
      "         -1.1718e-01,  2.0169e-01,  1.8926e-02, -2.2753e-04, -3.6112e-06,\n",
      "          3.0520e-01,  1.7557e-02, -1.6370e-02, -6.9981e-06, -4.0798e-06,\n",
      "          2.0449e-01,  2.2596e-02, -4.8097e-02,  2.3501e-01,  2.9390e-02,\n",
      "         -8.3936e-02, -1.4943e-01, -5.1008e-04,  1.6139e-01,  3.4919e-01,\n",
      "          7.1835e-02, -5.4768e-02,  1.9121e-01, -4.4819e-02, -7.0989e-02,\n",
      "          1.9729e-02, -1.9431e-01,  2.7402e-01,  7.2911e-02],\n",
      "        [ 1.4971e-01, -8.4089e-02, -2.4396e-02,  7.8274e-06,  3.9106e-08,\n",
      "         -1.6223e-07, -1.0466e-07,  1.8142e-01,  2.5903e-07,  7.5573e-07,\n",
      "          2.6000e-01, -7.5311e-07, -1.2969e-06, -3.7648e-06,  8.4282e-08,\n",
      "         -3.0571e-06, -1.9189e-06, -1.1718e-06, -5.6574e-07, -1.3422e-01,\n",
      "          1.8213e-01,  5.5594e-08, -1.8374e-02,  6.6594e-02,  6.3482e-02,\n",
      "         -2.7375e-02, -1.1140e-02, -3.8622e-07, -9.1330e-07, -2.5787e-02,\n",
      "         -2.4749e-08, -7.6563e-02, -3.9299e-02,  4.8566e-02, -7.7706e-02,\n",
      "         -1.5694e-07, -9.0249e-03,  3.3641e-07,  3.4788e-01, -4.9689e-08,\n",
      "          6.3312e-04,  4.2645e-02, -6.2772e-09,  1.2475e-07,  2.1537e-01,\n",
      "         -3.7962e-05, -5.1345e-07,  7.3989e-08, -4.9721e-07, -8.6085e-07,\n",
      "         -3.4671e-02, -8.8432e-07, -2.7226e-06, -9.9934e-07, -3.1686e-06,\n",
      "         -8.3336e-02, -2.8240e-06,  2.3633e-01,  2.4429e-07, -3.0315e-08,\n",
      "          1.4875e-01, -4.0784e-07,  1.3824e-01, -1.9586e-07]])), ('classifiers.7.bias', tensor([-1.5309e-06,  8.4121e-07,  5.6121e-02,  8.6134e-07,  1.6078e-06,\n",
      "        -1.2489e-01,  2.0323e-02, -2.4196e-01,  3.8078e-02, -3.6575e-06])), ('classifiers.8.weight', tensor([[ 3.1244e-08, -1.1472e-06, -1.9485e-07, -3.1048e-06, -6.8644e-07,\n",
      "         -2.6549e-08,  2.1826e-07, -1.2258e-06,  2.2180e-07, -1.8759e-07,\n",
      "         -5.1051e-07, -3.1424e-07, -4.1795e-07,  2.3769e-07,  7.4907e-07,\n",
      "         -2.7953e-07, -6.4400e-07,  2.6417e-07, -4.1118e-07, -4.4243e-08,\n",
      "          6.2632e-07, -6.0492e-09,  1.5591e-07, -3.2111e-07,  6.5188e-07,\n",
      "         -8.8041e-07, -1.0037e-07,  3.6443e-07, -4.3468e-07,  4.7726e-07,\n",
      "         -3.0636e-07, -1.9167e-07, -1.7758e-06, -1.4789e-07,  1.4858e-07,\n",
      "         -7.6457e-07, -8.1591e-07, -1.9007e-07, -1.0953e-06,  3.1659e-07,\n",
      "          1.7213e-07, -8.8919e-07, -7.5316e-07,  5.4523e-07, -8.7192e-07,\n",
      "         -5.4084e-06, -8.8291e-07, -7.0962e-09, -5.2830e-07,  3.0585e-08,\n",
      "         -5.9568e-08,  2.6803e-07, -1.1604e-07,  6.5120e-08,  2.7275e-07,\n",
      "          2.7780e-07,  2.6848e-07, -3.2097e-07, -1.3582e-07, -2.5546e-07,\n",
      "          3.0276e-07,  2.3653e-07, -2.7856e-07, -2.5259e-07],\n",
      "        [-2.3331e-06,  7.5769e-07,  1.2967e-07, -2.6296e-06, -4.8625e-07,\n",
      "         -1.3557e-07, -1.4959e-06,  1.9317e-07,  4.2227e-07, -7.1314e-07,\n",
      "         -3.9616e-07,  3.0258e-07, -5.0260e-07, -1.4140e-06, -6.4112e-07,\n",
      "          1.6388e-07,  1.1235e-08, -1.8020e-07,  8.0452e-08,  1.1951e-07,\n",
      "          3.8704e-08, -7.9933e-08, -1.8466e-08, -5.9864e-07, -4.4508e-07,\n",
      "         -1.5597e-06, -3.8076e-07, -1.3417e-06, -6.3076e-07,  1.5037e-07,\n",
      "         -8.9038e-07,  5.0575e-07, -1.1363e-06,  2.8677e-07, -1.0157e-06,\n",
      "         -1.0836e-06, -9.5344e-07,  3.1560e-07,  5.2590e-08, -5.8218e-07,\n",
      "          5.3089e-08,  1.1024e-07, -1.2437e-07, -5.4460e-07, -9.1833e-08,\n",
      "         -3.3636e-02, -1.1597e-06,  4.5138e-08, -1.4422e-06, -8.9970e-09,\n",
      "         -4.6168e-07,  5.9119e-07, -1.5198e-06,  2.9687e-08, -9.6427e-08,\n",
      "          3.1700e-07, -3.5153e-07,  6.5670e-07,  5.4746e-08, -8.3623e-07,\n",
      "         -6.2250e-07, -1.0850e-07, -3.5596e-07,  1.3938e-07],\n",
      "        [ 2.1152e-01,  1.9019e-06,  8.7625e-02,  1.2706e-05, -3.9662e-02,\n",
      "          2.1350e-07, -5.6598e-03,  6.9904e-02, -6.8852e-03, -1.0240e-02,\n",
      "          3.6464e-01, -1.8532e-01,  1.1851e-01, -2.4123e-06,  1.4432e-07,\n",
      "         -4.5643e-07, -2.3706e-07,  3.1275e-07, -2.2963e-06, -1.4425e-02,\n",
      "          2.8159e-01, -1.6469e-01, -1.0721e-01,  3.1370e-01, -8.7845e-07,\n",
      "         -1.8904e-01, -6.0388e-06,  1.3218e-01,  2.7719e-01,  1.2864e-01,\n",
      "         -2.9773e-06, -6.8838e-02,  2.4887e-01,  5.2253e-06, -1.3321e-03,\n",
      "         -1.5428e-01, -1.0897e-01, -1.2741e-01,  1.9525e-01, -4.3815e-07,\n",
      "          1.0509e-01,  8.5909e-02, -2.1216e-01,  3.7228e-02,  7.4845e-02,\n",
      "          7.5219e-02,  1.5955e-01, -1.0530e-01, -7.9996e-02, -1.0559e-06,\n",
      "         -8.0792e-07,  3.9389e-07,  5.0111e-02, -1.8553e-01, -1.3435e-01,\n",
      "          1.5810e-02, -4.4281e-02,  4.4612e-03, -9.7349e-08, -2.5220e-07,\n",
      "          8.0824e-02,  1.9287e-02,  9.2804e-02, -2.6851e-06],\n",
      "        [-1.2401e-01,  2.8668e-01, -1.7778e-01, -1.0595e-01, -1.1500e-05,\n",
      "          4.6033e-01,  2.5296e-01,  5.6739e-02, -2.4859e-02, -2.2537e-01,\n",
      "         -1.2237e-01,  7.6047e-06, -2.3481e-01,  4.8567e-03,  2.1425e-01,\n",
      "          9.6853e-07,  5.3918e-01,  3.2106e-01,  1.2191e-01, -3.3219e-04,\n",
      "          2.3989e-01,  5.2454e-02,  9.9443e-02, -7.7357e-06,  7.0992e-05,\n",
      "          1.0351e-02, -1.3825e-01,  1.4183e-02, -4.5947e-07,  5.5219e-02,\n",
      "          1.1757e-01,  1.4208e-01, -9.6836e-02,  1.2236e-01,  1.1040e-01,\n",
      "         -1.0305e-01, -7.9168e-02,  7.2166e-02, -5.3754e-06,  1.0263e-02,\n",
      "         -6.3277e-02, -3.3024e-03, -1.6184e-01, -1.5565e-01,  1.5244e-01,\n",
      "         -4.4206e-03, -1.3037e-01,  2.0027e-01,  3.8012e-04,  3.4408e-02,\n",
      "          6.4951e-02,  4.3902e-01,  1.2828e-01,  1.6425e-01,  2.1000e-02,\n",
      "         -3.3921e-06, -4.4966e-06, -3.4814e-04,  1.1094e-01,  2.4206e-01,\n",
      "          1.4118e-01,  9.0648e-02, -1.7189e-02,  2.4893e-01],\n",
      "        [ 8.3328e-08, -1.4021e-08,  1.0839e-07, -2.7577e-06, -1.1560e-06,\n",
      "         -2.1845e-08,  1.9451e-07, -4.1933e-07, -2.6469e-07, -6.0120e-07,\n",
      "         -1.2071e-07, -2.2607e-07,  9.1061e-08, -4.2980e-07,  1.5637e-08,\n",
      "          3.5232e-07,  4.3518e-08,  3.5683e-07, -5.9444e-07, -2.3246e-07,\n",
      "         -3.1614e-07, -5.1561e-07, -3.3256e-07, -5.5855e-08, -3.2488e-07,\n",
      "         -2.1716e-06,  3.2942e-07, -1.1333e-06, -6.7961e-07, -1.2370e-06,\n",
      "         -1.2066e-06,  4.9020e-07, -1.1759e-06, -2.4905e-07, -1.1452e-07,\n",
      "         -4.3669e-07,  4.7807e-07,  5.1867e-08,  2.3058e-07,  9.9166e-08,\n",
      "         -4.0579e-07, -7.0565e-08,  5.3173e-07,  2.9615e-07, -1.5933e-09,\n",
      "         -9.5282e-07,  2.9249e-07,  3.3549e-07, -2.7372e-07,  4.6431e-07,\n",
      "          4.9637e-07, -1.2339e-08, -5.1178e-07, -2.1793e-07, -6.3218e-07,\n",
      "          2.1608e-07, -2.6857e-06, -8.0328e-07, -8.0918e-08,  1.8928e-08,\n",
      "         -2.1290e-07, -4.4847e-07, -1.0940e-06, -1.9628e-07],\n",
      "        [-1.5223e-01, -3.3345e-02, -1.1933e-06,  2.3972e-02, -1.0286e-01,\n",
      "         -9.2204e-02,  1.2366e-01,  4.9995e-02,  1.1570e-01,  6.3008e-02,\n",
      "          1.0334e-06, -2.2721e-01,  6.1427e-03,  2.8290e-01, -6.7473e-02,\n",
      "         -6.3844e-06, -2.2956e-07,  1.4232e-02,  5.3220e-02, -2.3568e-01,\n",
      "          9.6868e-02, -2.7685e-07,  3.0388e-01,  2.4672e-01, -1.2059e-01,\n",
      "          2.6336e-01, -5.3484e-02,  5.9930e-06,  2.0851e-01, -1.5839e-01,\n",
      "          7.1773e-03, -1.0895e-01, -2.7202e-07, -1.7823e-02, -8.3036e-02,\n",
      "          1.4336e-01,  5.3256e-02, -1.0849e-01,  9.4840e-02,  9.4495e-02,\n",
      "         -7.7169e-02,  1.7505e-04,  2.7433e-02,  8.2212e-07, -1.3623e-02,\n",
      "          5.6353e-06,  7.6063e-04, -3.6147e-02,  1.2478e-01,  2.6338e-01,\n",
      "         -7.1375e-08,  2.8233e-01, -3.8354e-07,  1.8239e-04, -1.0577e-01,\n",
      "          8.8096e-02,  3.1521e-01,  1.9469e-01,  4.5953e-04,  1.0783e-06,\n",
      "         -1.7565e-01,  6.9954e-02, -9.9034e-08,  6.4614e-06],\n",
      "        [ 8.5519e-02, -2.4915e-06,  2.3952e-01, -1.1152e-01, -2.1163e-06,\n",
      "          5.1890e-07, -4.0054e-07,  6.9469e-02,  2.4479e-04,  2.6187e-02,\n",
      "         -1.0713e-06, -4.7509e-04,  5.5387e-06, -1.4712e-06,  2.1347e-01,\n",
      "         -8.7469e-02,  6.0004e-08, -4.0851e-07, -1.0152e-06,  2.8371e-01,\n",
      "         -1.1091e-06,  1.1177e-01, -2.0659e-06,  3.8468e-07, -4.5846e-02,\n",
      "         -2.2336e-07,  2.1601e-01, -1.2762e-01, -1.4199e-06, -2.8862e-02,\n",
      "          3.8231e-07, -3.7037e-02, -1.3426e-01,  6.9298e-05,  1.1514e-01,\n",
      "         -6.8700e-06,  2.5898e-01, -8.8293e-02, -1.4043e-07,  1.2479e-07,\n",
      "         -8.3387e-02, -1.7550e-05, -1.2477e-01, -1.7729e-01, -2.3168e-06,\n",
      "         -5.2560e-04,  2.3194e-01, -2.1458e-06,  4.6496e-07,  1.2843e-06,\n",
      "         -7.4179e-07,  1.3317e-08, -1.8628e-01, -5.2596e-02, -1.2707e-06,\n",
      "          1.9086e-07, -9.7638e-02,  2.7238e-02, -3.1894e-02, -6.7976e-02,\n",
      "         -2.3919e-06, -2.0087e-06,  1.0400e-02,  4.4692e-02],\n",
      "        [ 8.6650e-02, -2.4674e-07, -1.7549e-01,  2.6955e-02,  4.6516e-02,\n",
      "         -1.1011e-07, -6.4882e-06, -4.9217e-07, -7.3801e-06,  1.7279e-01,\n",
      "          1.0125e-07, -1.1216e-07, -5.1721e-02,  1.7916e-01, -3.4697e-02,\n",
      "         -1.7330e-05,  1.5358e-07,  3.0854e-07,  5.3772e-08,  1.6075e-01,\n",
      "         -1.6814e-06,  1.3516e-01, -1.6267e-02, -9.7940e-07,  3.4938e-02,\n",
      "         -2.0374e-02, -1.5862e-01, -7.8968e-02, -1.1245e-01, -2.2239e-06,\n",
      "         -3.7605e-02,  2.4680e-01, -1.5250e-05,  1.5780e-01, -5.1709e-02,\n",
      "         -1.2436e-01,  1.1822e-01, -1.7823e-08, -3.6002e-05, -2.5676e-07,\n",
      "          2.3714e-01,  6.8665e-02, -1.2740e-01,  9.0444e-02,  2.6821e-07,\n",
      "          1.0847e-01, -9.4874e-06, -5.9203e-02, -7.7388e-02,  1.2723e-01,\n",
      "         -5.9585e-02,  4.7804e-07, -8.1417e-02, -1.0276e-06,  1.8298e-01,\n",
      "         -8.6088e-02, -1.7860e-02, -2.0182e-06,  2.3500e-07, -8.9191e-07,\n",
      "         -4.8573e-02, -8.6650e-02,  8.7847e-02, -3.9333e-02],\n",
      "        [-1.3646e-04,  2.0381e-01,  1.0532e-01, -8.7137e-03,  8.6702e-02,\n",
      "          6.8308e-06, -1.3390e-02, -2.5826e-07, -1.5272e-01, -2.5263e-02,\n",
      "          1.8807e-01,  9.5480e-02,  1.6966e-01, -1.5353e-02, -2.3500e-01,\n",
      "          2.3171e-01, -3.1281e-07,  2.8417e-05,  4.8506e-02,  3.4763e-01,\n",
      "         -9.4410e-02,  1.8978e-07, -1.8941e-01, -8.5054e-03,  1.9297e-01,\n",
      "          1.2736e-04,  1.0110e-01, -5.6388e-02,  6.5822e-06,  1.6416e-01,\n",
      "          6.2537e-02, -9.9009e-02,  1.7736e-01, -1.1444e-01, -3.1917e-02,\n",
      "          1.4078e-01,  2.5858e-01,  1.1580e-01,  9.4660e-08,  1.5802e-01,\n",
      "          9.7138e-02, -3.9542e-02,  3.3605e-02,  9.5102e-02, -4.2243e-07,\n",
      "          6.2144e-02, -9.6732e-02,  7.2991e-02, -7.8545e-02, -1.4749e-01,\n",
      "         -3.4465e-02, -6.0954e-07,  1.8891e-02,  2.1812e-01, -5.9726e-04,\n",
      "         -5.1882e-03,  4.6710e-02,  1.0245e-04,  2.9858e-01,  2.8509e-01,\n",
      "         -2.3479e-01,  3.1158e-02,  7.0461e-05, -1.5940e-01],\n",
      "        [ 4.5567e-06, -5.0623e-02, -2.6810e-01, -1.3877e-06,  2.1022e-01,\n",
      "         -2.5549e-07,  4.1582e-08, -3.3771e-04,  9.5723e-02, -1.2452e-01,\n",
      "         -7.3611e-02,  4.1559e-02,  1.4231e-01,  3.6465e-02,  2.6769e-01,\n",
      "          2.9059e-01, -5.2411e-07, -1.9608e-02, -1.0938e-02, -1.0610e-04,\n",
      "         -1.0535e-06,  2.3231e-01, -9.5150e-02,  5.9291e-07, -9.0241e-02,\n",
      "         -8.7269e-02,  4.8422e-06, -1.4774e-01, -1.4156e-03, -1.5064e-01,\n",
      "         -3.6040e-02, -2.6710e-02,  1.4930e-02,  2.2222e-02,  2.0327e-01,\n",
      "         -2.4909e-02, -5.6369e-03,  1.1575e-01, -1.3880e-02, -1.1930e-01,\n",
      "          1.8382e-01, -2.8812e-02,  2.5703e-02,  1.2101e-01, -1.2155e-01,\n",
      "          2.1652e-02, -3.7851e-02,  3.2697e-01,  1.5765e-01, -1.0145e-01,\n",
      "         -1.0186e-06,  7.4566e-08, -6.3201e-06,  9.7178e-02,  3.2563e-01,\n",
      "          6.2080e-02,  2.2138e-02, -2.9646e-03, -5.3640e-02,  2.7456e-07,\n",
      "          9.9072e-02, -2.9098e-02, -7.1824e-02, -3.6157e-03]])), ('classifiers.8.bias', tensor([-2.5039e-01, -1.5461e-01,  1.2526e-05,  9.5816e-02, -1.7709e-01,\n",
      "         3.4967e-02, -4.8649e-08, -8.7741e-07,  9.1357e-02, -1.8347e-06])), ('classifiers.9.weight', tensor([[ 5.4014e-06, -2.6592e-07, -3.0880e-06,  1.5187e-01, -1.5748e-06,\n",
      "         -9.0959e-02,  8.1974e-08,  1.7018e-07, -1.9833e-06, -1.1368e-06,\n",
      "         -9.8493e-02,  2.3555e-01,  1.4522e-01, -1.2690e-06,  9.1957e-02,\n",
      "         -5.4694e-02, -5.5489e-03,  6.9493e-08, -9.3111e-08, -1.0869e-06,\n",
      "         -1.5984e-03, -5.8220e-07,  2.6199e-07,  6.0871e-07, -8.5534e-07,\n",
      "         -1.0363e-06,  7.1260e-07, -4.5736e-02, -5.4078e-02, -9.5512e-02,\n",
      "          5.3819e-07, -4.2081e-07, -1.1019e-01, -2.7135e-07, -1.8680e-06,\n",
      "         -4.5335e-07,  2.0156e-06,  1.5538e-01, -2.8339e-06, -4.2465e-07,\n",
      "          1.7164e-01, -1.4800e-07, -6.0020e-08, -1.8166e-06, -1.6229e-06,\n",
      "          1.1240e-01, -1.7463e-02,  1.1938e-05,  2.8776e-01,  1.2889e-07,\n",
      "          4.7482e-08, -5.8329e-07, -4.3159e-06,  6.5835e-02,  3.5867e-01,\n",
      "          1.2149e-01, -6.8838e-06, -4.2875e-06, -9.1727e-08,  3.3597e-07,\n",
      "         -9.1730e-07,  4.7964e-07, -5.3711e-02, -1.2804e-06],\n",
      "        [ 1.3428e-01, -9.6567e-07, -7.6835e-02,  8.0397e-07, -2.2166e-07,\n",
      "         -8.3932e-07, -3.5091e-07,  1.8948e-01, -6.4386e-02,  3.4874e-02,\n",
      "          1.6307e-01,  1.3549e-08, -9.5500e-07, -8.0150e-08, -3.3985e-07,\n",
      "         -6.1411e-07, -5.2787e-02, -3.3852e-03, -4.3660e-07,  9.8544e-08,\n",
      "          1.1991e-01, -2.8784e-08, -1.9718e-09,  2.1244e-01,  1.0216e-02,\n",
      "         -1.2563e-07,  3.5497e-07,  1.4534e-07, -5.9290e-08,  2.8011e-08,\n",
      "         -6.1010e-07, -7.9184e-07,  2.0449e-07,  2.0168e-01,  8.9250e-08,\n",
      "          8.5727e-09, -1.9259e-07,  1.2636e-07,  2.9194e-01, -7.6098e-08,\n",
      "          4.4439e-05,  7.8176e-02, -7.4129e-07, -1.3660e-07,  7.6095e-02,\n",
      "         -1.6422e-01, -6.0629e-07, -2.4729e-02,  2.9282e-08,  1.7554e-07,\n",
      "          1.7390e-02, -1.8528e-03, -4.4519e-02, -5.2791e-02, -2.5307e-06,\n",
      "         -7.6946e-07, -2.5973e-06,  1.9606e-01, -2.4937e-07,  1.1768e-07,\n",
      "          2.7820e-01, -3.5350e-02,  2.0468e-02, -9.1924e-07],\n",
      "        [-1.8112e-06, -3.1546e-07, -4.8979e-06, -7.0645e-08,  3.0796e-01,\n",
      "         -8.5684e-07, -3.8554e-07, -7.4971e-06,  6.9256e-02, -2.2379e-06,\n",
      "         -4.0542e-02, -8.2842e-07, -4.7189e-06,  7.6010e-02, -1.4382e-05,\n",
      "          2.8632e-01,  8.7268e-07,  2.6998e-07, -1.4915e-07,  1.1185e-01,\n",
      "          2.8879e-07,  3.9699e-01, -2.5804e-07, -6.3370e-07,  5.8784e-08,\n",
      "         -4.3482e-08,  2.6408e-06, -5.1084e-07, -6.9479e-07, -1.4304e-01,\n",
      "          7.2436e-07, -5.7223e-07,  1.2515e-01,  1.0723e-01,  1.6414e-01,\n",
      "         -1.5043e-07, -9.6102e-07, -6.5437e-07, -1.7435e-06,  6.8463e-08,\n",
      "         -5.8195e-02,  3.2815e-06,  2.6369e-01,  2.5551e-01,  3.3557e-07,\n",
      "         -2.5634e-01, -6.1179e-06,  5.9631e-02,  5.7107e-02, -3.2471e-07,\n",
      "          1.8483e-07, -1.1073e-06, -8.1985e-02, -2.7768e-06, -3.4194e-08,\n",
      "         -3.8040e-02,  3.4213e-02, -5.6473e-02, -5.7242e-09, -3.9184e-07,\n",
      "         -9.3718e-07, -1.2108e-06,  2.8833e-07, -2.3572e-07],\n",
      "        [ 5.9890e-08, -1.8110e-07, -2.2635e-08,  1.1946e-06,  3.2678e-07,\n",
      "         -3.8483e-07,  2.0243e-07,  1.3833e-07,  1.7058e-07, -3.7052e-07,\n",
      "          6.2469e-08,  3.1538e-08,  5.4230e-07,  5.4710e-07,  8.6560e-07,\n",
      "          3.8379e-07, -6.5524e-07, -5.2442e-07,  2.2231e-07, -3.7962e-07,\n",
      "         -1.9789e-07, -7.6055e-07,  1.9848e-07, -1.4802e-08, -2.2486e-07,\n",
      "         -3.5028e-07,  5.3585e-07, -2.7493e-07,  2.9449e-07, -1.4773e-06,\n",
      "         -3.6594e-07,  8.7806e-07, -8.0911e-08,  3.6512e-08,  2.2987e-07,\n",
      "         -5.3506e-07, -1.3816e-08,  3.4101e-07, -1.2884e-07, -9.0943e-07,\n",
      "         -6.5061e-08, -2.3905e-07, -1.4934e-08, -4.8094e-07, -2.1218e-07,\n",
      "         -6.8588e-07, -2.6464e-08, -6.3386e-07,  1.9262e-07,  3.6616e-08,\n",
      "         -9.3912e-08,  2.3674e-07, -4.5496e-07, -1.0905e-06,  6.6577e-08,\n",
      "         -1.5882e-06,  1.6790e-07,  4.0557e-07,  2.3737e-07, -1.3566e-07,\n",
      "         -1.2874e-07,  3.2893e-07,  2.7699e-07, -3.4317e-07],\n",
      "        [-4.3676e-07, -3.2190e-07,  2.9360e-08, -2.0101e-07,  1.0897e-07,\n",
      "         -6.3385e-07, -4.4862e-07,  4.8449e-08, -1.1921e-07, -4.0115e-08,\n",
      "          1.4664e-07,  3.2910e-07, -1.4011e-07, -7.4515e-08,  2.7549e-07,\n",
      "          1.1872e-08,  3.0703e-07, -1.3004e-08, -3.3556e-07, -5.8709e-07,\n",
      "         -5.7197e-07, -2.0388e-07,  3.3214e-07,  1.4280e-07, -4.4287e-07,\n",
      "          2.7115e-07, -2.1990e-07,  3.4978e-07, -9.3970e-07, -4.5635e-07,\n",
      "         -9.9099e-07, -3.4571e-07,  1.7146e-07,  7.9756e-07, -4.1690e-07,\n",
      "          6.5191e-08,  7.9085e-08,  6.3556e-08,  2.7090e-07, -4.0539e-08,\n",
      "         -1.2519e-07,  3.5516e-07,  2.4097e-07,  5.8927e-07, -2.1191e-07,\n",
      "         -5.4033e-08, -5.4240e-07, -1.2701e-08,  1.2986e-07,  2.8637e-07,\n",
      "         -2.1288e-07,  1.2027e-07,  6.5327e-07, -1.5501e-06,  1.1628e-07,\n",
      "          8.9435e-08, -1.4106e-06,  5.7730e-08, -8.9438e-08, -1.5290e-07,\n",
      "         -1.5749e-07,  2.0731e-07, -4.6670e-07, -4.4233e-08],\n",
      "        [-7.5259e-08, -1.0026e-06,  1.2945e-01,  1.5075e-03, -1.8782e-05,\n",
      "          7.2130e-07,  1.1862e-06, -6.1581e-07, -7.3928e-06, -2.7725e-07,\n",
      "         -4.9386e-07,  8.9416e-08,  1.1397e-01, -8.1288e-07, -9.7800e-02,\n",
      "          4.2860e-08, -2.3665e-06,  2.8177e-06, -3.5663e-06, -6.7324e-06,\n",
      "          4.4098e-03,  2.6442e-07, -7.6588e-06,  1.0432e-06,  1.3715e-06,\n",
      "         -6.0133e-07,  1.7676e-07,  2.3654e-01,  1.2203e-01,  6.4900e-02,\n",
      "          4.9727e-09, -7.0330e-08,  3.0024e-01,  5.7464e-08, -7.0956e-02,\n",
      "         -3.7823e-07, -2.5790e-06, -9.6806e-06, -1.0005e-05, -3.8070e-07,\n",
      "          5.0514e-02, -1.4116e-07, -8.7418e-02,  4.1959e-02, -4.8076e-07,\n",
      "          5.2818e-02,  3.2353e-01, -2.7825e-05, -9.8852e-03,  1.9677e-07,\n",
      "          3.4702e-08, -1.4832e-02,  2.8184e-01, -7.6665e-02, -4.5152e-07,\n",
      "          4.4656e-03, -1.1312e-05, -5.2513e-02, -6.0170e-07, -1.5656e-02,\n",
      "          4.6445e-07, -1.8048e-07, -2.4783e-08, -6.0361e-07],\n",
      "        [-5.4911e-07, -7.7199e-07, -2.0623e-07, -4.7558e-07, -6.3588e-07,\n",
      "          5.2597e-07, -4.3140e-07, -3.6514e-07, -5.2319e-08,  9.0775e-07,\n",
      "         -1.0333e-07, -3.9032e-08, -4.7729e-07, -1.3693e-07, -1.3934e-06,\n",
      "          3.1138e-08, -7.4728e-07,  9.4824e-07,  2.9386e-07, -4.3487e-07,\n",
      "         -2.8674e-07, -1.8179e-07,  2.5966e-07, -3.6998e-07, -3.3385e-07,\n",
      "         -3.2371e-08,  1.6698e-07, -2.2341e-08, -1.4896e-07, -1.2777e-07,\n",
      "         -2.5322e-07, -1.7516e-07, -1.5125e-06,  9.0376e-08,  2.0889e-07,\n",
      "          2.8937e-07, -9.5410e-08, -5.2081e-07,  1.0688e-06, -1.3394e-07,\n",
      "          9.9263e-08,  1.5837e-08, -3.5142e-08,  6.8391e-07, -8.0672e-07,\n",
      "          4.7038e-12, -6.3132e-08, -3.3527e-08,  4.9991e-09, -2.8122e-07,\n",
      "         -9.0300e-09, -5.1483e-07,  3.8649e-08,  2.4246e-07,  4.5601e-07,\n",
      "          7.3089e-07,  1.7938e-07,  4.9426e-07, -1.8807e-07, -3.8302e-07,\n",
      "          8.8606e-08,  5.0537e-07, -1.1531e-07,  4.3709e-07],\n",
      "        [-4.1793e-07, -3.1244e-07,  7.9618e-07,  2.3289e-01, -2.1828e-02,\n",
      "          1.4361e-07, -4.1867e-08, -8.5963e-07,  3.0809e-01,  3.8955e-07,\n",
      "          1.7538e-07, -5.4518e-02, -1.1653e-01, -3.7328e-06, -1.1392e-08,\n",
      "         -8.2656e-02, -1.3576e-07,  2.3992e-08, -6.9654e-07,  1.2590e-07,\n",
      "          7.7441e-02,  1.0533e-06,  1.1055e-01,  3.2443e-08, -2.4948e-09,\n",
      "          2.7098e-01, -8.4883e-08,  3.0464e-06,  1.9197e-01, -2.5552e-06,\n",
      "         -6.2507e-07, -4.2135e-07,  4.2191e-06, -1.9573e-06,  1.1111e-06,\n",
      "         -5.2829e-07,  1.6331e-07, -8.0758e-07,  3.2022e-02, -1.0751e-07,\n",
      "         -1.2129e-06, -2.0144e-06,  2.6939e-01, -7.6249e-02, -5.8607e-02,\n",
      "          2.8726e-06,  1.5366e-02, -4.1450e-03,  1.9188e-01, -5.6716e-08,\n",
      "          2.1921e-07, -8.2825e-07, -9.9124e-07,  2.4917e-03, -2.7645e-07,\n",
      "         -9.0403e-02,  7.1984e-02,  2.6640e-01, -1.3646e-06, -1.0212e-06,\n",
      "         -3.2553e-02, -6.8834e-07, -3.1982e-08,  1.6965e-06],\n",
      "        [-1.7720e-05,  2.6137e-01, -4.8217e-07, -1.1919e-01,  2.8092e-02,\n",
      "          1.4149e-01, -5.8735e-07, -9.1044e-02, -2.3537e-07, -1.0038e-01,\n",
      "         -1.0835e-02,  1.8883e-01, -7.7889e-02, -1.2260e-06, -3.6225e-08,\n",
      "         -7.0412e-02,  9.8350e-02, -9.7050e-07,  3.8784e-01, -4.9854e-02,\n",
      "         -3.8914e-07, -3.3319e-08, -1.1366e-06, -6.5224e-03,  1.7591e-07,\n",
      "         -1.8792e-06,  8.0849e-07, -1.1160e-06, -5.8023e-07, -5.3614e-08,\n",
      "          3.8537e-01,  3.7255e-01, -4.6584e-07, -2.4534e-02,  2.5948e-07,\n",
      "          1.0686e-07, -5.8510e-07,  6.1580e-04, -2.6107e-06, -9.8989e-08,\n",
      "         -1.4495e-01, -6.1024e-02,  1.8330e-07,  4.7076e-07,  2.2607e-01,\n",
      "         -1.3525e-06, -6.3726e-02, -1.0669e-07,  4.4368e-02, -2.7622e-08,\n",
      "          2.9419e-01,  1.5747e-01,  1.0190e-01, -7.1541e-02,  7.9059e-02,\n",
      "         -7.1231e-07, -5.8223e-07, -6.3320e-02, -3.7137e-02, -2.6639e-06,\n",
      "          3.1187e-01,  2.2212e-01,  5.7195e-02, -2.4365e-02],\n",
      "        [ 8.7210e-02,  1.6389e-01,  1.4473e-01,  2.3412e-01,  2.0615e-03,\n",
      "         -1.0110e-08,  3.3817e-01,  2.5694e-02, -7.1734e-06,  3.3053e-01,\n",
      "          7.6335e-03,  3.1275e-02,  3.0002e-02,  2.6162e-02,  4.2526e-03,\n",
      "          3.2324e-01,  2.0607e-01,  3.6934e-01,  1.1877e-01,  3.3395e-01,\n",
      "         -5.5265e-02,  2.5312e-01,  2.1816e-01,  1.3162e-02,  3.9513e-01,\n",
      "          2.7520e-01,  3.7979e-01,  4.7763e-03, -4.1892e-02,  1.7972e-01,\n",
      "          1.2626e-01,  1.3026e-03,  1.6860e-02,  1.9372e-01,  1.7455e-02,\n",
      "          1.4969e-01,  4.9195e-01,  4.3975e-02, -7.2384e-02,  3.8130e-01,\n",
      "          2.6528e-05,  2.7265e-02,  1.4181e-01,  3.8831e-05,  8.8418e-05,\n",
      "          3.9579e-02, -5.6158e-02, -2.1170e-02, -1.9824e-01,  1.8402e-01,\n",
      "         -3.4262e-02,  1.3445e-01,  6.0128e-03,  3.3662e-02, -2.4358e-02,\n",
      "          3.0451e-01,  1.4919e-03, -3.3179e-02,  5.0007e-01,  9.2303e-02,\n",
      "         -2.4247e-02,  1.7500e-06,  1.7889e-01,  4.2240e-01]])), ('classifiers.9.bias', tensor([-2.0550e-07,  3.0913e-08,  8.9216e-07, -9.8887e-02, -1.0223e-01,\n",
      "        -4.8996e-06, -9.5964e-02, -6.3121e-07,  8.4520e-07,  1.9316e-01]))]), 'model_architecture': {'input_size': 22, 'num_algorithms': 10, 'num_ranks': 10, 'hidden_sizes': [88, 256, 128, 64], 'model_class': 'RankingMLP'}, 'preprocessing': {'scaler_mean': array([ 5.80000000e+01,  7.21052632e+00,  3.99167638e+00,  1.95593160e+00,\n",
      "        1.46810511e-01,  7.07339306e+04,  1.05529648e+04,  1.05918766e+00,\n",
      "        2.72536967e+00,  1.00245208e+00,  5.16540261e-01,  2.46628335e-01,\n",
      "        6.11564143e-01, -1.49511210e-01,  1.12758686e-01,  1.12758686e+09,\n",
      "        1.85056499e-01,  1.85056499e+09, -7.22978129e-02,  1.00000000e+00,\n",
      "        1.05263158e-01,  1.57894737e-01]), 'scaler_scale': array([2.37841168e+01, 4.96915695e+00, 4.25195729e-01, 5.34917278e-01,\n",
      "       1.00050043e-01, 2.17753055e+05, 2.87264465e+04, 1.31226724e+00,\n",
      "       6.11179647e+00, 9.19216288e-01, 2.60965127e+00, 1.62308969e-01,\n",
      "       3.14792135e-01, 3.79162971e-01, 3.80346092e-01, 3.80346092e+09,\n",
      "       7.73886074e-01, 7.73886074e+09, 6.89090173e-01, 1.00000000e+00,\n",
      "       3.06892205e-01, 3.64642275e-01]), 'feature_names': ['n_samples', 'n_features', 'log_n_samples', 'log_n_features', 'feature_ratio', 'target_mean', 'target_std', 'target_skew', 'target_kurtosis', 'mean_feature_skew', 'mean_feature_kurtosis', 'mean_abs_corr', 'max_abs_corr', 'mean_predictor_r2', 'decision_stump_r2', 'stump_vs_mean_r2_ratio', 'simple_rule_r2', 'rule_vs_mean_r2_ratio', 'tree_advantage', 'tabpfn_suitable', 'svm_suitable', 'linear_favorable']}, 'algorithm_info': {'algorithm_names': ['BayesianRidge', 'LinearRegression', 'SVR', 'RandomForestRegressor', 'GradientBoostingRegressor', 'LGBMRegressor', 'HistGradientBoostingRegressor', 'XGBRegressor', 'DecisionTreeRegressor', 'MLPRegressor'], 'rank_columns': ['BayesianRidge_rank', 'LinearRegression_rank', 'SVR_rank', 'RandomForestRegressor_rank', 'GradientBoostingRegressor_rank', 'LGBMRegressor_rank', 'HistGradientBoostingRegressor_rank', 'XGBRegressor_rank', 'DecisionTreeRegressor_rank', 'MLPRegressor_rank']}, 'training_info': {'train_loss': 0.05980979651212692, 'test_loss': 0.05980979651212692, 'train_spearman_corr': 0.9999999999999998, 'test_spearman_corr': 0.9999999999999998, 'num_epochs': 779, 'best_loss': 1.4153243899345398}, 'metadata': {'num_datasets': 19, 'num_features': 22, 'num_algorithms': 10}}\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcbda6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (4329, 11)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (4329,)\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 30, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 8.306491\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\amlexam\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\miniconda3\\envs\\amlexam\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmeta_features\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_meta_features\n\u001b[32m      4\u001b[39m meta_features = extract_meta_features(X, y)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m prediction = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m(meta_features)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(prediction)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "from meta_features import extract_meta_features\n",
    "\n",
    "\n",
    "meta_features = extract_meta_features(X, y)\n",
    "prediction = model.predict(meta_features)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acada25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGBoost ranking model:\n",
      "  Model type: XGBoost\n",
      "  Number of algorithms: 10\n",
      "  Number of features: 22\n",
      "  Test Spearman correlation: 1.0000\n",
      "  Test Top-1 accuracy: 1.0000\n",
      "  Number of datasets: 19\n",
      "=========== Extracting meta-features ===========\n",
      " X Type: <class 'pandas.core.frame.DataFrame'> Shape: (4329, 11)\n",
      " y Type: <class 'pandas.core.series.Series'> Shape: (4329,)\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 30, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 8.257214\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\amlexam\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\miniconda3\\envs\\amlexam\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m models, checkpoint = load_ranking_meta_model(\u001b[33m\"\u001b[39m\u001b[33mmeta_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m meta_df = extract_meta_features(X,y)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m predictions = \u001b[43mpredict_algorithm_rankings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmeta_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balaj\\Desktop\\AutoML\\automl-tabular-pipeline\\src\\automl\\meta_trainer.py:324\u001b[39m, in \u001b[36mpredict_algorithm_rankings\u001b[39m\u001b[34m(models, checkpoint, meta_features_df, actual_rankings_df)\u001b[39m\n\u001b[32m    321\u001b[39m feature_names = checkpoint[\u001b[33m'\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mfeature_names\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# Ensure features are in correct order and handle missing values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m X = \u001b[43mmeta_features_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m]\u001b[49m.fillna(pd.Series(scaler_mean, index=feature_names))\n\u001b[32m    325\u001b[39m X_scaled = (X.values - scaler_mean) / scaler_scale\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Predict rankings for each algorithm\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from meta_trainer import load_ranking_meta_model, predict_algorithm_rankings\n",
    "models, checkpoint = load_ranking_meta_model(\"meta_model.pth\")\n",
    "\n",
    "meta_df = extract_meta_features(X,y)\n",
    "\n",
    "predictions = predict_algorithm_rankings(models, checkpoint,meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b73ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4858957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 algorithms: ['BayesianRidge', 'LinearRegression', 'SVR', 'RandomForestRegressor', 'GradientBoostingRegressor', 'LGBMRegressor', 'HistGradientBoostingRegressor', 'XGBRegressor', 'DecisionTreeRegressor', 'MLPRegressor']\n",
      "Analyzing 200 records\n",
      "\n",
      "Sample Results:\n",
      "\n",
      "Record 521:\n",
      "  Top 4 Actual:    ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 4 Predicted: ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 1 Match: \n",
      "  Top 4 Overlap: 4/4 (100%)\n",
      "\n",
      "Record 737:\n",
      "  Top 4 Actual:    ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 4 Predicted: ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 1 Match: \n",
      "  Top 4 Overlap: 4/4 (100%)\n",
      "\n",
      "Record 740:\n",
      "  Top 4 Actual:    ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 4 Predicted: ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 1 Match: \n",
      "  Top 4 Overlap: 4/4 (100%)\n",
      "\n",
      "Record 660:\n",
      "  Top 4 Actual:    ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 4 Predicted: ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 1 Match: \n",
      "  Top 4 Overlap: 4/4 (100%)\n",
      "\n",
      "Record 411:\n",
      "  Top 4 Actual:    ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 4 Predicted: ['LGBMRegressor', 'XGBRegressor', 'RandomForestRegressor', 'DecisionTreeRegressor']\n",
      "  Top 1 Match: \n",
      "  Top 4 Overlap: 4/4 (100%)\n",
      "\n",
      "==================================================\n",
      "SUMMARY STATISTICS\n",
      "==================================================\n",
      "Top 1 Accuracy: 0.955 (95.5%)\n",
      "Average Top 4 Overlap: 3.85/4 (96.2%)\n",
      "Perfect Top 4 Match: 184/200 records\n",
      "At least 3/4 match: 189/200 records\n",
      "\n",
      "Detailed analysis saved to: top4_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# Parse meta_preds.csv to analyze top 4 algorithm rankings\n",
    "import pandas as pd\n",
    "\n",
    "# Load the predictions CSV\n",
    "preds_df = pd.read_csv(\"meta_preds.csv\", index_col=0)\n",
    "\n",
    "# Get algorithm names by finding unique prefixes of ranking columns\n",
    "actual_cols = [col for col in preds_df.columns if col.endswith('_actual_rank')]\n",
    "predicted_cols = [col for col in preds_df.columns if col.endswith('_predicted_rank')]\n",
    "\n",
    "# Extract algorithm names\n",
    "algo_names = [col.replace('_actual_rank', '') for col in actual_cols]\n",
    "\n",
    "print(f\"Found {len(algo_names)} algorithms: {algo_names}\")\n",
    "print(f\"Analyzing {len(preds_df)} records\\n\")\n",
    "\n",
    "# Function to get top K algorithms from rankings\n",
    "def get_top_k_algorithms(row, rank_suffix, k=4):\n",
    "    \"\"\"Get top K algorithms based on their rankings (1=best, 2=second best, etc.)\"\"\"\n",
    "    algo_ranks = {}\n",
    "    for algo in algo_names:\n",
    "        col_name = f\"{algo}{rank_suffix}\"\n",
    "        if col_name in row:\n",
    "            algo_ranks[algo] = row[col_name]\n",
    "    \n",
    "    # Sort by rank (ascending: 1, 2, 3, 4...)\n",
    "    sorted_algos = sorted(algo_ranks.items(), key=lambda x: x[1])\n",
    "    return [algo for algo, rank in sorted_algos[:k]]\n",
    "\n",
    "# Analyze each record\n",
    "results = []\n",
    "for idx, row in preds_df.iterrows():\n",
    "    top4_actual = get_top_k_algorithms(row, '_actual_rank', k=4)\n",
    "    top4_predicted = get_top_k_algorithms(row, '_predicted_rank', k=4)\n",
    "    \n",
    "    # Calculate overlap (how many algorithms appear in both top 4 lists)\n",
    "    overlap = len(set(top4_actual).intersection(set(top4_predicted)))\n",
    "    \n",
    "    # Check if top 1 matches\n",
    "    top1_match = top4_actual[0] == top4_predicted[0] if top4_actual and top4_predicted else False\n",
    "    \n",
    "    results.append({\n",
    "        'record': idx,\n",
    "        'top4_actual': top4_actual,\n",
    "        'top4_predicted': top4_predicted,\n",
    "        'top1_actual': top4_actual[0] if top4_actual else None,\n",
    "        'top1_predicted': top4_predicted[0] if top4_predicted else None,\n",
    "        'top1_match': top1_match,\n",
    "        'top4_overlap': overlap,\n",
    "        'top4_overlap_pct': (overlap / 4) * 100\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display first few records\n",
    "print(\"Sample Results:\")\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"\\nRecord {row['record']}:\")\n",
    "    print(f\"  Top 4 Actual:    {row['top4_actual']}\")\n",
    "    print(f\"  Top 4 Predicted: {row['top4_predicted']}\")\n",
    "    print(f\"  Top 1 Match: {'' if row['top1_match'] else ''}\")\n",
    "    print(f\"  Top 4 Overlap: {row['top4_overlap']}/4 ({row['top4_overlap_pct']:.0f}%)\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Top 1 Accuracy: {results_df['top1_match'].mean():.3f} ({results_df['top1_match'].mean()*100:.1f}%)\")\n",
    "print(f\"Average Top 4 Overlap: {results_df['top4_overlap'].mean():.2f}/4 ({results_df['top4_overlap_pct'].mean():.1f}%)\")\n",
    "print(f\"Perfect Top 4 Match: {(results_df['top4_overlap'] == 4).sum()}/{len(results_df)} records\")\n",
    "print(f\"At least 3/4 match: {(results_df['top4_overlap'] >= 3).sum()}/{len(results_df)} records\")\n",
    "\n",
    "# Save detailed results\n",
    "results_df.to_csv(\"top4_analysis.csv\", index=False)\n",
    "print(f\"\\nDetailed analysis saved to: top4_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341f3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85430411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 algorithms: ['BayesianRidge', 'LinearRegression', 'SVR', 'RandomForestRegressor', 'GradientBoostingRegressor', 'LGBMRegressor', 'HistGradientBoostingRegressor', 'XGBRegressor', 'DecisionTreeRegressor', 'MLPRegressor']\n",
      "Analyzing 200 records\n",
      "\n",
      "EXACT TOP 4 ORDER ANALYSIS\n",
      "==================================================\n",
      "Total records: 200\n",
      "Exact top 4 matches: 184\n",
      "Mismatched records: 16\n",
      "Exact match rate: 92.0%\n",
      "\n",
      "============================================================\n",
      "RECORDS WITH MISMATCHED TOP 4 ORDER\n",
      "============================================================\n",
      "\n",
      "Record 76 (Mismatch #1):\n",
      "  Actual Top 4:    1. RandomForestRegressor  2. BayesianRidge  3. GradientBoostingRegressor  4. XGBRegressor\n",
      "  Predicted Top 4: 1. LinearRegression  2. RandomForestRegressor  3. BayesianRidge  4. LGBMRegressor\n",
      "  Differences:     Pos 1: RandomForestRegressor  LinearRegression | Pos 2: BayesianRidge  RandomForestRegressor | Pos 3: GradientBoostingRegressor  BayesianRidge | Pos 4: XGBRegressor  LGBMRegressor\n",
      "\n",
      "Record 59 (Mismatch #2):\n",
      "  Actual Top 4:    1. LGBMRegressor  2. BayesianRidge  3. LinearRegression  4. GradientBoostingRegressor\n",
      "  Predicted Top 4: 1. LGBMRegressor  2. XGBRegressor  3. RandomForestRegressor  4. DecisionTreeRegressor\n",
      "  Differences:     Pos 2: BayesianRidge  XGBRegressor | Pos 3: LinearRegression  RandomForestRegressor | Pos 4: GradientBoostingRegressor  DecisionTreeRegressor\n",
      "\n",
      "Record 23 (Mismatch #3):\n",
      "  Actual Top 4:    1. RandomForestRegressor  2. LGBMRegressor  3. HistGradientBoostingRegressor  4. GradientBoostingRegressor\n",
      "  Predicted Top 4: 1. BayesianRidge  2. LinearRegression  3. HistGradientBoostingRegressor  4. LGBMRegressor\n",
      "  Differences:     Pos 1: RandomForestRegressor  BayesianRidge | Pos 2: LGBMRegressor  LinearRegression | Pos 4: GradientBoostingRegressor  LGBMRegressor\n",
      "\n",
      "Record 30 (Mismatch #4):\n",
      "  Actual Top 4:    1. RandomForestRegressor  2. LinearRegression  3. SVR  4. GradientBoostingRegressor\n",
      "  Predicted Top 4: 1. GradientBoostingRegressor  2. RandomForestRegressor  3. XGBRegressor  4. DecisionTreeRegressor\n",
      "  Differences:     Pos 1: RandomForestRegressor  GradientBoostingRegressor | Pos 2: LinearRegression  RandomForestRegressor | Pos 3: SVR  XGBRegressor | Pos 4: GradientBoostingRegressor  DecisionTreeRegressor\n",
      "\n",
      "Record 10 (Mismatch #5):\n",
      "  Actual Top 4:    1. LinearRegression  2. BayesianRidge  3. HistGradientBoostingRegressor  4. LGBMRegressor\n",
      "  Predicted Top 4: 1. RandomForestRegressor  2. LGBMRegressor  3. HistGradientBoostingRegressor  4. XGBRegressor\n",
      "  Differences:     Pos 1: LinearRegression  RandomForestRegressor | Pos 2: BayesianRidge  LGBMRegressor | Pos 4: LGBMRegressor  XGBRegressor\n",
      "\n",
      "Record 54 (Mismatch #6):\n",
      "  Actual Top 4:    1. RandomForestRegressor  2. GradientBoostingRegressor  3. LinearRegression  4. MLPRegressor\n",
      "  Predicted Top 4: 1. LGBMRegressor  2. HistGradientBoostingRegressor  3. BayesianRidge  4. LinearRegression\n",
      "  Differences:     Pos 1: RandomForestRegressor  LGBMRegressor | Pos 2: GradientBoostingRegressor  HistGradientBoostingRegressor | Pos 3: LinearRegression  BayesianRidge | Pos 4: MLPRegressor  LinearRegression\n",
      "\n",
      "Record 70 (Mismatch #7):\n",
      "  Actual Top 4:    1. LGBMRegressor  2. XGBRegressor  3. RandomForestRegressor  4. DecisionTreeRegressor\n",
      "  Predicted Top 4: 1. LGBMRegressor  2. RandomForestRegressor  3. XGBRegressor  4. LinearRegression\n",
      "  Differences:     Pos 2: XGBRegressor  RandomForestRegressor | Pos 3: RandomForestRegressor  XGBRegressor | Pos 4: DecisionTreeRegressor  LinearRegression\n",
      "\n",
      "Record 39 (Mismatch #8):\n",
      "  Actual Top 4:    1. RandomForestRegressor  2. GradientBoostingRegressor  3. DecisionTreeRegressor  4. HistGradientBoostingRegressor\n",
      "  Predicted Top 4: 1. RandomForestRegressor  2. LGBMRegressor  3. XGBRegressor  4. HistGradientBoostingRegressor\n",
      "  Differences:     Pos 2: GradientBoostingRegressor  LGBMRegressor | Pos 3: DecisionTreeRegressor  XGBRegressor\n",
      "\n",
      "Record 66 (Mismatch #9):\n",
      "  Actual Top 4:    1. RandomForestRegressor  2. GradientBoostingRegressor  3. XGBRegressor  4. DecisionTreeRegressor\n",
      "  Predicted Top 4: 1. XGBRegressor  2. RandomForestRegressor  3. LGBMRegressor  4. DecisionTreeRegressor\n",
      "  Differences:     Pos 1: RandomForestRegressor  XGBRegressor | Pos 2: GradientBoostingRegressor  RandomForestRegressor | Pos 3: XGBRegressor  LGBMRegressor\n",
      "\n",
      "Record 67 (Mismatch #10):\n",
      "  Actual Top 4:    1. LGBMRegressor  2. XGBRegressor  3. RandomForestRegressor  4. DecisionTreeRegressor\n",
      "  Predicted Top 4: 1. LGBMRegressor  2. RandomForestRegressor  3. HistGradientBoostingRegressor  4. LinearRegression\n",
      "  Differences:     Pos 2: XGBRegressor  RandomForestRegressor | Pos 3: RandomForestRegressor  HistGradientBoostingRegressor | Pos 4: DecisionTreeRegressor  LinearRegression\n",
      "\n",
      "Record 55 (Mismatch #11):\n",
      "  Actual Top 4:    1. LGBMRegressor  2. XGBRegressor  3. RandomForestRegressor  4. DecisionTreeRegressor\n",
      "  Predicted Top 4: 1. LGBMRegressor  2. RandomForestRegressor  3. XGBRegressor  4. LinearRegression\n",
      "  Differences:     Pos 2: XGBRegressor  RandomForestRegressor | Pos 3: RandomForestRegressor  XGBRegressor | Pos 4: DecisionTreeRegressor  LinearRegression\n",
      "\n",
      "Record 25 (Mismatch #12):\n",
      "  Actual Top 4:    1. MLPRegressor  2. LinearRegression  3. GradientBoostingRegressor  4. RandomForestRegressor\n",
      "  Predicted Top 4: 1. LGBMRegressor  2. RandomForestRegressor  3. LinearRegression  4. BayesianRidge\n",
      "  Differences:     Pos 1: MLPRegressor  LGBMRegressor | Pos 2: LinearRegression  RandomForestRegressor | Pos 3: GradientBoostingRegressor  LinearRegression | Pos 4: RandomForestRegressor  BayesianRidge\n",
      "\n",
      "Record 72 (Mismatch #13):\n",
      "  Actual Top 4:    1. SVR  2. MLPRegressor  3. BayesianRidge  4. DecisionTreeRegressor\n",
      "  Predicted Top 4: 1. LinearRegression  2. BayesianRidge  3. RandomForestRegressor  4. LGBMRegressor\n",
      "  Differences:     Pos 1: SVR  LinearRegression | Pos 2: MLPRegressor  BayesianRidge | Pos 3: BayesianRidge  RandomForestRegressor | Pos 4: DecisionTreeRegressor  LGBMRegressor\n",
      "\n",
      "Record 44 (Mismatch #14):\n",
      "  Actual Top 4:    1. LGBMRegressor  2. XGBRegressor  3. RandomForestRegressor  4. DecisionTreeRegressor\n",
      "  Predicted Top 4: 1. LGBMRegressor  2. RandomForestRegressor  3. XGBRegressor  4. HistGradientBoostingRegressor\n",
      "  Differences:     Pos 2: XGBRegressor  RandomForestRegressor | Pos 3: RandomForestRegressor  XGBRegressor | Pos 4: DecisionTreeRegressor  HistGradientBoostingRegressor\n",
      "\n",
      "Record 60 (Mismatch #15):\n",
      "  Actual Top 4:    1. GradientBoostingRegressor  2. HistGradientBoostingRegressor  3. LGBMRegressor  4. MLPRegressor\n",
      "  Predicted Top 4: 1. LGBMRegressor  2. RandomForestRegressor  3. HistGradientBoostingRegressor  4. XGBRegressor\n",
      "  Differences:     Pos 1: GradientBoostingRegressor  LGBMRegressor | Pos 2: HistGradientBoostingRegressor  RandomForestRegressor | Pos 3: LGBMRegressor  HistGradientBoostingRegressor | Pos 4: MLPRegressor  XGBRegressor\n",
      "\n",
      "Record 78 (Mismatch #16):\n",
      "  Actual Top 4:    1. LGBMRegressor  2. HistGradientBoostingRegressor  3. RandomForestRegressor  4. GradientBoostingRegressor\n",
      "  Predicted Top 4: 1. LGBMRegressor  2. HistGradientBoostingRegressor  3. RandomForestRegressor  4. XGBRegressor\n",
      "  Differences:     Pos 4: GradientBoostingRegressor  XGBRegressor\n",
      "\n",
      "Mismatched records saved to: mismatched_top4_records.csv\n"
     ]
    }
   ],
   "source": [
    "# Show only records where top 4 exact order doesn't match\n",
    "import pandas as pd\n",
    "\n",
    "# Load the predictions CSV\n",
    "preds_df = pd.read_csv(\"meta_preds.csv\", index_col=0)\n",
    "\n",
    "# Get algorithm names by finding unique prefixes of ranking columns\n",
    "actual_cols = [col for col in preds_df.columns if col.endswith('_actual_rank')]\n",
    "predicted_cols = [col for col in preds_df.columns if col.endswith('_predicted_rank')]\n",
    "\n",
    "# Extract algorithm names\n",
    "algo_names = [col.replace('_actual_rank', '') for col in actual_cols]\n",
    "\n",
    "print(f\"Found {len(algo_names)} algorithms: {algo_names}\")\n",
    "print(f\"Analyzing {len(preds_df)} records\\n\")\n",
    "\n",
    "# Function to get top K algorithms in exact rank order\n",
    "def get_top_k_algorithms_ordered(row, rank_suffix, k=4):\n",
    "    \"\"\"Get top K algorithms in exact rank order (rank 1, rank 2, etc.)\"\"\"\n",
    "    algo_ranks = {}\n",
    "    for algo in algo_names:\n",
    "        col_name = f\"{algo}{rank_suffix}\"\n",
    "        if col_name in row:\n",
    "            algo_ranks[algo] = row[col_name]\n",
    "    \n",
    "    # Sort by rank (ascending: 1, 2, 3, 4...)\n",
    "    sorted_algos = sorted(algo_ranks.items(), key=lambda x: x[1])\n",
    "    return [algo for algo, rank in sorted_algos[:k]]\n",
    "\n",
    "# Find records with mismatched top 4 exact order\n",
    "mismatched_records = []\n",
    "total_records = 0\n",
    "exact_matches = 0\n",
    "\n",
    "for idx, row in preds_df.iterrows():\n",
    "    total_records += 1\n",
    "    top4_actual = get_top_k_algorithms_ordered(row, '_actual_rank', k=4)\n",
    "    top4_predicted = get_top_k_algorithms_ordered(row, '_predicted_rank', k=4)\n",
    "    \n",
    "    # Check if exact order matches\n",
    "    exact_match = top4_actual == top4_predicted\n",
    "    \n",
    "    if exact_match:\n",
    "        exact_matches += 1\n",
    "    else:\n",
    "        # Store mismatched record\n",
    "        mismatched_records.append({\n",
    "            'record': idx,\n",
    "            'top4_actual': top4_actual,\n",
    "            'top4_predicted': top4_predicted\n",
    "        })\n",
    "\n",
    "print(f\"EXACT TOP 4 ORDER ANALYSIS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Exact top 4 matches: {exact_matches}\")\n",
    "print(f\"Mismatched records: {len(mismatched_records)}\")\n",
    "print(f\"Exact match rate: {(exact_matches/total_records)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RECORDS WITH MISMATCHED TOP 4 ORDER\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if len(mismatched_records) == 0:\n",
    "    print(\" All records have perfect top 4 order matches!\")\n",
    "else:\n",
    "    for i, record in enumerate(mismatched_records):\n",
    "        print(f\"\\nRecord {record['record']} (Mismatch #{i+1}):\")\n",
    "        print(f\"  Actual Top 4:    {'  '.join([f'{j+1}. {algo}' for j, algo in enumerate(record['top4_actual'])])}\")\n",
    "        print(f\"  Predicted Top 4: {'  '.join([f'{j+1}. {algo}' for j, algo in enumerate(record['top4_predicted'])])}\")\n",
    "        \n",
    "        # Show position-wise differences\n",
    "        differences = []\n",
    "        for pos in range(4):\n",
    "            if pos < len(record['top4_actual']) and pos < len(record['top4_predicted']):\n",
    "                if record['top4_actual'][pos] != record['top4_predicted'][pos]:\n",
    "                    differences.append(f\"Pos {pos+1}: {record['top4_actual'][pos]}  {record['top4_predicted'][pos]}\")\n",
    "        \n",
    "        if differences:\n",
    "            print(f\"  Differences:     {' | '.join(differences)}\")\n",
    "\n",
    "# Save mismatched records for further analysis\n",
    "if mismatched_records:\n",
    "    mismatch_df = pd.DataFrame(mismatched_records)\n",
    "    mismatch_df.to_csv(\"mismatched_top4_records.csv\", index=False)\n",
    "    print(f\"\\nMismatched records saved to: mismatched_top4_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cdde8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING EXISTING META-MODEL PREDICTIONS\n",
      "======================================================================\n",
      "QUICK DIVERSITY CHECK:\n",
      " Total predictions: 200\n",
      " Unique algorithms: 6\n",
      " Most frequent: LGBMRegressor (96.0%)\n",
      " DIVERSITY ISSUE: One algorithm dominates!\n",
      "============================================================\n",
      "META-MODEL DIVERSITY EVALUATION\n",
      "============================================================\n",
      "Analyzing 200 existing predictions\n",
      "\n",
      "ALGORITHM RECOMMENDATION FREQUENCY:\n",
      "----------------------------------------\n",
      "LGBMRegressor        | 192 ( 96.0%) \n",
      "LinearRegression     |   3 (  1.5%) \n",
      "RandomForestRegressor |   2 (  1.0%) \n",
      "BayesianRidge        |   1 (  0.5%) \n",
      "GradientBoostingRegressor |   1 (  0.5%) \n",
      "XGBRegressor         |   1 (  0.5%) \n",
      "\n",
      "==================================================\n",
      "DIVERSITY METRICS\n",
      "==================================================\n",
      "Total predictions analyzed: 200\n",
      "Unique algorithms recommended: 6\n",
      "Most frequent algorithm: LGBMRegressor (96.0%)\n",
      "\n",
      "DIVERSITY ASSESSMENT:\n",
      " POOR DIVERSITY: LGBMRegressor dominates 96.0% of predictions\n",
      "    Meta-model may be over-fitted to one algorithm\n",
      "\n",
      "INFORMATION THEORY METRICS:\n",
      "Shannon Entropy: 0.329\n",
      "Normalized Entropy: 0.127 (1.0 = perfectly diverse)\n",
      " Low entropy - Poor diversity\n",
      "\n",
      "============================================================\n",
      "ALGORITHM PATTERN ANALYSIS\n",
      "============================================================\n",
      "No meta-features found in predictions file for pattern analysis\n",
      "\n",
      "PREDICTION ACCURACY:\n",
      "Correct predictions: 191/200 (95.5%)\n",
      "\n",
      "MOST ACCURATELY PREDICTED ALGORITHMS:\n",
      "  LGBMRegressor: 189/192 (98.4%)\n",
      "  RandomForestRegressor: 1/2 (50.0%)\n",
      "  LinearRegression: 1/3 (33.3%)\n",
      "\n",
      "==================================================\n",
      "RECOMMENDATIONS\n",
      "==================================================\n",
      " ACTIONS NEEDED:\n",
      "  1. Check training data balance - ensure diverse algorithm performance\n",
      "  2. Adjust loss function to penalize over-concentration\n",
      "  3. Use dataset stratification during meta-model training\n",
      "  4. Consider ensemble of specialized meta-models\n",
      "\n",
      "Evaluation complete! Results based on 200 predictions.\n"
     ]
    }
   ],
   "source": [
    "# Meta-Model Diversity Evaluation using existing predictions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_existing_predictions():\n",
    "    \"\"\"Evaluate meta-model diversity using existing meta_preds.csv\"\"\"\n",
    "    \n",
    "    # Load existing predictions\n",
    "    preds_df = pd.read_csv(\"meta_preds.csv\", index_col=0)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"META-MODEL DIVERSITY EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Analyzing {len(preds_df)} existing predictions\\n\")\n",
    "    \n",
    "    # Extract recommended algorithms\n",
    "    if 'recommended_algorithm' in preds_df.columns:\n",
    "        recommendations = preds_df['recommended_algorithm'].tolist()\n",
    "    else:\n",
    "        # If no recommended_algorithm column, extract from rank 1 predictions\n",
    "        algo_names = [col.replace('_predicted_rank', '') for col in preds_df.columns if col.endswith('_predicted_rank')]\n",
    "        recommendations = []\n",
    "        \n",
    "        for idx, row in preds_df.iterrows():\n",
    "            # Find algorithm with rank 1 (best prediction)\n",
    "            best_algo = None\n",
    "            for algo in algo_names:\n",
    "                if f\"{algo}_predicted_rank\" in row and row[f\"{algo}_predicted_rank\"] == 1:\n",
    "                    best_algo = algo\n",
    "                    break\n",
    "            recommendations.append(best_algo)\n",
    "    \n",
    "    # Count algorithm frequency\n",
    "    algo_frequency = Counter(recommendations)\n",
    "    total_predictions = len(recommendations)\n",
    "    unique_algorithms = len(algo_frequency)\n",
    "    \n",
    "    print(\"ALGORITHM RECOMMENDATION FREQUENCY:\")\n",
    "    print(\"-\" * 40)\n",
    "    for algo, count in sorted(algo_frequency.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total_predictions) * 100\n",
    "        bar = \"\" * int(percentage / 2)  # Simple bar chart\n",
    "        print(f\"{algo:20} | {count:3d} ({percentage:5.1f}%) {bar}\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"DIVERSITY METRICS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    diversity_ratio = unique_algorithms / len(set([algo for algo in algo_frequency.keys() if algo is not None]))\n",
    "    most_frequent = algo_frequency.most_common(1)[0] if algo_frequency else (None, 0)\n",
    "    concentration = most_frequent[1] / total_predictions if most_frequent[1] > 0 else 0\n",
    "    \n",
    "    print(f\"Total predictions analyzed: {total_predictions}\")\n",
    "    print(f\"Unique algorithms recommended: {unique_algorithms}\")\n",
    "    print(f\"Most frequent algorithm: {most_frequent[0]} ({concentration*100:.1f}%)\")\n",
    "    \n",
    "    # Diversity assessment\n",
    "    print(f\"\\nDIVERSITY ASSESSMENT:\")\n",
    "    if concentration > 0.7:\n",
    "        print(f\" POOR DIVERSITY: {most_frequent[0]} dominates {concentration*100:.1f}% of predictions\")\n",
    "        print(\"    Meta-model may be over-fitted to one algorithm\")\n",
    "        assessment = \"POOR\"\n",
    "    elif concentration > 0.5:\n",
    "        print(f\"  MODERATE DIVERSITY: {most_frequent[0]} used in {concentration*100:.1f}% of cases\")\n",
    "        print(\"    Some algorithm variety, but room for improvement\")\n",
    "        assessment = \"MODERATE\"\n",
    "    elif unique_algorithms < 3:\n",
    "        print(f\"  LIMITED VARIETY: Only {unique_algorithms} algorithms recommended\")\n",
    "        print(\"    Meta-model not utilizing full algorithm portfolio\")\n",
    "        assessment = \"LIMITED\"\n",
    "    else:\n",
    "        print(f\" GOOD DIVERSITY: {unique_algorithms} algorithms used, max concentration {concentration*100:.1f}%\")\n",
    "        print(\"    Meta-model shows dataset-specific behavior\")\n",
    "        assessment = \"GOOD\"\n",
    "    \n",
    "    # Entropy calculation (information theory measure of diversity)\n",
    "    probabilities = [count/total_predictions for count in algo_frequency.values()]\n",
    "    entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "    max_entropy = np.log2(len(algo_frequency))\n",
    "    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "    \n",
    "    print(f\"\\nINFORMATION THEORY METRICS:\")\n",
    "    print(f\"Shannon Entropy: {entropy:.3f}\")\n",
    "    print(f\"Normalized Entropy: {normalized_entropy:.3f} (1.0 = perfectly diverse)\")\n",
    "    \n",
    "    if normalized_entropy > 0.8:\n",
    "        print(\" High entropy - Very diverse predictions\")\n",
    "    elif normalized_entropy > 0.6:\n",
    "        print(\"  Moderate entropy - Some diversity\")\n",
    "    else:\n",
    "        print(\" Low entropy - Poor diversity\")\n",
    "    \n",
    "    return {\n",
    "        'assessment': assessment,\n",
    "        'concentration': concentration,\n",
    "        'entropy': normalized_entropy,\n",
    "        'unique_algorithms': unique_algorithms,\n",
    "        'most_frequent': most_frequent[0],\n",
    "        'frequency_distribution': dict(algo_frequency)\n",
    "    }\n",
    "\n",
    "def analyze_algorithm_patterns():\n",
    "    \"\"\"Analyze which algorithms are predicted for what types of datasets\"\"\"\n",
    "    \n",
    "    preds_df = pd.read_csv(\"meta_preds.csv\", index_col=0)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALGORITHM PATTERN ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # If meta-features are available in the predictions file\n",
    "    meta_feature_cols = [col for col in preds_df.columns if not col.endswith('_rank') and \n",
    "                        col != 'recommended_algorithm' and col != 'actual_best_algorithm']\n",
    "    \n",
    "    if len(meta_feature_cols) > 0:\n",
    "        print(f\"Found {len(meta_feature_cols)} meta-features for pattern analysis\")\n",
    "        \n",
    "        # Group by recommended algorithm\n",
    "        if 'recommended_algorithm' in preds_df.columns:\n",
    "            algo_groups = preds_df.groupby('recommended_algorithm')\n",
    "            \n",
    "            print(f\"\\nPATTERN ANALYSIS BY ALGORITHM:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for algo, group in algo_groups:\n",
    "                if len(group) < 2:  # Skip algorithms with only 1 prediction\n",
    "                    continue\n",
    "                    \n",
    "                print(f\"\\n{algo} ({len(group)} datasets):\")\n",
    "                \n",
    "                # Analyze key meta-features\n",
    "                key_features = ['n_samples', 'n_features', 'feature_ratio', 'target_mean', 'target_std']\n",
    "                available_features = [f for f in key_features if f in meta_feature_cols]\n",
    "                \n",
    "                for feature in available_features:\n",
    "                    if feature in group.columns:\n",
    "                        mean_val = group[feature].mean()\n",
    "                        std_val = group[feature].std()\n",
    "                        print(f\"  {feature}: {mean_val:.3f}  {std_val:.3f}\")\n",
    "    else:\n",
    "        print(\"No meta-features found in predictions file for pattern analysis\")\n",
    "    \n",
    "    # Analyze actual vs predicted performance correlation\n",
    "    if 'actual_best_algorithm' in preds_df.columns and 'recommended_algorithm' in preds_df.columns:\n",
    "        correct_predictions = (preds_df['actual_best_algorithm'] == preds_df['recommended_algorithm']).sum()\n",
    "        total_predictions = len(preds_df)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        print(f\"\\nPREDICTION ACCURACY:\")\n",
    "        print(f\"Correct predictions: {correct_predictions}/{total_predictions} ({accuracy*100:.1f}%)\")\n",
    "        \n",
    "        # Analyze which algorithms are correctly predicted most often\n",
    "        correct_mask = preds_df['actual_best_algorithm'] == preds_df['recommended_algorithm']\n",
    "        correct_algos = preds_df[correct_mask]['recommended_algorithm'].value_counts()\n",
    "        \n",
    "        print(f\"\\nMOST ACCURATELY PREDICTED ALGORITHMS:\")\n",
    "        for algo, count in correct_algos.head().items():\n",
    "            total_algo_predictions = (preds_df['recommended_algorithm'] == algo).sum()\n",
    "            algo_accuracy = count / total_algo_predictions if total_algo_predictions > 0 else 0\n",
    "            print(f\"  {algo}: {count}/{total_algo_predictions} ({algo_accuracy*100:.1f}%)\")\n",
    "\n",
    "def quick_diversity_check():\n",
    "    \"\"\"Quick one-line diversity check\"\"\"\n",
    "    \n",
    "    preds_df = pd.read_csv(\"meta_preds.csv\", index_col=0)\n",
    "    \n",
    "    if 'recommended_algorithm' in preds_df.columns:\n",
    "        recommendations = preds_df['recommended_algorithm']\n",
    "        algo_counts = recommendations.value_counts()\n",
    "        \n",
    "        print(\"QUICK DIVERSITY CHECK:\")\n",
    "        print(f\" Total predictions: {len(recommendations)}\")\n",
    "        print(f\" Unique algorithms: {len(algo_counts)}\")\n",
    "        print(f\" Most frequent: {algo_counts.index[0]} ({algo_counts.iloc[0]/len(recommendations)*100:.1f}%)\")\n",
    "        \n",
    "        if algo_counts.iloc[0]/len(recommendations) > 0.7:\n",
    "            print(\" DIVERSITY ISSUE: One algorithm dominates!\")\n",
    "        else:\n",
    "            print(\" ACCEPTABLE DIVERSITY\")\n",
    "\n",
    "# Run the evaluation\n",
    "print(\"EVALUATING EXISTING META-MODEL PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Quick check first\n",
    "quick_diversity_check()\n",
    "\n",
    "# Detailed evaluation\n",
    "results = evaluate_existing_predictions()\n",
    "\n",
    "# Pattern analysis\n",
    "analyze_algorithm_patterns()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "if results['assessment'] == 'POOR':\n",
    "    print(\" ACTIONS NEEDED:\")\n",
    "    print(\"  1. Check training data balance - ensure diverse algorithm performance\")\n",
    "    print(\"  2. Adjust loss function to penalize over-concentration\")\n",
    "    print(\"  3. Use dataset stratification during meta-model training\")\n",
    "    print(\"  4. Consider ensemble of specialized meta-models\")\n",
    "\n",
    "elif results['assessment'] == 'MODERATE':\n",
    "    print(\" IMPROVEMENTS SUGGESTED:\")\n",
    "    print(\"  1. Add more diverse training datasets\")\n",
    "    print(\"  2. Fine-tune meta-feature extraction\")\n",
    "    print(\"  3. Consider regularization to prevent algorithm bias\")\n",
    "\n",
    "else:\n",
    "    print(\" GOOD PERFORMANCE:\")\n",
    "    print(\"  Meta-model shows good algorithm diversity\")\n",
    "    print(\"  Continue monitoring on new datasets\")\n",
    "\n",
    "print(f\"\\nEvaluation complete! Results based on {len(pd.read_csv('meta_preds.csv'))} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d969f637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING ALGORITHM DIVERSITY IN META_Y.CSV\n",
      "======================================================================\n",
      "Loaded meta_Y.csv with 1000 records\n",
      "============================================================\n",
      "TOP 4 ALGORITHM DIVERSITY ANALYSIS\n",
      "============================================================\n",
      "Found 10 algorithms: ['BayesianRidge', 'LinearRegression', 'SVR', 'RandomForestRegressor', 'GradientBoostingRegressor', 'LGBMRegressor', 'HistGradientBoostingRegressor', 'XGBRegressor', 'DecisionTreeRegressor', 'MLPRegressor']\n",
      "Analyzing 1000 records\n",
      "\n",
      "1. OVERALL TOP 4 SET DIVERSITY:\n",
      "----------------------------------------\n",
      "Total records: 1000\n",
      "Unique top 4 combinations: 62\n",
      "Diversity ratio: 0.062\n",
      "\n",
      "Most frequent top 4 combinations:\n",
      "   1. 1.LGBMRegressor  2.XGBRegressor  3.RandomForestRegressor  4.DecisionTreeRegressor\n",
      "      Frequency: 932/1000 (93.2%)\n",
      "\n",
      "   2. 1.RandomForestRegressor  2.GradientBoostingRegressor  3.DecisionTreeRegressor  4.HistGradientBoostingRegressor\n",
      "      Frequency: 3/1000 (0.3%)\n",
      "\n",
      "   3. 1.BayesianRidge  2.LinearRegression  3.SVR  4.RandomForestRegressor\n",
      "      Frequency: 2/1000 (0.2%)\n",
      "\n",
      "   4. 1.MLPRegressor  2.RandomForestRegressor  3.LGBMRegressor  4.HistGradientBoostingRegressor\n",
      "      Frequency: 2/1000 (0.2%)\n",
      "\n",
      "   5. 1.RandomForestRegressor  2.LGBMRegressor  3.HistGradientBoostingRegressor  4.GradientBoostingRegressor\n",
      "      Frequency: 2/1000 (0.2%)\n",
      "\n",
      "   6. 1.GradientBoostingRegressor  2.XGBRegressor  3.RandomForestRegressor  4.DecisionTreeRegressor\n",
      "      Frequency: 2/1000 (0.2%)\n",
      "   7. 1.LGBMRegressor  2.HistGradientBoostingRegressor  3.RandomForestRegressor  4.GradientBoostingRegressor\n",
      "      Frequency: 2/1000 (0.2%)\n",
      "   8. 1.RandomForestRegressor  2.XGBRegressor  3.GradientBoostingRegressor  4.LGBMRegressor\n",
      "      Frequency: 1/1000 (0.1%)\n",
      "   9. 1.LGBMRegressor  2.HistGradientBoostingRegressor  3.RandomForestRegressor  4.XGBRegressor\n",
      "      Frequency: 1/1000 (0.1%)\n",
      "  10. 1.BayesianRidge  2.SVR  3.LinearRegression  4.HistGradientBoostingRegressor\n",
      "      Frequency: 1/1000 (0.1%)\n",
      "\n",
      "2. POSITION-SPECIFIC DIVERSITY:\n",
      "----------------------------------------\n",
      "\n",
      "Rank 1 (Best  4th):\n",
      "  Unique algorithms appearing: 10/10\n",
      "    LGBMRegressor        | 938 ( 93.8%) \n",
      "    RandomForestRegressor |  19 (  1.9%) \n",
      "    LinearRegression     |  11 (  1.1%) \n",
      "    XGBRegressor         |   7 (  0.7%) \n",
      "    BayesianRidge        |   6 (  0.6%) \n",
      "     POOR diversity: LGBMRegressor dominates 93.8%\n",
      "\n",
      "Rank 2 (Best  4th):\n",
      "  Unique algorithms appearing: 9/10\n",
      "    XGBRegressor         | 939 ( 93.9%) \n",
      "    GradientBoostingRegressor |  13 (  1.3%) \n",
      "    BayesianRidge        |  12 (  1.2%) \n",
      "    LinearRegression     |  11 (  1.1%) \n",
      "    HistGradientBoostingRegressor |   7 (  0.7%) \n",
      "     POOR diversity: XGBRegressor dominates 93.9%\n",
      "\n",
      "Rank 3 (Best  4th):\n",
      "  Unique algorithms appearing: 10/10\n",
      "    RandomForestRegressor | 944 ( 94.4%) \n",
      "    GradientBoostingRegressor |  10 (  1.0%) \n",
      "    XGBRegressor         |   8 (  0.8%) \n",
      "    LGBMRegressor        |   7 (  0.7%) \n",
      "    DecisionTreeRegressor |   7 (  0.7%) \n",
      "     POOR diversity: RandomForestRegressor dominates 94.4%\n",
      "\n",
      "Rank 4 (Best  4th):\n",
      "  Unique algorithms appearing: 10/10\n",
      "    DecisionTreeRegressor | 939 ( 93.9%) \n",
      "    HistGradientBoostingRegressor |  14 (  1.4%) \n",
      "    RandomForestRegressor |  11 (  1.1%) \n",
      "    GradientBoostingRegressor |   9 (  0.9%) \n",
      "    LGBMRegressor        |   8 (  0.8%) \n",
      "     POOR diversity: DecisionTreeRegressor dominates 93.9%\n",
      "\n",
      "3. ALGORITHM INCLUSION IN TOP 4:\n",
      "----------------------------------------\n",
      "How often each algorithm appears in top 4 (any position):\n",
      "  RandomForestRegressor | 981/1000 ( 98.1%) \n",
      "  XGBRegressor         | 962/1000 ( 96.2%) \n",
      "  LGBMRegressor        | 960/1000 ( 96.0%) \n",
      "  DecisionTreeRegressor | 947/1000 ( 94.7%) \n",
      "  GradientBoostingRegressor |  38/1000 (  3.8%) \n",
      "  BayesianRidge        |  29/1000 (  2.9%) \n",
      "  HistGradientBoostingRegressor |  29/1000 (  2.9%) \n",
      "  LinearRegression     |  28/1000 (  2.8%) \n",
      "  SVR                  |  13/1000 (  1.3%) \n",
      "  MLPRegressor         |  13/1000 (  1.3%) \n",
      "\n",
      "4. ALGORITHM CO-OCCURRENCE IN TOP 4:\n",
      "----------------------------------------\n",
      "Most frequent algorithm pairs in top 4:\n",
      "   1. RandomForestRegressor + XGBRegressor: 957 times (95.7%)\n",
      "   2. LGBMRegressor + RandomForestRegressor: 951 times (95.1%)\n",
      "   3. DecisionTreeRegressor + RandomForestRegressor: 945 times (94.5%)\n",
      "   4. LGBMRegressor + XGBRegressor: 944 times (94.4%)\n",
      "   5. DecisionTreeRegressor + XGBRegressor: 942 times (94.2%)\n",
      "   6. DecisionTreeRegressor + LGBMRegressor: 934 times (93.4%)\n",
      "   7. GradientBoostingRegressor + RandomForestRegressor: 32 times (3.2%)\n",
      "   8. BayesianRidge + LinearRegression: 24 times (2.4%)\n",
      "   9. HistGradientBoostingRegressor + LGBMRegressor: 20 times (2.0%)\n",
      "  10. GradientBoostingRegressor + XGBRegressor: 18 times (1.8%)\n",
      "\n",
      "==================================================\n",
      "DIVERSITY ASSESSMENT\n",
      "==================================================\n",
      "Overall Top 4 Diversity: 0.062\n",
      "Most common combination appears: 93.2% of the time\n",
      " POOR: Low diversity - similar algorithms always in top 4\n",
      "\n",
      "Position-specific diversity:\n",
      "  Rank 1: 10/10 unique algos, LGBMRegressor leads (93.8%)\n",
      "  Rank 2: 9/10 unique algos, XGBRegressor leads (93.9%)\n",
      "  Rank 3: 10/10 unique algos, RandomForestRegressor leads (94.4%)\n",
      "  Rank 4: 10/10 unique algos, DecisionTreeRegressor leads (93.9%)\n",
      "\n",
      "==================================================\n",
      "RECOMMENDATIONS\n",
      "==================================================\n",
      " URGENT ACTIONS NEEDED:\n",
      "  1. Training data lacks diversity - same algorithms always win\n",
      "  2. Add datasets that favor different algorithm combinations\n",
      "  3. Check if meta-features capture enough dataset variety\n",
      "  4. Consider synthetic dataset generation for rare cases\n",
      "\n",
      "  DOMINANCE ISSUES DETECTED:\n",
      "  Rank 1: LGBMRegressor appears 93.8% of the time\n",
      "  Rank 2: XGBRegressor appears 93.9% of the time\n",
      "  Rank 3: RandomForestRegressor appears 94.4% of the time\n",
      "  Rank 4: DecisionTreeRegressor appears 93.9% of the time\n",
      "   Need more diverse datasets to break algorithm dominance\n",
      "\n",
      " QUICK SUMMARY:\n",
      "    Overall diversity: 0.062\n",
      "    Unique top-4 combinations: 62\n",
      "    Most common combo appears: 93.2% of time\n",
      "    Algorithm inclusion range: 13 - 981 times\n"
     ]
    }
   ],
   "source": [
    "# Analyze Algorithm Diversity in Top 4 Ranks from meta_Y.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_top4_diversity():\n",
    "    \"\"\"Analyze diversity of top 4 algorithm rankings in meta_Y.csv\"\"\"\n",
    "    \n",
    "    # Load the meta_Y.csv file\n",
    "    try:\n",
    "        meta_y_df = pd.read_csv(\"meta_Y.csv\", index_col=0)\n",
    "        print(f\"Loaded meta_Y.csv with {len(meta_y_df)} records\")\n",
    "    except FileNotFoundError:\n",
    "        print(\" meta_Y.csv not found!\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"TOP 4 ALGORITHM DIVERSITY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get algorithm names by finding columns ending with '_rank'\n",
    "    rank_columns = [col for col in meta_y_df.columns if col.endswith('_rank')]\n",
    "    algo_names = [col.replace('_rank', '') for col in rank_columns]\n",
    "    \n",
    "    print(f\"Found {len(algo_names)} algorithms: {algo_names}\")\n",
    "    print(f\"Analyzing {len(meta_y_df)} records\\n\")\n",
    "    \n",
    "    # Function to get top K algorithms for each record\n",
    "    def get_top_k_algorithms(row, k=4):\n",
    "        \"\"\"Get top K algorithms based on their rankings (1=best, 2=second best, etc.)\"\"\"\n",
    "        algo_ranks = {}\n",
    "        for algo in algo_names:\n",
    "            col_name = f\"{algo}_rank\"\n",
    "            if col_name in row:\n",
    "                algo_ranks[algo] = row[col_name]\n",
    "        \n",
    "        # Sort by rank (ascending: 1, 2, 3, 4...)\n",
    "        sorted_algos = sorted(algo_ranks.items(), key=lambda x: x[1])\n",
    "        return [algo for algo, rank in sorted_algos[:k]]\n",
    "    \n",
    "    # Extract top 4 for each record\n",
    "    all_top4_sets = []\n",
    "    rank_position_algorithms = {1: [], 2: [], 3: [], 4: []}  # Track which algo appears at each position\n",
    "    \n",
    "    for idx, row in meta_y_df.iterrows():\n",
    "        top4 = get_top_k_algorithms(row, k=4)\n",
    "        all_top4_sets.append(tuple(top4))  # Convert to tuple for hashing\n",
    "        \n",
    "        # Track position-specific algorithms\n",
    "        for pos, algo in enumerate(top4[:4], 1):\n",
    "            rank_position_algorithms[pos].append(algo)\n",
    "    \n",
    "    # 1. OVERALL TOP 4 SET DIVERSITY\n",
    "    print(\"1. OVERALL TOP 4 SET DIVERSITY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    unique_top4_sets = set(all_top4_sets)\n",
    "    set_frequency = Counter(all_top4_sets)\n",
    "    \n",
    "    print(f\"Total records: {len(all_top4_sets)}\")\n",
    "    print(f\"Unique top 4 combinations: {len(unique_top4_sets)}\")\n",
    "    print(f\"Diversity ratio: {len(unique_top4_sets)/len(all_top4_sets):.3f}\")\n",
    "    \n",
    "    # Show most common top 4 combinations\n",
    "    print(f\"\\nMost frequent top 4 combinations:\")\n",
    "    for i, (top4_set, count) in enumerate(set_frequency.most_common(10)):\n",
    "        percentage = (count / len(all_top4_sets)) * 100\n",
    "        top4_str = \"  \".join([f\"{j+1}.{algo}\" for j, algo in enumerate(top4_set)])\n",
    "        print(f\"  {i+1:2d}. {top4_str}\")\n",
    "        print(f\"      Frequency: {count}/{len(all_top4_sets)} ({percentage:.1f}%)\")\n",
    "        if i < 5:  # Show details for top 5\n",
    "            print()\n",
    "    \n",
    "    # 2. POSITION-SPECIFIC DIVERSITY\n",
    "    print(f\"\\n2. POSITION-SPECIFIC DIVERSITY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for position in [1, 2, 3, 4]:\n",
    "        position_algos = rank_position_algorithms[position]\n",
    "        algo_frequency = Counter(position_algos)\n",
    "        unique_algos_at_position = len(algo_frequency)\n",
    "        \n",
    "        print(f\"\\nRank {position} (Best  4th):\")\n",
    "        print(f\"  Unique algorithms appearing: {unique_algos_at_position}/{len(algo_names)}\")\n",
    "        \n",
    "        # Show top algorithms for this position\n",
    "        for algo, count in algo_frequency.most_common(5):\n",
    "            percentage = (count / len(position_algos)) * 100\n",
    "            bar = \"\" * min(int(percentage / 3), 20)  # Visual bar\n",
    "            print(f\"    {algo:20} | {count:3d} ({percentage:5.1f}%) {bar}\")\n",
    "        \n",
    "        # Diversity assessment for this position\n",
    "        most_frequent_pct = (algo_frequency.most_common(1)[0][1] / len(position_algos)) * 100\n",
    "        if most_frequent_pct > 70:\n",
    "            print(f\"     POOR diversity: {algo_frequency.most_common(1)[0][0]} dominates {most_frequent_pct:.1f}%\")\n",
    "        elif most_frequent_pct > 50:\n",
    "            print(f\"      MODERATE diversity: {algo_frequency.most_common(1)[0][0]} appears {most_frequent_pct:.1f}%\")\n",
    "        else:\n",
    "            print(f\"     GOOD diversity: Most frequent is {most_frequent_pct:.1f}%\")\n",
    "    \n",
    "    # 3. ALGORITHM INCLUSION FREQUENCY\n",
    "    print(f\"\\n3. ALGORITHM INCLUSION IN TOP 4:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Count how often each algorithm appears in ANY of the top 4 positions\n",
    "    algo_inclusion_count = Counter()\n",
    "    for top4_set in all_top4_sets:\n",
    "        for algo in top4_set:\n",
    "            algo_inclusion_count[algo] += 1\n",
    "    \n",
    "    print(f\"How often each algorithm appears in top 4 (any position):\")\n",
    "    for algo, count in algo_inclusion_count.most_common():\n",
    "        percentage = (count / len(all_top4_sets)) * 100\n",
    "        bar = \"\" * min(int(percentage / 2), 30)\n",
    "        print(f\"  {algo:20} | {count:3d}/{len(all_top4_sets)} ({percentage:5.1f}%) {bar}\")\n",
    "    \n",
    "    # 4. PAIRWISE ALGORITHM CO-OCCURRENCE\n",
    "    print(f\"\\n4. ALGORITHM CO-OCCURRENCE IN TOP 4:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Find which algorithms frequently appear together in top 4\n",
    "    pair_counts = Counter()\n",
    "    for top4_set in all_top4_sets:\n",
    "        # Get all pairs of algorithms in this top 4\n",
    "        for algo1, algo2 in combinations(top4_set, 2):\n",
    "            pair = tuple(sorted([algo1, algo2]))  # Sort to ensure consistent ordering\n",
    "            pair_counts[pair] += 1\n",
    "    \n",
    "    print(f\"Most frequent algorithm pairs in top 4:\")\n",
    "    for i, (pair, count) in enumerate(pair_counts.most_common(10)):\n",
    "        percentage = (count / len(all_top4_sets)) * 100\n",
    "        print(f\"  {i+1:2d}. {pair[0]} + {pair[1]}: {count} times ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 5. DIVERSITY ASSESSMENT\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"DIVERSITY ASSESSMENT\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    overall_diversity = len(unique_top4_sets) / len(all_top4_sets)\n",
    "    most_common_set_pct = (set_frequency.most_common(1)[0][1] / len(all_top4_sets)) * 100\n",
    "    \n",
    "    print(f\"Overall Top 4 Diversity: {overall_diversity:.3f}\")\n",
    "    print(f\"Most common combination appears: {most_common_set_pct:.1f}% of the time\")\n",
    "    \n",
    "    if overall_diversity > 0.8:\n",
    "        print(\" EXCELLENT: Very high diversity in top 4 combinations\")\n",
    "    elif overall_diversity > 0.5:\n",
    "        print(\" GOOD: Good diversity in top 4 combinations\")\n",
    "    elif overall_diversity > 0.3:\n",
    "        print(\"  MODERATE: Some diversity, but room for improvement\")\n",
    "    else:\n",
    "        print(\" POOR: Low diversity - similar algorithms always in top 4\")\n",
    "    \n",
    "    # Position-specific assessment\n",
    "    print(f\"\\nPosition-specific diversity:\")\n",
    "    for pos in [1, 2, 3, 4]:\n",
    "        pos_algos = rank_position_algorithms[pos]\n",
    "        pos_diversity = len(set(pos_algos)) / len(algo_names)\n",
    "        most_frequent_at_pos = Counter(pos_algos).most_common(1)[0]\n",
    "        dominance_pct = (most_frequent_at_pos[1] / len(pos_algos)) * 100\n",
    "        \n",
    "        print(f\"  Rank {pos}: {len(set(pos_algos))}/{len(algo_names)} unique algos, {most_frequent_at_pos[0]} leads ({dominance_pct:.1f}%)\")\n",
    "    \n",
    "    # 6. RECOMMENDATIONS\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if overall_diversity < 0.3:\n",
    "        print(\" URGENT ACTIONS NEEDED:\")\n",
    "        print(\"  1. Training data lacks diversity - same algorithms always win\")\n",
    "        print(\"  2. Add datasets that favor different algorithm combinations\")\n",
    "        print(\"  3. Check if meta-features capture enough dataset variety\")\n",
    "        print(\"  4. Consider synthetic dataset generation for rare cases\")\n",
    "    elif overall_diversity < 0.5:\n",
    "        print(\" IMPROVEMENTS SUGGESTED:\")\n",
    "        print(\"  1. Increase dataset variety in training\")\n",
    "        print(\"  2. Ensure balanced algorithm performance across datasets\")\n",
    "        print(\"  3. Check for data collection bias\")\n",
    "    else:\n",
    "        print(\" GOOD DIVERSITY:\")\n",
    "        print(\"  Your meta-learning data shows good algorithm variety\")\n",
    "        print(\"  Continue monitoring for edge cases\")\n",
    "    \n",
    "    # Check for specific algorithm dominance issues\n",
    "    dominant_algos = []\n",
    "    for pos in [1, 2, 3, 4]:\n",
    "        pos_counter = Counter(rank_position_algorithms[pos])\n",
    "        most_freq = pos_counter.most_common(1)[0]\n",
    "        if (most_freq[1] / len(rank_position_algorithms[pos])) > 0.6:\n",
    "            dominant_algos.append((pos, most_freq[0], most_freq[1] / len(rank_position_algorithms[pos]) * 100))\n",
    "    \n",
    "    if dominant_algos:\n",
    "        print(f\"\\n  DOMINANCE ISSUES DETECTED:\")\n",
    "        for pos, algo, pct in dominant_algos:\n",
    "            print(f\"  Rank {pos}: {algo} appears {pct:.1f}% of the time\")\n",
    "        print(\"   Need more diverse datasets to break algorithm dominance\")\n",
    "    \n",
    "    return {\n",
    "        'overall_diversity': overall_diversity,\n",
    "        'unique_combinations': len(unique_top4_sets),\n",
    "        'most_common_percentage': most_common_set_pct,\n",
    "        'algorithm_inclusion': dict(algo_inclusion_count),\n",
    "        'position_diversity': {pos: len(set(rank_position_algorithms[pos])) for pos in [1,2,3,4]}\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "print(\"ANALYZING ALGORITHM DIVERSITY IN META_Y.CSV\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = analyze_top4_diversity()\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n QUICK SUMMARY:\")\n",
    "    print(f\"    Overall diversity: {results['overall_diversity']:.3f}\")\n",
    "    print(f\"    Unique top-4 combinations: {results['unique_combinations']}\")\n",
    "    print(f\"    Most common combo appears: {results['most_common_percentage']:.1f}% of time\")\n",
    "    print(f\"    Algorithm inclusion range: {min(results['algorithm_inclusion'].values())} - {max(results['algorithm_inclusion'].values())} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
